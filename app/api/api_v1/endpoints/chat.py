from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from app.schemas.chat import (
    ChatRoom, ChatRoomCreate, ChatRoomList, 
    ChatMessageCreate, ChatMessage, ChatMessageList,
    ChatRequest, TokenUsage, PromptGenerateRequest
)
from app.crud import crud_chat, crud_stats, crud_project, crud_subscription
from app.crud.crud_anonymous_usage import crud_anonymous_usage
from app.db.session import get_db
from app.core.security import get_current_user
from app.models.user import User
import json
from app.core.config import settings
import logging
import base64
from typing import Optional, List, AsyncGenerator, Dict, Any, Set
import os
from datetime import datetime, timezone
from app.models.subscription import Subscription
import time
import asyncio
import io
import hashlib
import uuid
from typing import Dict, Set
from fastapi import HTTPException

# ìƒˆë¡œìš´ Google Genai ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from google import genai
from google.genai import types

from app.core.models import (
    ACTIVE_MODELS, get_model_config, get_multimodal_models, 
    ALLOWED_MODELS, ModelProvider
)

router = APIRouter()
logger = logging.getLogger(__name__)

# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
MULTIMODAL_MODELS = get_multimodal_models()

# Gemini ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
GEMINI_MODELS = [
    model_name for model_name, config in ACTIVE_MODELS.items()
    if config.provider == ModelProvider.GOOGLE
]

MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
GEMINI_INLINE_DATA_LIMIT = 10 * 1024 * 1024  # 10MB (Gemini API ì œí•œ)

# ì‚¬ê³  ê¸°ëŠ¥ ìµœì í™” ì„¤ì • ì¶”ê°€
THINKING_OPTIMIZATION = {
    "gemini-2.5-flash": {
        "default_budget": 0,  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
        "max_budget": 24576,
        "adaptive": True  # ìƒí™©ì— ë”°ë¼ ì ì‘í˜• ì‚¬ê³  ì˜ˆì‚°
    },
    "gemini-2.5-pro": {
        "default_budget": 4096,  # ProëŠ” í’ˆì§ˆ ìœ„í•´ ê¸°ë³¸ ì‚¬ê³  ìœ ì§€
        "max_budget": 8192,
        "adaptive": True
    }
}

# ì±„íŒ… ì„¸ì…˜ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
CHAT_SESSION_CACHE = {}

# ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì„¤ì •
STREAMING_BUFFER_SIZE = 1024  # ë°”ì´íŠ¸ ë‹¨ìœ„
STREAMING_FLUSH_INTERVAL = 0.1  # ì´ˆ ë‹¨ìœ„

# ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì„¤ì •
CONTEXT_COMPRESSION_THRESHOLD = 0.8  # ì»¨í…ìŠ¤íŠ¸ 80% ì´ˆê³¼ ì‹œ ì••ì¶•
CONTEXT_SUMMARY_RATIO = 0.3  # ì••ì¶• ì‹œ 30%ë¡œ ìš”ì•½

def generateUniqueId():
    return int(time.time() * 1000)

async def process_file_to_base64(file: UploadFile) -> tuple[str, str]:
    try:
        contents = await file.read()
        
        # íŒŒì¼ì´ Gemini API ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° File API ì‚¬ìš©
        if len(contents) > GEMINI_INLINE_DATA_LIMIT:
            logger.warning(f"File {file.filename} ({len(contents)} bytes) exceeds Gemini inline data limit. Using File API instead.")
            
            # Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = get_gemini_client()
            if not client:
                raise HTTPException(status_code=500, detail="Gemini API í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨")
            
            try:
                # File APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë¡œë“œ
                uploaded_file = client.files.upload(
                    file=io.BytesIO(contents),
                    config=dict(
                        mime_type=file.content_type,
                        display_name=f"chat_{file.filename}"
                    )
                )
                
                # íŒŒì¼ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
                max_wait_time = 30
                wait_time = 0
                while uploaded_file.state.name == 'PROCESSING' and wait_time < max_wait_time:
                    await asyncio.sleep(2)
                    wait_time += 2
                    try:
                        uploaded_file = client.files.get(name=uploaded_file.name)
                    except Exception as e:
                        logger.error(f"Error checking file status: {e}", exc_info=True)
                        break
                
                # ì²˜ë¦¬ ìƒíƒœ í™•ì¸
                if uploaded_file.state.name != 'ACTIVE':
                    logger.warning(f"File {file.filename} is in state {uploaded_file.state.name}")
                
                # File API URI ë°˜í™˜ (base64 ëŒ€ì‹ )
                return f"FILE_API:{uploaded_file.name}", file.content_type
                
            except Exception as e:
                logger.error(f"File API upload failed for {file.filename}: {e}", exc_info=True)
                # í´ë°±: íŒŒì¼ ì •ë³´ë§Œ ì „ì†¡
                file_info = f"íŒŒì¼ëª…: {file.filename}, í¬ê¸°: {len(contents)} bytes, íƒ€ì…: {file.content_type} (File API ì—…ë¡œë“œ ì‹¤íŒ¨)"
                base64_data = base64.b64encode(file_info.encode()).decode('utf-8')
                return base64_data, file.content_type
        else:
            # ì‘ì€ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
            base64_data = base64.b64encode(contents).decode('utf-8')
            return base64_data, file.content_type
            
    except Exception as e:
        raise

def get_gemini_client():
    """ìƒˆë¡œìš´ Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if not settings.GEMINI_API_KEY:
            return None
        
        # ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = genai.Client(api_key=settings.GEMINI_API_KEY)
        return client
    except Exception as e:
        logger.error(f"Gemini client creation error: {e}", exc_info=True)
        return None

async def count_gemini_tokens(text: str, model: str, client) -> dict:
    """ì •í™•í•œ Gemini ëª¨ë¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        result = client.models.count_tokens(
            model=model,
            contents=text
        )
        return {
            "input_tokens": result.total_tokens,
            "output_tokens": 0
        }
    except Exception as e:
        logger.error(f"Gemini token counting error: {e}", exc_info=True)
        return {
            "input_tokens": len(text) // 4,  # ëŒ€ëµì ì¸ í† í° ê³„ì‚°
            "output_tokens": 0
        }

# í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_weather_function():
    """ë‚ ì”¨ ì •ë³´ ì¡°íšŒ í•¨ìˆ˜"""
    return {
        "name": "get_weather",
        "description": "ì§€ì •ëœ ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "ë‚ ì”¨ë¥¼ ì¡°íšŒí•  ë„ì‹œ ì´ë¦„ (ì˜ˆ: ì„œìš¸, ë¶€ì‚°)",
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "ì˜¨ë„ ë‹¨ìœ„ (ê¸°ë³¸ê°’: celsius)",
                },
            },
            "required": ["location"],
        },
    }

def create_calculator_function():
    """ê³„ì‚°ê¸° í•¨ìˆ˜"""
    return {
        "name": "calculate",
        "description": "ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string",
                    "description": "ê³„ì‚°í•  ìˆ˜í•™ í‘œí˜„ì‹ (ì˜ˆ: 2+3*4, sqrt(16))",
                },
            },
            "required": ["expression"],
        },
    }

def create_search_function():
    """ê²€ìƒ‰ í•¨ìˆ˜"""
    return {
        "name": "search_knowledge",
        "description": "ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸",
                },
                "category": {
                    "type": "string",
                    "enum": ["academic", "general", "technical"],
                    "description": "ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬",
                },
            },
            "required": ["query"],
        },
    }

async def execute_function_call(function_name: str, arguments: dict) -> dict:
    """í•¨ìˆ˜ í˜¸ì¶œ ì‹¤í–‰"""
    try:
        if function_name == "get_weather":
            location = arguments.get("location", "")
            unit = arguments.get("unit", "celsius")
            # ì‹¤ì œ ë‚ ì”¨ API í˜¸ì¶œ ëŒ€ì‹  ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            return {
                "location": location,
                "temperature": 22 if unit == "celsius" else 72,
                "unit": unit,
                "description": "ë§‘ìŒ",
                "humidity": 65
            }
        
        elif function_name == "calculate":
            expression = arguments.get("expression", "")
            try:
                # ì•ˆì „í•œ ìˆ˜í•™ ê³„ì‚°
                import math
                import re
                
                # í—ˆìš©ëœ í•¨ìˆ˜ë“¤ë§Œ ì‚¬ìš©
                allowed_names = {
                    "sqrt": math.sqrt,
                    "sin": math.sin,
                    "cos": math.cos,
                    "tan": math.tan,
                    "log": math.log,
                    "exp": math.exp,
                    "pow": pow,
                    "abs": abs,
                    "round": round,
                    "pi": math.pi,
                    "e": math.e
                }
                
                # ì•ˆì „í•œ í‘œí˜„ì‹ì¸ì§€ í™•ì¸
                if re.search(r'[a-zA-Z_]', expression):
                    # í•¨ìˆ˜ëª…ì´ í¬í•¨ëœ ê²½ìš° í—ˆìš©ëœ í•¨ìˆ˜ì¸ì§€ í™•ì¸
                    for name in allowed_names.keys():
                        expression = expression.replace(name, f"allowed_names['{name}']")
                
                result = eval(expression, {"__builtins__": {}, "allowed_names": allowed_names})
                return {"result": result, "expression": arguments.get("expression", "")}
            except Exception as e:
                return {"error": f"ê³„ì‚° ì˜¤ë¥˜: {str(e)}", "expression": arguments.get("expression", "")}
        
        elif function_name == "search_knowledge":
            query = arguments.get("query", "")
            category = arguments.get("category", "general")
            # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ì‹  ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            return {
                "query": query,
                "category": category,
                "results": [
                    {"title": f"{query}ì— ëŒ€í•œ ì •ë³´", "content": f"{category} ì¹´í…Œê³ ë¦¬ì—ì„œ {query}ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."}
                ]
            }
        
        else:
            return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {function_name}"}
    
    except Exception as e:
        return {"error": f"í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"}

async def generate_gemini_stream_response(
    request: Request,
    messages: list,
    model: str,
    room_id: str,
    db: Session,
    user_id: str,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    file_names: Optional[List[str]] = None,
    enable_thinking: bool = True,
    enable_grounding: bool = True,
    enable_code_execution: bool = True,
    thinking_budget: int = 8192
) -> AsyncGenerator[str, None]:
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")

        config = get_model_config(model)
        if not config or config.provider != ModelProvider.GOOGLE:
            raise HTTPException(status_code=400, detail="Invalid Gemini model")

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not messages or len(messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )

        # í† í° ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
        # ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ì¶œë ¥ í† í°ì„ ìœ„í•œ ì—¬ìœ  ê³µê°„ í™•ë³´)
        MAX_CONTEXT_TOKENS = config.max_tokens - 2048  # ì¶œë ¥ì„ ìœ„í•œ 2048 í† í° ì˜ˆì•½
        
        # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœê·¼ ë©”ì‹œì§€ë¶€í„° í¬í•¨
        valid_messages = []
        total_tokens = 0
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í° ê³„ì‚°
        if config.system_prompt:
            system_tokens = await count_gemini_tokens(config.system_prompt, model, client)
            total_tokens += system_tokens.get("input_tokens", 0)
        
        # íŒŒì¼ í† í° ê³„ì‚° (ìˆëŠ” ê²½ìš°)
        file_tokens = 0
        if file_data_list and file_types:
            # ì´ë¯¸ì§€ëŠ” íƒ€ì¼ë‹¹ 258 í† í°, PDFëŠ” í˜ì´ì§€ë‹¹ 258 í† í°
            for file_type in file_types:
                if file_type.startswith("image/"):
                    file_tokens += 258  # Gemini 2.5 ê¸°ì¤€
                elif file_type == "application/pdf":
                    file_tokens += 258 * 10  # ì˜ˆìƒ í˜ì´ì§€ ìˆ˜
        
        total_tokens += file_tokens
        
        # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ ê²€í† í•˜ë©´ì„œ í† í° ì˜ˆì‚° ë‚´ì—ì„œ í¬í•¨
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("content") and msg["content"].strip():
                # ë©”ì‹œì§€ í† í° ê³„ì‚°
                msg_tokens = await count_gemini_tokens(
                    f"{msg['role']}: {msg['content']}", 
                    model, 
                    client
                )
                msg_token_count = msg_tokens.get("input_tokens", 0)
                
                # í† í° ì˜ˆì‚° í™•ì¸
                if total_tokens + msg_token_count <= MAX_CONTEXT_TOKENS:
                    valid_messages.insert(0, msg)
                    total_tokens += msg_token_count
                else:
                    # í† í° í•œê³„ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                    logger.info(f"Context window limit reached. Including {len(valid_messages)} messages out of {len(messages)}")
                    break
        
        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ë©”ì‹œì§€ëŠ” í¬í•¨ë˜ì–´ì•¼ í•¨
        if len(valid_messages) == 0 and len(messages) > 0:
            last_msg = messages[-1]
            if last_msg.get("content") and last_msg["content"].strip():
                valid_messages = [last_msg]
        
        if len(valid_messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="No valid message content found"
            )
        
        logger.info(f"Context management: Using {len(valid_messages)} messages with {total_tokens} tokens")

        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì ìš© (í•„ìš”í•œ ê²½ìš°)
        if len(valid_messages) > 5:  # 5ê°œ ì´ìƒ ë©”ì‹œì§€ê°€ ìˆì„ ë•Œë§Œ ì••ì¶• ê³ ë ¤
            valid_messages = await compress_context_if_needed(
                client=client,
                model=model,
                messages=valid_messages,
                max_tokens=MAX_CONTEXT_TOKENS
            )

        # ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ë©€í‹°ëª¨ë‹¬ ì»¨í…ì¸  ìƒì„±
        if file_data_list and file_types and file_names:
            for file_data, file_type, file_name in zip(file_data_list, file_types, file_names):
                # File APIë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì¸ì§€ í™•ì¸
                if file_data.startswith("FILE_API:"):
                    # File API URIì—ì„œ íŒŒì¼ ì´ë¦„ ì¶”ì¶œ
                    file_uri = file_data.replace("FILE_API:", "")
                    try:
                        # File API ê°ì²´ë¡œ ì§ì ‘ ì¶”ê°€
                        uploaded_file = client.files.get(name=file_uri)
                        contents.append(uploaded_file)
                        logger.info(f"Added File API file: {file_name} ({file_uri})")
                    except Exception as e:
                        logger.error(f"Failed to get File API file {file_uri}: {e}", exc_info=True)
                        # í´ë°±: íŒŒì¼ ì •ë³´ í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                        contents.append(f"íŒŒì¼: {file_name} (File API ì²˜ë¦¬ ì‹¤íŒ¨)")
                else:
                    # ê¸°ì¡´ base64 ë°©ì‹ ì²˜ë¦¬
                    if file_type.startswith("image/"):
                        contents.append(
                            types.Part.from_bytes(
                                data=base64.b64decode(file_data),
                                mime_type=file_type
                            )
                        )
                    elif file_type == "application/pdf":
                        # PDF íŒŒì¼ ì²˜ë¦¬
                        pdf_data = base64.b64decode(file_data)
                        contents.append(
                            types.Part.from_bytes(
                                data=pdf_data,
                                mime_type=file_type
                            )
                        )

        # ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation_text = ""
        for message in valid_messages:
            role_text = "Human" if message["role"] == "user" else "Assistant"
            conversation_text += f"{role_text}: {message['content']}\n"

        contents.append(conversation_text)

        # ë„êµ¬ ì„¤ì • - ìµœì‹  API êµ¬ì¡° ì‚¬ìš©
        tools = []
        
        if enable_grounding:
            # Google ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì¶”ê°€ (ìµœì‹  API ë°©ì‹)
            tools.append(types.Tool(google_search=types.GoogleSearch()))
        
        if enable_code_execution:
            # ì½”ë“œ ì‹¤í–‰ ì¶”ê°€
            tools.append(types.Tool(code_execution=types.ToolCodeExecution()))

        # ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬ (ì»¨í…ìŠ¤íŠ¸ ìºì‹± ëŒ€ì²´)
        chat_session = None
        if room_id:
            chat_session = await get_or_create_chat_session(
                client=client,
                model=model,
                room_id=room_id,
                system_instruction=config.system_prompt
            )

        # ìƒì„± ì„¤ì • (ìµœì í™”ëœ êµ¬ì¡°)
        generation_config = types.GenerateContentConfig(
            system_instruction=config.system_prompt,
            temperature=config.temperature,
            top_p=config.top_p,
            max_output_tokens=config.max_tokens,
            tools=tools if tools else None
        )

        # ìµœì í™”ëœ ì‚¬ê³  ê¸°ëŠ¥ ì„¤ì •
        if enable_thinking:
            optimized_thinking_config = await get_optimized_thinking_config(
                model=model,
                request_type="complex" if len(valid_messages) > 5 else "simple",
                user_preference=None  # í–¥í›„ ì‚¬ìš©ì ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
            )
            if optimized_thinking_config:
                generation_config.thinking_config = optimized_thinking_config

        # ì…ë ¥ í† í° ê³„ì‚°
        input_token_count = await count_gemini_tokens(conversation_text, model, client)
        input_tokens = input_token_count.get("input_tokens", 0)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ë²„í¼ë§ ìµœì í™”)
        accumulated_content = ""
        accumulated_thinking = ""
        thought_time = 0.0
        citations = []
        web_search_queries = []
        streaming_completed = False  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì—¬ë¶€ ì²´í¬
        is_disconnected = False  # ì—°ê²° ì¤‘ë‹¨ í”Œë˜ê·¸ ì¶”ê°€
        
        # ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì´ˆê¸°í™”
        content_buffer = StreamingBuffer()
        thinking_buffer = StreamingBuffer()

        try:
            response = client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=generation_config
            )

            start_time = time.time()

            for chunk in response:
                # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
                if await request.is_disconnected():
                    is_disconnected = True
                    logger.warning("Client disconnected. Stopping stream.")
                    break
                if chunk.candidates and len(chunk.candidates) > 0:
                    candidate = chunk.candidates[0]
                    
                    # ì½˜í…ì¸  íŒŒíŠ¸ ì²˜ë¦¬
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            # ì‚¬ê³  ë‚´ìš©ê³¼ ì¼ë°˜ ì½˜í…ì¸  ë¶„ë¦¬
                            if hasattr(part, 'thought') and part.thought:
                                # ì‚¬ê³  ë‚´ìš©ë§Œ ì²˜ë¦¬
                                if part.text:
                                    accumulated_thinking += part.text
                                    thought_time = time.time() - start_time
                                    
                                    # ë²„í¼ë§ëœ ì‚¬ê³  ë‚´ìš© ì „ì†¡
                                    if thinking_buffer.add_chunk(part.text):
                                        buffered_content = thinking_buffer.flush()
                                        try:
                                            yield f"data: {json.dumps({'reasoning_content': buffered_content, 'thought_time': thought_time})}\n\n"
                                        except (ConnectionError, BrokenPipeError, GeneratorExit):
                                            logger.warning("Client disconnected during reasoning streaming")
                                            return
                            elif part.text:
                                # ì¼ë°˜ ì‘ë‹µ ë‚´ìš©ë§Œ ì²˜ë¦¬
                                accumulated_content += part.text
                                
                                # ë²„í¼ë§ëœ ì¼ë°˜ ë‚´ìš© ì „ì†¡
                                if content_buffer.add_chunk(part.text):
                                    buffered_content = content_buffer.flush()
                                    try:
                                        yield f"data: {json.dumps({'content': buffered_content})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        logger.warning("Client disconnected during content streaming")
                                        return

                    # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (ìµœì‹  API êµ¬ì¡°)
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding = candidate.grounding_metadata
                        
                        # ì‹¤ì œ ê°’ë“¤ í™•ì¸
                        logger.debug("=== GROUNDING VALUES DEBUG ===")
                        logger.debug(f"web_search_queries: {getattr(grounding, 'web_search_queries', None)}")
                        logger.debug(f"grounding_chunks: {getattr(grounding, 'grounding_chunks', None)}")
                        logger.debug(f"grounding_supports: {getattr(grounding, 'grounding_supports', None)}")
                        
                        # ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜ì§‘
                        if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                            logger.debug(f"Adding web_search_queries: {grounding.web_search_queries}")
                            web_search_queries.extend(grounding.web_search_queries)
                        
                        # grounding_supportsì—ì„œ citations ì¶”ì¶œ ì‹œë„
                        if hasattr(grounding, 'grounding_supports') and grounding.grounding_supports:
                            logger.debug(f"Found grounding_supports: {len(grounding.grounding_supports)} supports")
                            new_citations = []
                            for i, support in enumerate(grounding.grounding_supports):
                                logger.debug(f"Support {i}: type={type(support)}, dir={dir(support)}")
                                logger.debug(f"Support {i} content: {support}")
                                
                                # grounding_chunk_indicesê°€ ìˆëŠ”ì§€ í™•ì¸
                                if hasattr(support, 'grounding_chunk_indices') and support.grounding_chunk_indices:
                                    for chunk_idx in support.grounding_chunk_indices:
                                        if (hasattr(grounding, 'grounding_chunks') and 
                                            grounding.grounding_chunks and 
                                            chunk_idx < len(grounding.grounding_chunks)):
                                            chunk = grounding.grounding_chunks[chunk_idx]
                                            logger.debug(f"Referenced chunk {chunk_idx}: {chunk}")
                                            
                                            if hasattr(chunk, 'web') and chunk.web:
                                                citation = {
                                                    "url": getattr(chunk.web, 'uri', ''),
                                                    "title": getattr(chunk.web, 'title', '')
                                                }
                                                logger.debug(f"Extracted citation from support: {citation}")
                                                if citation['url'] and not any(c['url'] == citation['url'] for c in citations):
                                                    citations.append(citation)
                                                    new_citations.append(citation)
                        
                        # ì§ì ‘ grounding chunksì—ì„œë„ ì‹œë„
                        if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                            logger.debug(f"Found grounding_chunks: {len(grounding.grounding_chunks)} chunks")
                            new_citations = []
                            for i, chunk in enumerate(grounding.grounding_chunks):
                                logger.debug(f"Direct chunk {i}: {chunk}")
                                if hasattr(chunk, 'web') and chunk.web:
                                    citation = {
                                        "url": getattr(chunk.web, 'uri', ''),
                                        "title": getattr(chunk.web, 'title', '')
                                    }
                                    logger.debug(f"Direct extracted citation: {citation}")
                                    if citation['url'] and not any(c['url'] == citation['url'] for c in citations):
                                        citations.append(citation)
                                        new_citations.append(citation)
                            
                            # ìƒˆë¡œìš´ ì¸ìš© ì •ë³´ë§Œ ì „ì†¡
                            if new_citations:
                                logger.debug(f"Sending {len(new_citations)} new citations")
                                try:
                                    yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    logger.warning("Client disconnected during citations streaming")
                                    return
                        
                        # ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì†¡
                        if web_search_queries:
                            logger.debug(f"Sending search queries: {web_search_queries}")
                            try:
                                yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                            except (ConnectionError, BrokenPipeError, GeneratorExit):
                                logger.warning("Client disconnected during search queries streaming")
                                return

            # ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if is_disconnected:
                logger.info("Skipping post-processing due to client disconnection.")
                return
            
            # ë²„í¼ì— ë‚¨ì€ ë‚´ìš© ì²˜ë¦¬
            remaining_content = content_buffer.flush()
            if remaining_content:
                try:
                    yield f"data: {json.dumps({'content': remaining_content})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    logger.warning("Client disconnected during final content flush")
                    return
            
            remaining_thinking = thinking_buffer.flush()
            if remaining_thinking:
                try:
                    yield f"data: {json.dumps({'reasoning_content': remaining_thinking, 'thought_time': thought_time})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    logger.warning("Client disconnected during final thinking flush")
                    return
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨
            streaming_completed = True
            
            # ì¶œë ¥ í† í° ê³„ì‚°
            output_token_count = await count_gemini_tokens(accumulated_content, model, client)
            output_tokens = output_token_count.get("input_tokens", 0)
            
            # ì‚¬ê³  í† í° ê³„ì‚°
            thinking_tokens = 0
            if accumulated_thinking:
                thinking_token_count = await count_gemini_tokens(accumulated_thinking, model, client)
                thinking_tokens = thinking_token_count.get("input_tokens", 0)

            # í† í° ì‚¬ìš©ëŸ‰ ì €ì¥ (KST ì‹œê°„ìœ¼ë¡œ ì €ì¥)
            from pytz import timezone
            kst = timezone('Asia/Seoul')
            crud_stats.create_token_usage(
                db=db,
                user_id=user_id,
                room_id=room_id,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens + thinking_tokens,  # ì‚¬ê³  í† í° í¬í•¨
                timestamp=datetime.now(kst)
            )

        except (ConnectionError, BrokenPipeError, GeneratorExit):
            streaming_completed = False  # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            logger.warning("Client disconnected during main streaming loop")
            return
        except Exception as api_error:
            streaming_completed = False  # ì—ëŸ¬ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            error_message = f"Gemini API Error: {str(api_error)}"
            try:
                yield f"data: {json.dumps({'error': error_message})}\n\n"
            except (ConnectionError, BrokenPipeError, GeneratorExit):
                logger.warning("Client disconnected during error streaming")
                return
        
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥ (ìƒˆë¡œìš´ DB ì„¸ì…˜ ì‚¬ìš©)
        if streaming_completed and accumulated_content:
            logger.debug("=== SAVING MESSAGE DEBUG ===")
            logger.debug(f"streaming_completed: {streaming_completed}")
            logger.debug(f"accumulated_content length: {len(accumulated_content)}")
            logger.debug(f"citations count: {len(citations)}")
            logger.debug(f"citations: {citations}")
            
            # ìƒˆë¡œìš´ DB ì„¸ì…˜ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ ì„¸ì…˜ê³¼ ë¶„ë¦¬)
            from app.db.session import SessionLocal
            new_db = SessionLocal()
            try:
                message_create = ChatMessageCreate(
                    content=accumulated_content,
                    role="assistant",
                    room_id=room_id,
                    reasoning_content=accumulated_thinking if accumulated_thinking else None,
                    thought_time=thought_time if thought_time > 0 else None,
                    citations=citations if citations else None
                )
                saved_message = crud_chat.create_message(new_db, room_id, message_create)
                logger.info(f"Message saved with ID: {saved_message.id}")
                logger.debug(f"Saved message citations: {saved_message.citations}")
                logger.debug("=== END SAVING DEBUG ===")
            finally:
                new_db.close()
        else:
            logger.info("=== MESSAGE NOT SAVED ===")
            logger.info(f"streaming_completed: {streaming_completed}")
            logger.info(f"accumulated_content: {bool(accumulated_content)}")
            logger.info(f"Reason: {'Streaming was interrupted' if not streaming_completed else 'No content'}")
            logger.info("=== END NOT SAVED DEBUG ===")

    except Exception as e:
        error_message = f"Stream Generation Error: {str(e)}"
        try:
            yield f"data: {json.dumps({'error': error_message})}\n\n"
        except (ConnectionError, BrokenPipeError, GeneratorExit):
            logger.warning("Client disconnected during final error streaming (generate_gemini_stream_response)")
            return

async def generate_stream_response(
    request: Request,
    chat_request: ChatRequest,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    room_id: Optional[str] = None,
    db: Optional[Session] = None,
    user_id: Optional[str] = None,
    subscription_plan: Optional[str] = None,
    file_names: Optional[List[str]] = None
):
    try:
        config = get_model_config(chat_request.model)
        if not config:
            raise HTTPException(status_code=400, detail="Invalid model specified")

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not chat_request.messages or len(chat_request.messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )

        # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
        MAX_MESSAGES = 10
        valid_messages = [
            msg for msg in chat_request.messages[-MAX_MESSAGES:] 
            if msg.content and msg.content.strip()
        ]
        
        if len(valid_messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="No valid message content found"
            )

        # Gemini ëª¨ë¸ ì²˜ë¦¬
        formatted_messages = [
            {"role": msg.role, "content": msg.content} 
            for msg in valid_messages
        ]
        
        # ê³ ê¸‰ ê¸°ëŠ¥ ì„¤ì • (êµ¬ë… ë“±ê¸‰ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
        enable_thinking = True
        enable_grounding = True
        enable_code_execution = True
        thinking_budget = 8192
        
        if subscription_plan:
            # êµ¬ë… ë“±ê¸‰ì— ë”°ë¥¸ ê¸°ëŠ¥ ì œí•œ (ì˜ˆì‹œ)
            if subscription_plan == "FREE":
                thinking_budget = 1024
                enable_grounding = False
                enable_code_execution = False
            elif subscription_plan == "BASIC":
                thinking_budget = 4096
                enable_grounding = True
                enable_code_execution = False
        
        async for chunk in generate_gemini_stream_response(
            request,
            formatted_messages, 
            chat_request.model, 
            room_id or "", 
            db, 
            user_id or "",
            file_data_list, 
            file_types, 
            file_names,
            enable_thinking=enable_thinking,
            enable_grounding=enable_grounding,
            enable_code_execution=enable_code_execution,
            thinking_budget=thinking_budget
        ):
            yield chunk

    except Exception as e:
        error_message = f"Stream Generation Error: {str(e)}"
        try:
            yield f"data: {json.dumps({'error': error_message})}\n\n"
        except (ConnectionError, BrokenPipeError, GeneratorExit):
            logger.warning("Client disconnected during final error streaming (generate_stream_response)")
            return

async def validate_file(file: UploadFile) -> bool:
    """íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    if file.size and file.size > MAX_FILE_SIZE:
        return False
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¥
    if file.content_type.startswith("image/"):
        return True
    elif file.content_type == "application/pdf":
        return True
    elif file.content_type in ["text/plain", "text/csv", "application/json"]:
        return True
    elif file.content_type.startswith("text/"):
        return True
    elif file.content_type in ["application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                             "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                             "application/vnd.openxmlformats-officedocument.presentationml.presentation"]:
        return True
    elif file.content_type.startswith("application/"):
        return True
    
    return False

async def generate_chat_room_name(first_message: str) -> str:
    """Open-WebUI ìŠ¤íƒ€ì¼ì˜ AI ê¸°ë°˜ ì±„íŒ…ë°© ì œëª© ìƒì„±"""
    try:
        # ë¹ˆ ë©”ì‹œì§€ ì²˜ë¦¬
        if not first_message or len(first_message.strip()) == 0:
            return "ìƒˆ ì±„íŒ…"
        
        # ê°„ë‹¨í•œ fallback ë¨¼ì € ìƒì„± (AI ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        words = first_message.strip().split()
        fallback_title = " ".join(words[:3]) if len(words) >= 3 else " ".join(words)
        if len(fallback_title) > 20:
            fallback_title = fallback_title[:17] + "..."
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        client = get_gemini_client()
        if not client:
            logger.warning("Gemini client not available, using fallback")
            return fallback_title
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt_template = """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ì„í•´ì„œ ëŒ€í™” ì£¼ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°„ê²°í•œ í•œêµ­ì–´ ì œëª©ì„ ë§Œë“œì„¸ìš”.

ê·œì¹™:
- 3-5ë‹¨ì–´ + ì´ëª¨ì§€ 1ê°œ
- ëŒ€í™” ì£¼ì œë‚˜ ì§ˆë¬¸ ë‚´ìš©ì„ ìš”ì•½
- ë‹¨ìˆœí•œ ì¸ì‚¬ë§("ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "hi" ë“±)ì€ ë°˜ë“œì‹œ "ğŸ’¬ ì¼ë°˜ ëŒ€í™”"ë¡œ ì²˜ë¦¬
- ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ êµ¬ì²´ì ì¸ ì œëª© ìƒì„±
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ

ì˜ˆì‹œ:
{{"title": "ğŸ“š íŒŒì´ì¬ í•™ìŠµ ì§ˆë¬¸"}}
{{"title": "ğŸ• ìš”ë¦¬ ë ˆì‹œí”¼ ë¬¸ì˜"}}
{{"title": "ğŸ’» í”„ë¡œê·¸ë˜ë° ë„ì›€"}}
{{"title": "ğŸ’¬ ì¼ë°˜ ëŒ€í™”"}}

íŠ¹ë³„ ì²˜ë¦¬:
- "ì•ˆë…•", "í•˜ì´", "í—¬ë¡œ", "hi", "hello" ë“± â†’ {{"title": "ğŸ’¬ ì¼ë°˜ ëŒ€í™”"}}
- ë‹¨ìˆœ ì¸ì‚¬ ì´ì™¸ì˜ ì˜ë¯¸ìˆëŠ” ë‚´ìš© â†’ êµ¬ì²´ì ì¸ ì œëª© ìƒì„±

ì‚¬ìš©ì ë©”ì‹œì§€: {message}

JSON ì‘ë‹µ:"""

        # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
        limited_message = first_message[:200] if len(first_message) > 200 else first_message
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = prompt_template.format(message=limited_message)
        
        logger.info(f"Generating AI title for message: '{limited_message[:50]}...'")
        
        # Gemini API í˜¸ì¶œ
        logger.info(f"Final prompt being sent to Gemini: {repr(prompt)}")
        try:
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=[prompt],
                config=types.GenerateContentConfig(
                    temperature=0.1,  # ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                    max_output_tokens=100  # JSON ì‘ë‹µìš©
                )
            )
            logger.info(f"Gemini raw response: {repr(response.text)}")
            
            # JSON ì‘ë‹µ íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            if hasattr(response, 'text') and response.text:
                import json
                import re
                try:
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ``` í˜•íƒœ)
                    text = response.text.strip()
                    if text.startswith('```'):
                        # ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', text, re.DOTALL)
                        if json_match:
                            text = json_match.group(1).strip()
                    
                    result = json.loads(text)
                    if 'title' in result and result['title']:
                        ai_title = result['title'].strip()
                        # ê¸¸ì´ ì œí•œ í™•ì¸
                        if len(ai_title) <= 25:
                            logger.info(f"âœ“ AI generated title: '{ai_title}'")
                            return ai_title
                        else:
                            logger.warning(f"AI title too long: '{ai_title}', using fallback")
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse JSON response: {e}, response: '{response.text}'")
            
            # candidates êµ¬ì¡° ì‹œë„ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and candidate.content:
                        if hasattr(candidate.content, 'parts') and candidate.content.parts:
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    try:
                                        import json
                                        import re
                                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                                        text = part.text.strip()
                                        if text.startswith('```'):
                                            json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', text, re.DOTALL)
                                            if json_match:
                                                text = json_match.group(1).strip()
                                        
                                        result = json.loads(text)
                                        if 'title' in result and result['title']:
                                            ai_title = result['title'].strip()
                                            if len(ai_title) <= 25:
                                                logger.info(f"âœ“ AI generated title from candidates: '{ai_title}'")
                                                return ai_title
                                    except json.JSONDecodeError:
                                        continue
            
        except Exception as api_error:
            logger.warning(f"Gemini API call failed: {api_error}")
        
        # AI ìƒì„± ì‹¤íŒ¨ ì‹œ fallback ì‚¬ìš©
        logger.info(f"Using fallback title: '{fallback_title}'")
        return fallback_title
        
            
    except Exception as e:
        logger.error(f"Chat room name generation error: {e}", exc_info=True)
        return "ìƒˆ ì±„íŒ…"

@router.post("/title/generate", summary="ì±„íŒ…ë°© ì œëª© ìƒì„±")
async def generate_title_api(
    request: Request
):
    """Open-WebUI ìŠ¤íƒ€ì¼ ì±„íŒ…ë°© ì œëª© ìƒì„± API"""
    try:
        body = await request.json()
        messages = body.get("messages", [])
        
        if not messages:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )
        
        # ì²« ë²ˆì§¸ user ë©”ì‹œì§€ ì°¾ê¸°
        first_user_message = None
        for msg in messages:
            if msg.get("role") == "user" and msg.get("content"):
                first_user_message = msg.get("content")
                break
        
        if not first_user_message:
            raise HTTPException(
                status_code=400,
                detail="No user message found"
            )
        
        # ì œëª© ìƒì„±
        title = await generate_chat_room_name(first_user_message)
        
        return {
            "title": title,
            "status": "success"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Title generation API error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error during title generation"
        )

@router.post("/rooms", response_model=ChatRoom, summary="ìƒˆ ì±„íŒ…ë°© ìƒì„±")
def create_chat_room(
    room: ChatRoomCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ìƒˆë¡œìš´ ì±„íŒ…ë°©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    - **title**: ì±„íŒ…ë°© ì œëª© (í•„ìˆ˜)
    - **model**: ì‚¬ìš©í•  AI ëª¨ë¸ (í•„ìˆ˜, ì§€ì›ë˜ëŠ” ëª¨ë¸: gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash-exp ë“±)
    - **system_prompt**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
    - **project_id**: í”„ë¡œì íŠ¸ ID (ì„ íƒ, í”„ë¡œì íŠ¸ ë‚´ ì±„íŒ…ë°©ì¸ ê²½ìš°)
    
    **ì§€ì›ë˜ëŠ” AI ëª¨ë¸:**
    - Gemini Flash (ë¹ ë¥¸ ì‘ë‹µ)
    - Gemini Pro (ê³ í’ˆì§ˆ ì‘ë‹µ)
    - Gemini Flash Thinking (ì¶”ë¡  ê³¼ì • í¬í•¨)
    
    **ì‘ë‹µ:**
    - ìƒì„±ëœ ì±„íŒ…ë°© ì •ë³´ ë°˜í™˜
    """
    return crud_chat.create_chat_room(db, room, current_user.id)

@router.post("/rooms/{room_id}/generate-name")
async def generate_room_name(
    room_id: str,
    message_content: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±„íŒ…ë°© ì´ë¦„ì„ ìƒì„±í•˜ê³  ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # ì±„íŒ…ë°© ì†Œìœ ê¶Œ í™•ì¸
        chat_room = crud_chat.get_chat_room(db, room_id, current_user.id)
        if not chat_room:
            raise HTTPException(status_code=404, detail="Chat room not found")
        
        # ì±„íŒ…ë°© ì´ë¦„ ìƒì„±
        generated_name = await generate_chat_room_name(message_content)
        
        # ì±„íŒ…ë°© ì´ë¦„ ì—…ë°ì´íŠ¸
        from app.schemas.chat import ChatRoomCreate
        room_update = ChatRoomCreate(
            name=generated_name
        )
        
        updated_room = crud_chat.update_chat_room(db, room_id, room_update, current_user.id)
        
        return {
            "room_id": room_id,
            "generated_name": generated_name,
            "updated_room": updated_room
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Room name generation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to generate room name")

@router.get("/rooms", response_model=ChatRoomList)
async def get_chatroom(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    rooms = crud_chat.get_chatroom(db, current_user.id)
    return ChatRoomList(rooms=rooms)

@router.delete("/rooms/{room_id}")
async def delete_chat_room(
    room_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    crud_chat.delete_chat_room(db, room_id, current_user.id)
    return {"message": "Room deleted successfully"}

@router.post("/rooms/{room_id}/messages")
async def create_message(
    room_id: str,
    message: ChatMessageCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    # ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
    if hasattr(message, 'model') and message.model:
        config = get_model_config(message.model)
        if not config:
            raise HTTPException(status_code=400, detail="Invalid model specified")
    
    # ë©”ì‹œì§€ ìƒì„±
    created_message = crud_chat.create_message(db, room_id, message)
    
    # êµ¬ë… ì •ë³´ í™•ì¸ ë° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
    if hasattr(message, 'model') and message.model:
        updated_subscription = crud_subscription.update_model_usage(
            db, current_user.id, message.model
        )
        
        if not updated_subscription:
            # ìƒì„±ëœ ë©”ì‹œì§€ ì‚­ì œ (ë¡¤ë°±)
            db.delete(created_message)
            db.commit()
            raise HTTPException(
                status_code=403, 
                detail="Usage limit exceeded for this model group"
            )
    
    return created_message

@router.get("/rooms/{room_id}/messages", response_model=ChatMessageList)
async def get_chat_messages(
    room_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    messages = crud_chat.get_room_messages(db, room_id, current_user.id)
    return ChatMessageList(messages=messages)

@router.post("/rooms/{room_id}/chat")
async def create_chat_message(
    room_id: str,
    request: Request,
    request_data: str = Form(...),
    files: List[UploadFile] = File([]),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        # JSON íŒŒì‹±
        try:
            parsed_data = json.loads(request_data)
            chat_request = ChatRequest(**parsed_data)
        except (json.JSONDecodeError, ValueError) as e:
            raise HTTPException(status_code=400, detail=f"Invalid JSON format: {str(e)}")

        # ëª¨ë¸ ì„¤ì • í™•ì¸
        config = get_model_config(chat_request.model)
        if not config:
            raise HTTPException(status_code=400, detail="Invalid model specified")

        # íŒŒì¼ ì²˜ë¦¬
        file_data_list = []
        file_types = []
        file_names = []
        
        if files and files[0].filename:
            if not config.supports_multimodal:
                raise HTTPException(
                    status_code=400,
                    detail="File upload is not supported for this model"
                )
            
            for file in files:
                if not await validate_file(file):
                    raise HTTPException(
                        status_code=400,
                        detail=f"Invalid file: {file.filename}"
                    )
                
                file_data, file_type = await process_file_to_base64(file)
                file_data_list.append(file_data)
                file_types.append(file_type)
                file_names.append(file.filename)

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        if chat_request.messages:
            last_message = chat_request.messages[-1]
            user_message = ChatMessageCreate(
                content=last_message.content,
                role="user",
                room_id=room_id,
                files=[{
                    "type": file_type,
                    "name": file_name,
                    "data": file_data
                } for file_data, file_type, file_name in zip(file_data_list, file_types, file_names)] if file_data_list else None
            )
            crud_chat.create_message(db, room_id, user_message)

        # êµ¬ë… ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ (crud_subscription ì‚¬ìš©)
        updated_subscription = crud_subscription.update_model_usage(
            db, current_user.id, chat_request.model
        )
        
        if not updated_subscription:
            raise HTTPException(
                status_code=403, 
                detail="Usage limit exceeded for this model group"
            )

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        return StreamingResponse(
            generate_stream_response(
                request,
                chat_request,
                file_data_list,
                file_types,
                room_id,
                db,
                current_user.id,
                updated_subscription.plan,
                file_names
            ),
            media_type="text/plain"
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@router.patch("/rooms/{room_id}", response_model=ChatRoom)
async def update_chat_room(
    room_id: str,
    room: ChatRoomCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    return crud_chat.update_chat_room(db, room_id, room, current_user.id)

@router.get("/rooms/{room_id}", response_model=ChatRoom)
async def get_chat_room(
    room_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    return crud_chat.get_chat_room(db, room_id, current_user.id)

# ì»¨í…ìŠ¤íŠ¸ ìºì‹± ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@router.post("/cache")
async def create_context_cache(
    content: str = Form(...),
    model: str = Form(...),
    ttl: int = Form(3600),  # ê¸°ë³¸ 1ì‹œê°„
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìƒì„±"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # ìºì‹œ ìƒì„±
        cache = client.caches.create(
            model=model,
            config=types.CreateCachedContentConfig(
                display_name=f"cache_{current_user.id}_{int(time.time())}",
                contents=[content],
                ttl=f"{ttl}s"
            )
        )
        
        return {
            "cache_name": cache.name,
            "ttl": ttl,
            "message": "Cache created successfully"
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create cache: {str(e)}")

@router.get("/cache")
async def list_context_caches(
    current_user: User = Depends(get_current_user)
):
    """ì‚¬ìš©ìì˜ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ëª©ë¡ ì¡°íšŒ"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        caches = []
        for cache in client.caches.list():
            if cache.display_name and cache.display_name.startswith(f"cache_{current_user.id}_"):
                caches.append({
                    "name": cache.name,
                    "display_name": cache.display_name,
                    "model": cache.model,
                    "create_time": cache.create_time.isoformat() if cache.create_time else None,
                    "expire_time": cache.expire_time.isoformat() if cache.expire_time else None
                })
        
        return {"caches": caches}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list caches: {str(e)}")

@router.delete("/cache/{cache_name}")
async def delete_context_cache(
    cache_name: str,
    current_user: User = Depends(get_current_user)
):
    """ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        client.caches.delete(cache_name)
        
        return {"message": "Cache deleted successfully"}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete cache: {str(e)}")

@router.get("/stats/token-usage", response_model=List[TokenUsage])
async def get_token_usage(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    user_id: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
    start_dt = None
    end_dt = None
    
    if start_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
    if end_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
    
    return crud_stats.get_token_usage(
        db=db,
        start_date=start_dt,
        end_date=end_dt,
        user_id=user_id or current_user.id
    )

@router.get("/stats/chat-usage")
async def get_chat_usage(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    user_id: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
    start_dt = None
    end_dt = None
    
    if start_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
    if end_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
    
    return crud_stats.get_chat_statistics(
        db=db,
        start_date=start_dt,
        end_date=end_dt,
        user_id=user_id
    )

@router.get("/stats/token-usage-history")
async def get_token_usage_history(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    user_id: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
    start_dt = None
    end_dt = None
    
    if start_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
    if end_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
    
    return crud_stats.get_token_usage_history(
        db=db,
        start=start_dt,
        end=end_dt,
        user_id=user_id
    )

# ì„ë² ë”© ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@router.post("/embeddings")
async def create_embeddings(
    texts: List[str] = Form(...),
    model: str = Form("text-embedding-004"),
    task_type: str = Form("SEMANTIC_SIMILARITY"),
    current_user: User = Depends(get_current_user)
):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        embeddings = []
        for text in texts:
            result = client.models.embed_content(
                model=model,
                contents=text,
                config=types.EmbedContentConfig(task_type=task_type)
            )
            embeddings.append({
                "text": text,
                "embedding": result.embeddings[0] if result.embeddings else []
            })
        
        return {"embeddings": embeddings}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create embeddings: {str(e)}")

# í”„ë¡¬í”„íŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ ê°œì„ 
@router.post("/generate-prompt")
async def generate_prompt(
    category: str = Form(...),  # ì¹´í…Œê³ ë¦¬ (í•™ìŠµ, ì°½ì‘, ë¶„ì„, ë²ˆì—­, ì½”ë”© ë“±)
    task_description: str = Form(...),  # ì‘ì—… ì„¤ëª…
    style: str = Form("ì¹œê·¼í•œ"),  # ìŠ¤íƒ€ì¼ (ì¹œê·¼í•œ, ì „ë¬¸ì , ì°½ì˜ì , ê°„ê²°í•œ)
    complexity: str = Form("ì¤‘ê°„"),  # ë³µì¡ë„ (ê°„ë‹¨, ì¤‘ê°„, ê³ ê¸‰)
    output_format: str = Form("ììœ í˜•ì‹"),  # ì¶œë ¥ í˜•ì‹ (ììœ í˜•ì‹, ë‹¨ê³„ë³„, í‘œí˜•ì‹, ë¦¬ìŠ¤íŠ¸)
    include_examples: bool = Form(True),  # ì˜ˆì‹œ í¬í•¨ ì—¬ë¶€
    include_constraints: bool = Form(False),  # ì œì•½ì‚¬í•­ í¬í•¨ ì—¬ë¶€
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ê°œì„ ëœ AI í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° (ë¡œê·¸ì¸ í•„ìš”)"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ ì‹œìŠ¤í…œ ì§€ì‹œ ìƒì„±
        category_instructions = {
            "í•™ìŠµ": "êµìœ¡ ë° í•™ìŠµ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì „ë¬¸ê°€ë¡œì„œ, í•™ìŠµìì˜ ì´í•´ë„ë¥¼ ë†’ì´ê³  ë‹¨ê³„ì  í•™ìŠµì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì°½ì‘": "ì°½ì˜ì  ì½˜í…ì¸  ìƒì„± ì „ë¬¸ê°€ë¡œì„œ, ìƒìƒë ¥ì„ ìê·¹í•˜ê³  ë…ì°½ì ì¸ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¶„ì„": "ë°ì´í„° ë¶„ì„ ë° ë…¼ë¦¬ì  ì‚¬ê³  ì „ë¬¸ê°€ë¡œì„œ, ì²´ê³„ì ì´ê³  ê°ê´€ì ì¸ ë¶„ì„ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë²ˆì—­": "ë‹¤êµ­ì–´ ë²ˆì—­ ì „ë¬¸ê°€ë¡œì„œ, ë¬¸ë§¥ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ ì •í™•íˆ ì „ë‹¬í•˜ëŠ” ë²ˆì—­ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì½”ë”©": "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì „ë¬¸ê°€ë¡œì„œ, íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì½”ë“œ ì‘ì„±ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ë° ì˜ì‚¬ê²°ì • ì „ë¬¸ê°€ë¡œì„œ, ì‹¤ìš©ì ì´ê³  ê²°ê³¼ ì§€í–¥ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì¼ë°˜": "ë²”ìš© í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ì–‘í•œ ìƒí™©ì— ì ìš© ê°€ëŠ¥í•œ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        }
        
        # ìŠ¤íƒ€ì¼ë³„ í†¤ ì„¤ì •
        style_tones = {
            "ì¹œê·¼í•œ": "ì¹œê·¼í•˜ê³  ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ í†¤ìœ¼ë¡œ, ì‚¬ìš©ìì™€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ ë„",
            "ì „ë¬¸ì ": "ì „ë¬¸ì ì´ê³  ì •í™•í•œ í†¤ìœ¼ë¡œ, ì‹ ë¢°ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ì œê³µ",
            "ì°½ì˜ì ": "ì°½ì˜ì ì´ê³  ì˜ê°ì„ ì£¼ëŠ” í†¤ìœ¼ë¡œ, í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¥¼ ìê·¹",
            "ê°„ê²°í•œ": "ëª…í™•í•˜ê³  ê°„ê²°í•œ í†¤ìœ¼ë¡œ, íš¨ìœ¨ì ì¸ ì†Œí†µì„ ì¶”êµ¬"
        }
        
        # ë³µì¡ë„ë³„ ì ‘ê·¼ ë°©ì‹
        complexity_approaches = {
            "ê°„ë‹¨": "ì´ˆë³´ìë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì¸ ì ‘ê·¼",
            "ì¤‘ê°„": "ê¸°ë³¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ê· í˜•ì¡íŒ ì ‘ê·¼",
            "ê³ ê¸‰": "ì „ë¬¸ì  ì§€ì‹ê³¼ ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ìš”êµ¬í•˜ëŠ” ê³ ê¸‰ ì ‘ê·¼"
        }
        
        # ì¶œë ¥ í˜•ì‹ë³„ êµ¬ì¡°
        format_structures = {
            "ììœ í˜•ì‹": "ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ìœ ì—°í•œ ì‘ë‹µ êµ¬ì¡°",
            "ë‹¨ê³„ë³„": "1ë‹¨ê³„, 2ë‹¨ê³„ ë“± ìˆœì°¨ì  ë‹¨ê³„ë³„ ì‘ë‹µ êµ¬ì¡°",
            "í‘œí˜•ì‹": "í‘œë‚˜ ì°¨íŠ¸ í˜•íƒœë¡œ ì •ë¦¬ëœ ì²´ê³„ì  ì‘ë‹µ êµ¬ì¡°",
            "ë¦¬ìŠ¤íŠ¸": "ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ ëª©ë¡ í˜•íƒœì˜ ëª…í™•í•œ ì‘ë‹µ êµ¬ì¡°"
        }
        
        system_instruction = f"""
        ë‹¹ì‹ ì€ {category_instructions.get(category, category_instructions["ì¼ë°˜"])}
        
        í”„ë¡¬í”„íŠ¸ ìƒì„± ì›ì¹™:
        1. {style_tones.get(style, style_tones["ì¹œê·¼í•œ"])}
        2. {complexity_approaches.get(complexity, complexity_approaches["ì¤‘ê°„"])}
        3. {format_structures.get(output_format, format_structures["ììœ í˜•ì‹"])}
        4. ëª…í™•í•œ ì§€ì‹œì‚¬í•­ê³¼ êµ¬ì²´ì ì¸ ê¸°ëŒ€ ê²°ê³¼ë¥¼ í¬í•¨
        5. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ìµœì ì˜ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        ì‘ë‹µ í˜•ì‹:
        - í”„ë¡¬í”„íŠ¸ ì œëª© (ê°„ê²°í•˜ê³  ëª©ì ì´ ëª…í™•)
        - ë©”ì¸ í”„ë¡¬í”„íŠ¸ (ì‹¤ì œ ì‚¬ìš©í•  ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸)
        - ì‚¬ìš© íŒ (íš¨ê³¼ì ì¸ ì‚¬ìš© ë°©ë²•)
        - ë³€í˜• ì œì•ˆ (ìƒí™©ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ë³€í˜• ë°©ë²•)
        """
        
        user_request = f"""
        ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        ğŸ“‹ **ê¸°ë³¸ ì •ë³´**
        - ì¹´í…Œê³ ë¦¬: {category}
        - ì‘ì—… ì„¤ëª…: {task_description}
        - ìŠ¤íƒ€ì¼: {style}
        - ë³µì¡ë„: {complexity}
        - ì¶œë ¥ í˜•ì‹: {output_format}
        
        ğŸ“Œ **ì¶”ê°€ ìš”êµ¬ì‚¬í•­**
        - ì˜ˆì‹œ í¬í•¨: {'ì˜ˆ' if include_examples else 'ì•„ë‹ˆì˜¤'}
        - ì œì•½ì‚¬í•­ í¬í•¨: {'ì˜ˆ' if include_constraints else 'ì•„ë‹ˆì˜¤'}
        
        ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ ë°”ë¡œ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì™„ì„±ëœ í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[user_request],
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.8,  # ì°½ì˜ì„±ì„ ìœ„í•´ ì˜¨ë„ ì¡°ê¸ˆ ìƒìŠ¹
                max_output_tokens=3000,  # ë” ìì„¸í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•´ í† í° ì¦ê°€
            )
        )
        
        return {
            "generated_prompt": response.text,
            "category": category,
            "task_description": task_description,
            "style": style,
            "complexity": complexity,
            "output_format": output_format,
            "settings": {
                "include_examples": include_examples,
                "include_constraints": include_constraints
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate prompt: {str(e)}")

# ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
@router.post("/search")
async def search_web(
    request: Request,
    query: str = Form(...),
    room_id: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Google ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ (ìŠ¤íŠ¸ë¦¬ë°)"""
    try:
        # ì‚¬ìš©ì ê²€ìƒ‰ ì§ˆë¬¸ì„ DBì— ì €ì¥
        user_message = ChatMessageCreate(
            content=f"ğŸ” ê²€ìƒ‰: {query}",
            role="user",
            room_id=room_id
        )
        crud_chat.create_message(db, room_id, user_message)
        
        async def generate_search_stream():
            try:
                client = get_gemini_client()
                if not client:
                    yield f"data: {json.dumps({'error': 'Gemini client not available'})}\n\n"
                    return
                
                # ê²€ìƒ‰ ë„êµ¬ ì„¤ì • (ìµœì‹  API ë°©ì‹)
                tools = [types.Tool(google_search=types.GoogleSearch())]
                
                # ê²€ìƒ‰ì„ ìœ„í•œ ì‹œìŠ¤í…œ ì§€ì‹œ
                system_instruction = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬ì— ëŒ€í•´ ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
                ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³  ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”."""
                
                # ìƒì„± ì„¤ì •
                generation_config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.7,
                    max_output_tokens=2048,
                    tools=tools
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì‹¤í–‰
                response = client.models.generate_content_stream(
                    model="gemini-2.5-flash",
                    contents=[f"ë‹¤ìŒì— ëŒ€í•´ ê²€ìƒ‰í•´ì£¼ì„¸ìš”: {query}"],
                    config=generation_config
                )
                
                accumulated_content = ""
                citations = []
                web_search_queries = []
                citations_sent = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ set
                search_completed = False  # ê²€ìƒ‰ ì™„ë£Œ ì—¬ë¶€ ì²´í¬
                is_disconnected = False  # ì—°ê²° ì¤‘ë‹¨ í”Œë˜ê·¸ ì¶”ê°€
                
                for chunk in response:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
                    if await request.is_disconnected():
                        is_disconnected = True
                        logger.warning("Client disconnected during search. Stopping stream.")
                        break
                    if chunk.candidates and len(chunk.candidates) > 0:
                        candidate = chunk.candidates[0]
                        
                        # ì½˜í…ì¸  ì²˜ë¦¬
                        if candidate.content and candidate.content.parts:
                            for part in candidate.content.parts:
                                if part.text:
                                    accumulated_content += part.text
                                    try:
                                        yield f"data: {json.dumps({'content': part.text})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        logger.warning("Client disconnected during search content streaming")
                                        return
                        
                        # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (ìµœì‹  API êµ¬ì¡°)
                        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                            grounding = candidate.grounding_metadata
                            
                            # ë””ë²„ê¹…: grounding metadata êµ¬ì¡° í™•ì¸ (ê²€ìƒ‰ìš©)
                            logger.debug("=== SEARCH GROUNDING METADATA DEBUG ===")
                            logger.debug(f"grounding type: {type(grounding)}")
                            logger.debug(f"grounding dir: {dir(grounding)}")
                            
                            # ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜ì§‘
                            if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                                logger.debug(f"Found web_search_queries: {grounding.web_search_queries}")
                                web_search_queries.extend(grounding.web_search_queries)
                            
                            # grounding chunksì—ì„œ citations ì¶”ì¶œ
                            if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                                logger.debug(f"Found grounding_chunks: {len(grounding.grounding_chunks)} chunks")
                                new_citations = []
                                for i, chunk in enumerate(grounding.grounding_chunks):
                                    logger.debug(f"Search Chunk {i}: type={type(chunk)}, dir={dir(chunk)}")
                                    if hasattr(chunk, 'web') and chunk.web:
                                        logger.debug(f"Search Chunk {i} web: type={type(chunk.web)}, dir={dir(chunk.web)}")
                                        citation_url = chunk.web.uri
                                        
                                        # Mixed Content ë¬¸ì œ ë°©ì§€: HTTP URLì„ HTTPSë¡œ ë³€í™˜
                                        if citation_url.startswith('http://'):
                                            citation_url = citation_url.replace('http://', 'https://', 1)
                                        
                                        # ì¤‘ë³µ ë°©ì§€
                                        if citation_url not in citations_sent:
                                            citation = {
                                                "url": citation_url,
                                                "title": chunk.web.title if hasattr(chunk.web, 'title') else ""
                                            }
                                            logger.debug(f"Search extracted citation: {citation}")
                                            citations.append(citation)
                                            new_citations.append(citation)
                                            citations_sent.add(citation_url)
                                
                                # ìƒˆë¡œìš´ ì¸ìš© ì •ë³´ë§Œ ì „ì†¡
                                if new_citations:
                                    logger.debug(f"Search sending {len(new_citations)} new citations")
                                    try:
                                        yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        logger.warning("Client disconnected during search citations streaming")
                                        return
                
                # ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if is_disconnected:
                    logger.info("Skipping search post-processing due to client disconnection.")
                    return
                
                # ê²€ìƒ‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨
                search_completed = True
                
                # ìµœì¢… ë©”íƒ€ë°ì´í„° ì „ì†¡
                if web_search_queries:
                    try:
                        yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                        logger.warning("Client disconnected during final search queries streaming")
                        return
                
            except Exception as e:
                search_completed = False  # ì—ëŸ¬ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
                try:
                    yield f"data: {json.dumps({'error': f'Search failed: {str(e)}'})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    logger.warning("Client disconnected during search error streaming")
                    return
            
            # ê²€ìƒ‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥
            if search_completed and accumulated_content:
                logger.debug("=== SEARCH SAVING DEBUG ===")
                logger.debug(f"search_completed: {search_completed}")
                logger.debug(f"Saving search response with {len(citations)} citations: {citations}")
                ai_message = ChatMessageCreate(
                    content=accumulated_content,
                    role="assistant",
                    room_id=room_id,
                    citations=citations if citations else None
                )
                saved_message = crud_chat.create_message(db, room_id, ai_message)
                logger.debug(f"Saved message citations: {saved_message.citations}")
                logger.debug("=== END SEARCH SAVING DEBUG ===")
            else:
                logger.info("=== SEARCH MESSAGE NOT SAVED ===")
                logger.info(f"search_completed: {search_completed}")
                logger.info(f"accumulated_content: {bool(accumulated_content)}")
                logger.info(f"Reason: {'Search was interrupted' if not search_completed else 'No content'}")
                logger.info("=== END SEARCH NOT SAVED DEBUG ===")
        
        return StreamingResponse(
            generate_search_stream(),
            media_type="text/plain"
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

async def get_optimized_thinking_config(
    model: str, 
    request_type: str = "normal",
    user_preference: Optional[str] = None
) -> Optional[types.ThinkingConfig]:
    """ì ì‘í˜• ì‚¬ê³  ì„¤ì • ìƒì„±"""
    if model not in THINKING_OPTIMIZATION:
        return None
    
    config = THINKING_OPTIMIZATION[model]
    
    # ì‚¬ìš©ì ì„ í˜¸ë„ ê¸°ë°˜ ì¡°ì •
    if user_preference == "fast":
        budget = 0  # ë¹ ë¥¸ ì‘ë‹µ
    elif user_preference == "quality":
        budget = config["max_budget"]  # ìµœê³  í’ˆì§ˆ
    else:
        # ì ì‘í˜• ì˜ˆì‚° ê³„ì‚°
        if request_type == "simple":
            budget = config["default_budget"]
        elif request_type == "complex":
            budget = config["max_budget"] // 2
        else:
            budget = config["default_budget"]
    
    return types.ThinkingConfig(
        thinking_budget=budget,
        include_thoughts=budget > 0
    )

async def get_or_create_chat_session(
    client, 
    model: str, 
    room_id: str,
    system_instruction: Optional[str] = None
):
    """ì±„íŒ… ì„¸ì…˜ ìºì‹œ ê´€ë¦¬"""
    cache_key = f"{model}:{room_id}"
    
    # ê¸°ì¡´ ì„¸ì…˜ í™•ì¸
    if cache_key in CHAT_SESSION_CACHE:
        session_info = CHAT_SESSION_CACHE[cache_key]
        # ì„¸ì…˜ ë§Œë£Œ í™•ì¸ (1ì‹œê°„)
        if time.time() - session_info["created_at"] < 3600:
            return session_info["session"]
    
    # ìƒˆ ì„¸ì…˜ ìƒì„±
    try:
        chat_session = client.chats.create(
            model=model,
            config=types.GenerateContentConfig(
                system_instruction=system_instruction
            ) if system_instruction else None
        )
        
        CHAT_SESSION_CACHE[cache_key] = {
            "session": chat_session,
            "created_at": time.time()
        }
        
        return chat_session
    except Exception as e:
        logger.error(f"Chat session creation error: {e}", exc_info=True)
        return None

async def compress_context_if_needed(
    client,
    model: str,
    messages: List[dict],
    max_tokens: int
) -> List[dict]:
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (í•„ìš”í•œ ê²½ìš°)"""
    # í† í° ìˆ˜ ê³„ì‚°
    total_tokens = 0
    for msg in messages:
        token_count = await count_gemini_tokens(msg["content"], model, client)
        total_tokens += token_count.get("input_tokens", 0)
    
    # ì••ì¶•ì´ í•„ìš”í•œì§€ í™•ì¸
    if total_tokens < max_tokens * CONTEXT_COMPRESSION_THRESHOLD:
        return messages
    
    logger.info(f"Context compression needed: {total_tokens} tokens > {max_tokens * CONTEXT_COMPRESSION_THRESHOLD}")
    
    # ìµœì‹  ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    keep_recent = 3  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ ìœ ì§€
    recent_messages = messages[-keep_recent:]
    old_messages = messages[:-keep_recent]
    
    if not old_messages:
        return recent_messages
    
    # ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    try:
        summary_content = "\n".join([
            f"{msg['role']}: {msg['content']}" 
            for msg in old_messages
        ])
        
        summary_response = client.models.generate_content(
            model="gemini-2.5-flash",  # ìš”ì•½ì€ ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            contents=[f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš” (í•µì‹¬ ë‚´ìš©ë§Œ):\n\n{summary_content}"],
            config=types.GenerateContentConfig(
                temperature=0.3,
                max_output_tokens=512,
                thinking_config=types.ThinkingConfig(thinking_budget=0)  # ìš”ì•½ì€ ì‚¬ê³  ì—†ì´
            )
        )
        
        # ìš”ì•½ëœ ë©”ì‹œì§€ë¡œ êµì²´
        compressed_messages = [
            {"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_response.text}"}
        ] + recent_messages
        
        logger.info(f"Context compressed: {len(messages)} -> {len(compressed_messages)} messages")
        return compressed_messages
        
    except Exception as e:
        logger.error(f"Context compression error: {e}", exc_info=True)
        return messages[-keep_recent:]  # ì‹¤íŒ¨ì‹œ ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€

class StreamingBuffer:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë²„í¼ë§"""
    def __init__(self, buffer_size: int = STREAMING_BUFFER_SIZE):
        self.buffer = []
        self.buffer_size = buffer_size
        self.current_size = 0
        self.last_flush = time.time()
    
    def add_chunk(self, chunk: str) -> bool:
        """ì²­í¬ ì¶”ê°€, í”ŒëŸ¬ì‹œ í•„ìš”ì‹œ True ë°˜í™˜"""
        self.buffer.append(chunk)
        self.current_size += len(chunk.encode('utf-8'))
        
        # ë²„í¼ê°€ ê°€ë“ ì°¼ê±°ë‚˜ ì¼ì • ì‹œê°„ì´ ì§€ë‚œ ê²½ìš° í”ŒëŸ¬ì‹œ
        now = time.time()
        return (self.current_size >= self.buffer_size or 
                now - self.last_flush >= STREAMING_FLUSH_INTERVAL)
    
    def flush(self) -> str:
        """ë²„í¼ ë‚´ìš© ë°˜í™˜ ë° ì´ˆê¸°í™”"""
        if not self.buffer:
            return ""
        
        content = "".join(self.buffer)
        self.buffer.clear()
        self.current_size = 0
        self.last_flush = time.time()
        return content

# ============================================================================
# ìµëª… ì‚¬ìš©ì ì±„íŒ… ê´€ë ¨ í•¨ìˆ˜ë“¤
# ============================================================================

def get_client_ip(request: Request) -> str:
    """í´ë¼ì´ì–¸íŠ¸ IP ì£¼ì†Œ ì¶”ì¶œ"""
    # X-Forwarded-For í—¤ë” í™•ì¸ (í”„ë¡ì‹œ/ë¡œë“œë°¸ëŸ°ì„œ í™˜ê²½)
    forwarded_for = request.headers.get("X-Forwarded-For")
    if forwarded_for:
        # ì²« ë²ˆì§¸ IP ì£¼ì†Œ ì‚¬ìš© (ì›ë³¸ í´ë¼ì´ì–¸íŠ¸ IP)
        return forwarded_for.split(",")[0].strip()
    
    # X-Real-IP í—¤ë” í™•ì¸ (Nginx ë“±)
    real_ip = request.headers.get("X-Real-IP")
    if real_ip:
        return real_ip.strip()
    
    # ì§ì ‘ ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ IP
    return request.client.host if request.client else "unknown"

def generate_anonymous_session_id() -> str:
    """ìµëª… ì‚¬ìš©ì ì„¸ì…˜ ID ìƒì„±"""
    return str(uuid.uuid4())

async def check_anonymous_rate_limit(
    request: Request, 
    db: Session,
    session_id: str,
    limit: int = 5
) -> tuple[bool, int, str]:
    """
    ìµëª… ì‚¬ìš©ì ì‚¬ìš©ëŸ‰ ì œí•œ í™•ì¸
    
    Returns:
        (is_limit_exceeded, current_usage, ip_address)
    """
    ip_address = get_client_ip(request)
    
    # ì‚¬ìš©ëŸ‰ í™•ì¸
    current_usage = crud_anonymous_usage.get_usage_count(db, session_id, ip_address)
    is_limit_exceeded = crud_anonymous_usage.check_usage_limit(db, session_id, ip_address, limit)
    
    return is_limit_exceeded, current_usage, ip_address

async def generate_anonymous_gemini_stream_response(
    request: Request,
    messages: list,
    model: str,
    session_id: str,
    ip_address: str,
    db: Session
) -> AsyncGenerator[str, None]:
    """ìµëª… ì‚¬ìš©ìë¥¼ ìœ„í•œ ì œí•œëœ Gemini ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    try:
        client = get_gemini_client()
        if not client:
            yield f"data: {json.dumps({'error': 'AI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'})}\n\n"
            return
        
        # ì œí•œëœ ì‹œìŠ¤í…œ ì§€ì‹œ (ê°„ë‹¨í•œ ë‹µë³€ ìœ ë„)
        system_instruction = """ë‹¹ì‹ ì€ Sungblab AI êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
í•™ìŠµìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¹œê·¼í•˜ê³  ëª…í™•í•œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ì€ 500ìë¥¼ ë„˜ì§€ ì•Šë„ë¡ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ë˜, í•µì‹¬ ë‚´ìš©ì€ ë†“ì¹˜ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”."""
        
        # ì œí•œëœ ìƒì„± ì„¤ì • (ë¹„ìš© ì ˆì•½)
        generation_config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            temperature=0.7,
            max_output_tokens=512,  # ìµëª… ì‚¬ìš©ìëŠ” ì§§ì€ ë‹µë³€ë§Œ
            # ìµëª… ì‚¬ìš©ìëŠ” function calling, grounding ë“± ê³ ê¸‰ ê¸°ëŠ¥ ì œí•œ
        )
        
        # ë©”ì‹œì§€ í¬ë§· ë³€í™˜
        formatted_messages = []
        for msg in messages:
            role = "user" if msg["role"] == "user" else "model"
            formatted_messages.append({
                "role": role,
                "parts": [{"text": msg["content"]}]
            })
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        response = client.models.generate_content_stream(
            model=model,
            contents=formatted_messages,
            config=generation_config
        )
        
        accumulated_content = ""
        
        for chunk in response:
            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
            if await request.is_disconnected():
                logger.warning("Anonymous client disconnected. Stopping stream.")
                break
                
            if chunk.candidates and len(chunk.candidates) > 0:
                candidate = chunk.candidates[0]
                
                if candidate.content and candidate.content.parts:
                    for part in candidate.content.parts:
                        if part.text:
                            accumulated_content += part.text
                            try:
                                yield f"data: {json.dumps({'content': part.text})}\n\n"
                            except (ConnectionError, BrokenPipeError, GeneratorExit):
                                logger.warning("Anonymous client disconnected during streaming")
                                return
        
        # ì‚¬ìš©ëŸ‰ ì¦ê°€ (ì„±ê³µì ìœ¼ë¡œ ì‘ë‹µì´ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ)
        if accumulated_content.strip():
            crud_anonymous_usage.increment_usage(db, session_id, ip_address)
            
    except Exception as e:
        logger.error(f"Anonymous chat error: {e}", exc_info=True)
        yield f"data: {json.dumps({'error': 'ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'})}\n\n"


# ============================================================================
# ìµëª… ì‚¬ìš©ì API ì—”ë“œí¬ì¸íŠ¸ë“¤
# ============================================================================

@router.post("/anonymous-chat")
async def anonymous_chat(
    request: Request,
    session_id: str = Form(...),
    message: str = Form(...),
    model: str = Form("gemini-2.5-flash"),  # ìµëª… ì‚¬ìš©ìëŠ” Flash ëª¨ë¸ë§Œ
    db: Session = Depends(get_db)
):
    """ìµëª… ì‚¬ìš©ìë¥¼ ìœ„í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ë¡œê·¸ì¸ ë¶ˆí•„ìš”, 5íšŒ ì œí•œ)"""
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not message.strip():
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        if len(message) > 2000:  # ìµëª… ì‚¬ìš©ìëŠ” ì§§ì€ ë©”ì‹œì§€ë§Œ
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. (ìµœëŒ€ 2000ì)")
        
        # ì„¸ì…˜ ID ê²€ì¦
        try:
            uuid.UUID(session_id)  # ìœ íš¨í•œ UUIDì¸ì§€ í™•ì¸
        except ValueError:
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ì„¸ì…˜ IDì…ë‹ˆë‹¤.")
        
        # ì‚¬ìš©ëŸ‰ ì œí•œ í™•ì¸
        is_limit_exceeded, current_usage, ip_address = await check_anonymous_rate_limit(
            request, db, session_id
        )
        
        if is_limit_exceeded:
            raise HTTPException(
                status_code=429, 
                detail={
                    "message": "ìµëª… ì±„íŒ… íšŸìˆ˜ë¥¼ ëª¨ë‘ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. íšŒì›ê°€ì…í•˜ê³  ë” ë§ì€ ê¸°ëŠ¥ì„ ì´ìš©í•´ë³´ì„¸ìš”!",
                    "current_usage": current_usage,
                    "limit": 5
                }
            )
        
        # í—ˆìš©ëœ ëª¨ë¸ì¸ì§€ í™•ì¸ (ìµëª… ì‚¬ìš©ìëŠ” Flashë§Œ)
        if model not in ["gemini-2.5-flash"]:
            model = "gemini-2.5-flash"
        
        # ê°„ë‹¨í•œ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (ìµœê·¼ 5ê°œë§Œ)
        messages = [
            {"role": "user", "content": message}
        ]
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        async def generate_anonymous_stream():
            async for chunk in generate_anonymous_gemini_stream_response(
                request, messages, model, session_id, ip_address, db
            ):
                yield chunk
        
        return StreamingResponse(
            generate_anonymous_stream(),
            media_type="text/plain",
            headers={
                "X-Anonymous-Usage": str(current_usage + 1),
                "X-Anonymous-Limit": "5"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Anonymous chat error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@router.get("/anonymous-usage/{session_id}")
async def get_anonymous_usage(
    request: Request,
    session_id: str,
    db: Session = Depends(get_db)
):
    """ìµëª… ì‚¬ìš©ìì˜ í˜„ì¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    
    try:
        # ì„¸ì…˜ ID ê²€ì¦
        try:
            uuid.UUID(session_id)
        except ValueError:
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ì„¸ì…˜ IDì…ë‹ˆë‹¤.")
        
        ip_address = get_client_ip(request)
        current_usage = crud_anonymous_usage.get_usage_count(db, session_id, ip_address)
        
        return {
            "session_id": session_id,
            "current_usage": current_usage,
            "limit": 5,
            "remaining": max(0, 5 - current_usage),
            "is_limit_exceeded": current_usage >= 5
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Anonymous usage check error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@router.post("/anonymous-session")
async def create_anonymous_session(request: Request):
    """ìƒˆë¡œìš´ ìµëª… ì„¸ì…˜ ID ìƒì„±"""
    
    try:
        session_id = generate_anonymous_session_id()
        ip_address = get_client_ip(request)
        
        return {
            "session_id": session_id,
            "ip_address": ip_address,  # ë””ë²„ê¹…ìš© (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì œê±°)
            "limit": 5,
            "message": "ìµëª… ì„¸ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 5ë²ˆì˜ ë¬´ë£Œ ì±„íŒ…ì„ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        logger.error(f"Anonymous session creation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


# ê´€ë¦¬ììš© ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒì )
@router.get("/admin/anonymous-stats")
async def get_anonymous_stats(
    date: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ìµëª… ì‚¬ìš©ì í†µê³„ ì¡°íšŒ (ê´€ë¦¬ìë§Œ)"""
    
    # ê´€ë¦¬ì ê¶Œí•œ í™•ì¸ (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
    if not current_user.email.endswith("@admin.com"):  # ì‹¤ì œ ê´€ë¦¬ì ì¡°ê±´ìœ¼ë¡œ ìˆ˜ì •
        raise HTTPException(status_code=403, detail="ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    try:
        target_date = None
        if date:
            target_date = datetime.fromisoformat(date)
            
        stats = crud_anonymous_usage.get_daily_stats(db, target_date)
        return stats
        
    except Exception as e:
        logger.error(f"Anonymous stats error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
