from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form, Request
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from app.schemas.chat import (
    ChatRoom, ChatRoomCreate, ChatRoomList, 
    ChatMessageCreate, ChatMessage, ChatMessageList,
    ChatRequest, TokenUsage, PromptGenerateRequest
)
from app.crud import crud_chat, crud_stats, crud_project, crud_subscription
from app.crud.crud_anonymous_usage import crud_anonymous_usage
from app.db.session import get_db
from app.db.retry_session import get_robust_db, retry_db_operation
from app.core.security import get_current_user
from app.models.user import User
import json
from app.core.config import settings
import logging
import base64
from typing import Optional, List, AsyncGenerator, Dict, Any, Set
import os
from datetime import datetime, timezone
from app.models.subscription import Subscription
import time
import asyncio
import io
import hashlib
import uuid
from typing import Dict, Set
from fastapi import HTTPException

# ìƒˆë¡œìš´ Google Genai ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from google import genai
from google.genai import types

from app.core.models import (
    ACTIVE_MODELS, get_model_config, get_multimodal_models, 
    ALLOWED_MODELS, ModelProvider
)

router = APIRouter()
logger = logging.getLogger(__name__)

# ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
MULTIMODAL_MODELS = get_multimodal_models()

# Gemini ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
GEMINI_MODELS = [
    model_name for model_name, config in ACTIVE_MODELS.items()
    if config.provider == ModelProvider.GOOGLE
]

MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
GEMINI_INLINE_DATA_LIMIT = 10 * 1024 * 1024  # 10MB (Gemini API ì œí•œ)

# ìµœì‹  Gemini ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í•œë„ ì„¤ì • (API ë¬¸ì„œ ê¸°ì¤€)
MODEL_CONTEXT_LIMITS = {
    "gemini-2.5-pro": {
        "total_tokens": 2_000_000,  # 2M í† í°
        "output_reserve": 4096,     # ì¶œë ¥ìš© ì˜ˆì•½
        "system_reserve": 2048,     # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš© ì˜ˆì•½
        "file_reserve": 10000       # íŒŒì¼ìš© ì˜ˆì•½
    },
    "gemini-2.5-flash": {
        "total_tokens": 1_000_000,  # 1M í† í°
        "output_reserve": 2048,     # ì¶œë ¥ìš© ì˜ˆì•½
        "system_reserve": 1024,     # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš© ì˜ˆì•½
        "file_reserve": 5000        # íŒŒì¼ìš© ì˜ˆì•½
    },
    "gemini-2.0-flash": {
        "total_tokens": 1_000_000,  # 1M í† í°
        "output_reserve": 2048,
        "system_reserve": 1024,
        "file_reserve": 5000
    },
    "gemini-1.5-pro": {
        "total_tokens": 2_000_000,  # 2M í† í°
        "output_reserve": 4096,
        "system_reserve": 2048,
        "file_reserve": 10000
    },
    "gemini-1.5-flash": {
        "total_tokens": 1_000_000,  # 1M í† í°
        "output_reserve": 2048,
        "system_reserve": 1024,
        "file_reserve": 5000
    }
}

# ì‚¬ê³  ê¸°ëŠ¥ ìµœì í™” ì„¤ì • ì¶”ê°€
THINKING_OPTIMIZATION = {
    "gemini-2.5-flash": {
        "default_budget": 0,  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
        "max_budget": 24576,
        "adaptive": True  # ìƒí™©ì— ë”°ë¼ ì ì‘í˜• ì‚¬ê³  ì˜ˆì‚°
    },
    "gemini-2.5-pro": {
        "default_budget": 4096,  # ProëŠ” í’ˆì§ˆ ìœ„í•´ ê¸°ë³¸ ì‚¬ê³  ìœ ì§€
        "max_budget": 8192,
        "adaptive": True
    }
}

# ì±„íŒ… ì„¸ì…˜ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
CHAT_SESSION_CACHE = {}

# ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì„¤ì •
STREAMING_BUFFER_SIZE = 1024  # ë°”ì´íŠ¸ ë‹¨ìœ„
STREAMING_FLUSH_INTERVAL = 0.1  # ì´ˆ ë‹¨ìœ„

# ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì„¤ì •
CONTEXT_COMPRESSION_THRESHOLD = 0.8  # ì»¨í…ìŠ¤íŠ¸ 80% ì´ˆê³¼ ì‹œ ì••ì¶•
CONTEXT_SUMMARY_RATIO = 0.3  # ì••ì¶• ì‹œ 30%ë¡œ ìš”ì•½

def generateUniqueId():
    return int(time.time() * 1000)

async def process_file_to_base64(file: UploadFile) -> tuple[str, str]:
    try:
        contents = await file.read()
        
        # íŒŒì¼ì´ Gemini API ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° File API ì‚¬ìš©
        if len(contents) > GEMINI_INLINE_DATA_LIMIT:
            logger.warning(f"File {file.filename} ({len(contents)} bytes) exceeds Gemini inline data limit. Using File API instead.")
            
            # Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = get_gemini_client()
            if not client:
                raise HTTPException(status_code=500, detail="Gemini API í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨")
            
            try:
                # File APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë¡œë“œ
                uploaded_file = client.files.upload(
                    file=io.BytesIO(contents),
                    config=dict(
                        mime_type=file.content_type,
                        display_name=f"chat_{file.filename}"
                    )
                )
                
                # íŒŒì¼ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
                max_wait_time = 30
                wait_time = 0
                while uploaded_file.state.name == 'PROCESSING' and wait_time < max_wait_time:
                    await asyncio.sleep(2)
                    wait_time += 2
                    try:
                        uploaded_file = client.files.get(name=uploaded_file.name)
                    except Exception as e:
                        logger.error(f"Error checking file status: {e}", exc_info=True)
                        break
                
                # ì²˜ë¦¬ ìƒíƒœ í™•ì¸
                if uploaded_file.state.name != 'ACTIVE':
                    logger.warning(f"File {file.filename} is in state {uploaded_file.state.name}")
                
                # File API URI ë°˜í™˜ (base64 ëŒ€ì‹ )
                return f"FILE_API:{uploaded_file.name}", file.content_type
                
            except Exception as e:
                logger.error(f"File API upload failed for {file.filename}: {e}", exc_info=True)
                # í´ë°±: íŒŒì¼ ì •ë³´ë§Œ ì „ì†¡
                file_info = f"íŒŒì¼ëª…: {file.filename}, í¬ê¸°: {len(contents)} bytes, íƒ€ì…: {file.content_type} (File API ì—…ë¡œë“œ ì‹¤íŒ¨)"
                base64_data = base64.b64encode(file_info.encode()).decode('utf-8')
                return base64_data, file.content_type
        else:
            # ì‘ì€ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
            base64_data = base64.b64encode(contents).decode('utf-8')
            return base64_data, file.content_type
            
    except Exception as e:
        raise

def get_gemini_client():
    """ìƒˆë¡œìš´ Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if not settings.GEMINI_API_KEY:
            return None
        
        # ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = genai.Client(api_key=settings.GEMINI_API_KEY)
        return client
    except Exception as e:
        logger.error(f"Gemini client creation error: {e}", exc_info=True)
        return None

def count_tokens_with_tiktoken(text: str, model: str = "gpt-4") -> dict:
    """tiktokenì„ ì‚¬ìš©í•œ ì •í™•í•œ í† í° ê³„ì‚°"""
    import tiktoken
    
    try:
        # Gemini ëª¨ë¸ì€ OpenAIì™€ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, 
        # tiktokenì˜ cl100k_baseëŠ” ë¹„êµì  ì •í™•í•œ ì¶”ì •ì„ ì œê³µ
        if "gemini" in model.lower():
            # Geminiìš©ìœ¼ë¡œ cl100k_base ì‚¬ìš© (GPT-4ì™€ ìœ ì‚¬í•œ í† í°í™”)
            encoding_name = "cl100k_base"
        else:
            # ê¸°íƒ€ ëª¨ë¸ìš© ê¸°ë³¸ê°’
            encoding_name = "cl100k_base"
            
        encoding = tiktoken.get_encoding(encoding_name)
        token_count = len(encoding.encode(text))
        
        return {
            "input_tokens": token_count,
            "output_tokens": 0,
            "method": "tiktoken",
            "encoding": encoding_name
        }
    except Exception as e:
        logger.warning(f"tiktoken calculation failed: {e}, using fallback")
        return fallback_token_calculation(text)

def fallback_token_calculation(text: str) -> dict:
    """tiktoken ì‹¤íŒ¨ ì‹œ fallback ê³„ì‚°"""
    import re
    
    # í•œêµ­ì–´/ì˜ì–´ í˜¼í•© í…ìŠ¤íŠ¸ ì •í™•í•œ ì¶”ì •
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    numbers_symbols = len(re.findall(r'[0-9\s\.,;:!?\-\(\)\[\]{}]', text))
    other_chars = len(text) - korean_chars - english_chars - numbers_symbols
    
    # í•œêµ­ì–´ 1.3ì/í† í°, ì˜ì–´ 3.5ì/í† í° (tiktoken ê¸°ì¤€ ì¡°ì •)
    estimated_tokens = (
        korean_chars / 1.3 + 
        english_chars / 3.5 + 
        numbers_symbols / 2.5 + 
        other_chars / 2
    )
    
    return {
        "input_tokens": max(1, int(estimated_tokens)),
        "output_tokens": 0,
        "method": "fallback",
        "encoding": "estimated"
    }

# ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì„¤ì •
MAX_MESSAGES_WINDOW = 15  # ìµœëŒ€ ë©”ì‹œì§€ ê°œìˆ˜
SUMMARY_TRIGGER_THRESHOLD = 12  # ìš”ì•½ íŠ¸ë¦¬ê±° ë©”ì‹œì§€ ê°œìˆ˜
CONTEXT_SUMMARY_CACHE = {}  # ìš”ì•½ ìºì‹œ

def get_dynamic_context_limit(model: str, system_tokens: int = 0, file_tokens: int = 0) -> int:
    """ëª¨ë¸ë³„ ë™ì  ì»¨í…ìŠ¤íŠ¸ í•œë„ ê³„ì‚° (ìµœì‹  API ê¸°ì¤€)"""
    # ê¸°ë³¸ê°’ ì„¤ì • (í˜¸í™˜ì„±ì„ ìœ„í•´)
    default_config = {
        "total_tokens": 1_000_000,
        "output_reserve": 2048,
        "system_reserve": 1024,
        "file_reserve": 5000
    }
    
    config = MODEL_CONTEXT_LIMITS.get(model, default_config)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ ê³„ì‚°
    available_tokens = (
        config["total_tokens"] 
        - config["output_reserve"] 
        - system_tokens 
        - file_tokens
    )
    
    # ìµœì†Œ í•œë„ ë³´ì¥ (ë„ˆë¬´ ì‘ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    min_context = 10000
    if available_tokens < min_context:
        logger.warning(f"Calculated context too small ({available_tokens}), using minimum: {min_context}")
        return min_context
    
    logger.info(f"Dynamic context limit for {model}: {available_tokens} tokens")
    return available_tokens

async def summarize_old_messages(client, model: str, messages: list, target_length: int = 3) -> str:
    """ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì ˆì•½"""
    if len(messages) <= target_length:
        return ""
    
    # ìš”ì•½í•  ë©”ì‹œì§€ë“¤ (ìµœì‹  3ê°œ ì œì™¸)
    messages_to_summarize = messages[:-target_length]
    
    # ìš”ì•½ ë‚´ìš© êµ¬ì„±
    summary_content = ""
    for msg in messages_to_summarize:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
        summary_content += f"{role}: {msg['content'][:200]}{'...' if len(msg['content']) > 200 else ''}\n"
    
    # ìš”ì•½ ìƒì„± í”„ë¡¬í”„íŠ¸
    summary_prompt = f"""ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ í•µì‹¬ë§Œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš” (200ì ì´ë‚´):

{summary_content}

ìš”ì•½ í˜•ì‹: "ì‚¬ìš©ìê°€ [ì£¼ì œ]ì— ëŒ€í•´ ì§ˆë¬¸í–ˆê³ , AIê°€ [ë‹µë³€ ìš”ì•½]ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤."""
    
    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",  # ìš”ì•½ì€ ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            contents=[summary_prompt],
            config=types.GenerateContentConfig(
                temperature=0.3,
                max_output_tokens=300,
                thinking_config=types.ThinkingConfig(thinking_budget=0)  # ìš”ì•½ì€ ì‚¬ê³  ì—†ì´
            )
        )
        
        summary = response.text.strip()
        logger.info(f"Generated conversation summary: {summary[:100]}...")
        return summary
        
    except Exception as e:
        logger.error(f"Failed to generate summary: {e}")
        # Fallback: ê°„ë‹¨í•œ ìš”ì•½
        return f"ì´ì „ ëŒ€í™” ({len(messages_to_summarize)}ê°œ ë©”ì‹œì§€): ì£¼ìš” ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤"

def apply_sliding_window_with_summary(messages: list, room_id: str, max_messages: int = MAX_MESSAGES_WINDOW) -> list:
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ì™€ ìš”ì•½ì„ ì ìš©í•œ ë©”ì‹œì§€ ê´€ë¦¬"""
    
    if len(messages) <= max_messages:
        return messages
    
    # room_id ìœ íš¨ì„± ê²€ì‚¬
    if not room_id or room_id.strip() == "":
        # room_idê°€ ì—†ìœ¼ë©´ ê°„ë‹¨íˆ ìµœì‹  ë©”ì‹œì§€ë§Œ ë°˜í™˜
        logger.warning("No valid room_id provided, using simple sliding window")
        return messages[-max_messages:]
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = f"{room_id}_{len(messages)}"
    
    # ìµœì‹  ë©”ì‹œì§€ë“¤ ìœ ì§€
    recent_messages = messages[-max_messages:]
    
    # ìš”ì•½ì´ í•„ìš”í•œ ê²½ìš°
    if len(messages) > SUMMARY_TRIGGER_THRESHOLD:
        # ìš”ì•½ ìºì‹œ í™•ì¸
        if cache_key not in CONTEXT_SUMMARY_CACHE:
            # ìš”ì•½ ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¼ë‹¨ ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©)
            old_messages = messages[:-max_messages]
            summary_text = f"[ì´ì „ ëŒ€í™” ìš”ì•½: {len(old_messages)}ê°œ ë©”ì‹œì§€]"
            CONTEXT_SUMMARY_CACHE[cache_key] = summary_text
        
        # ìš”ì•½ì„ ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¡œ ì¶”ê°€
        summary_message = {
            "role": "system",
            "content": CONTEXT_SUMMARY_CACHE[cache_key],
            "created_at": messages[0].get("created_at", ""),
            "updated_at": messages[0].get("updated_at", ""),
            "is_summary": True
        }
        
        return [summary_message] + recent_messages
    
    return recent_messages

def intelligent_message_selection(messages: list, max_tokens: int, model: str) -> list:
    """ì§€ëŠ¥í˜• ë©”ì‹œì§€ ì„ ë³„ (ì¤‘ìš”ë„ + í† í° ê¸°ë°˜)"""
    if not messages:
        return []
    
    selected_messages = []
    current_tokens = 0
    
    # ìµœì‹  3ê°œ ë©”ì‹œì§€ëŠ” ë¬´ì¡°ê±´ í¬í•¨
    recent_count = min(3, len(messages))
    recent_messages = messages[-recent_count:]
    
    for msg in recent_messages:
        token_info = count_tokens_with_tiktoken(msg.get("content", ""), model)
        current_tokens += token_info["input_tokens"]
    
    # ë‚¨ì€ í† í° ì˜ˆì‚°ìœ¼ë¡œ ë‚˜ë¨¸ì§€ ë©”ì‹œì§€ ì„ ë³„
    remaining_messages = messages[:-recent_count] if len(messages) > recent_count else []
    remaining_budget = max_tokens - current_tokens
    
    # ì¤‘ìš”ë„ ê¸°ë°˜ ì •ë ¬
    scored_messages = []
    for msg in remaining_messages:
        content = msg.get("content", "")
        score = calculate_message_importance(content, msg.get("role", ""))
        token_info = count_tokens_with_tiktoken(content, model)
        
        scored_messages.append({
            "message": msg,
            "score": score,
            "tokens": token_info["input_tokens"]
        })
    
    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    scored_messages.sort(key=lambda x: x["score"], reverse=True)
    
    # í† í° ì˜ˆì‚° ë‚´ì—ì„œ ì„ ë³„
    for item in scored_messages:
        if current_tokens + item["tokens"] <= remaining_budget:
            selected_messages.append(item["message"])
            current_tokens += item["tokens"]
    
    # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
    selected_messages.sort(key=lambda x: x.get("created_at", ""))
    
    return selected_messages + recent_messages

def calculate_message_importance(content: str, role: str) -> float:
    """ë©”ì‹œì§€ ì¤‘ìš”ë„ ê³„ì‚°"""
    if not content:
        return 0.0
    
    score = 0.0
    
    # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ì ë‹¹í•œ ê¸¸ì´ê°€ ì¢‹ìŒ)
    length = len(content)
    if 20 <= length <= 500:
        score += 1.0
    elif length > 500:
        score += 0.7  # ë„ˆë¬´ ê¸´ ë©”ì‹œì§€ëŠ” ì•½ê°„ ê°ì 
    else:
        score += 0.3  # ë„ˆë¬´ ì§§ì€ ë©”ì‹œì§€ëŠ” ê°ì 
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
    important_keywords = [
        "ì§ˆë¬¸", "ë¬¸ì œ", "ì˜¤ë¥˜", "ì—ëŸ¬", "ë„ì›€", "ì„¤ëª…", "ë°©ë²•", 
        "ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡", "ì¤‘ìš”", "í•µì‹¬", "ìš”ì•½", "ê²°ë¡ "
    ]
    keyword_score = sum(0.2 for keyword in important_keywords if keyword in content)
    score += min(keyword_score, 1.5)  # ìµœëŒ€ 1.5ì 
    
    # ì—­í•  ê¸°ë°˜ ì ìˆ˜
    if role == "user":
        score *= 1.3  # ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ë” ì¤‘ìš”
    
    # íŠ¹ìˆ˜ ë¬¸ì ì ìˆ˜ (ì§ˆë¬¸í‘œ, ì½”ë“œ ë¸”ë¡ ë“±)
    if "?" in content or "```" in content:
        score += 0.3
    
    return score

# í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_weather_function():
    """ë‚ ì”¨ ì •ë³´ ì¡°íšŒ í•¨ìˆ˜"""
    return {
        "name": "get_weather",
        "description": "ì§€ì •ëœ ìœ„ì¹˜ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "ë‚ ì”¨ë¥¼ ì¡°íšŒí•  ë„ì‹œ ì´ë¦„ (ì˜ˆ: ì„œìš¸, ë¶€ì‚°)",
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "ì˜¨ë„ ë‹¨ìœ„ (ê¸°ë³¸ê°’: celsius)",
                },
            },
            "required": ["location"],
        },
    }

def create_calculator_function():
    """ê³„ì‚°ê¸° í•¨ìˆ˜"""
    return {
        "name": "calculate",
        "description": "ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {
                    "type": "string",
                    "description": "ê³„ì‚°í•  ìˆ˜í•™ í‘œí˜„ì‹ (ì˜ˆ: 2+3*4, sqrt(16))",
                },
            },
            "required": ["expression"],
        },
    }

def create_search_function():
    """ê²€ìƒ‰ í•¨ìˆ˜"""
    return {
        "name": "search_knowledge",
        "description": "ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸",
                },
                "category": {
                    "type": "string",
                    "enum": ["academic", "general", "technical"],
                    "description": "ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬",
                },
            },
            "required": ["query"],
        },
    }

async def execute_function_call(function_name: str, arguments: dict) -> dict:
    """í•¨ìˆ˜ í˜¸ì¶œ ì‹¤í–‰"""
    try:
        if function_name == "get_weather":
            location = arguments.get("location", "")
            unit = arguments.get("unit", "celsius")
            # ì‹¤ì œ ë‚ ì”¨ API í˜¸ì¶œ ëŒ€ì‹  ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            return {
                "location": location,
                "temperature": 22 if unit == "celsius" else 72,
                "unit": unit,
                "description": "ë§‘ìŒ",
                "humidity": 65
            }
        
        elif function_name == "calculate":
            expression = arguments.get("expression", "")
            try:
                # ì•ˆì „í•œ ìˆ˜í•™ ê³„ì‚°
                import math
                import re
                
                # í—ˆìš©ëœ í•¨ìˆ˜ë“¤ë§Œ ì‚¬ìš©
                allowed_names = {
                    "sqrt": math.sqrt,
                    "sin": math.sin,
                    "cos": math.cos,
                    "tan": math.tan,
                    "log": math.log,
                    "exp": math.exp,
                    "pow": pow,
                    "abs": abs,
                    "round": round,
                    "pi": math.pi,
                    "e": math.e
                }
                
                # ì•ˆì „í•œ í‘œí˜„ì‹ì¸ì§€ í™•ì¸
                if re.search(r'[a-zA-Z_]', expression):
                    # í•¨ìˆ˜ëª…ì´ í¬í•¨ëœ ê²½ìš° í—ˆìš©ëœ í•¨ìˆ˜ì¸ì§€ í™•ì¸
                    for name in allowed_names.keys():
                        expression = expression.replace(name, f"allowed_names['{name}']")
                
                result = eval(expression, {"__builtins__": {}, "allowed_names": allowed_names})
                return {"result": result, "expression": arguments.get("expression", "")}
            except Exception as e:
                return {"error": f"ê³„ì‚° ì˜¤ë¥˜: {str(e)}", "expression": arguments.get("expression", "")}
        
        elif function_name == "search_knowledge":
            query = arguments.get("query", "")
            category = arguments.get("category", "general")
            # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ì‹  ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            return {
                "query": query,
                "category": category,
                "results": [
                    {"title": f"{query}ì— ëŒ€í•œ ì •ë³´", "content": f"{category} ì¹´í…Œê³ ë¦¬ì—ì„œ {query}ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."}
                ]
            }
        
        else:
            return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {function_name}"}
    
    except Exception as e:
        return {"error": f"í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"}

async def generate_gemini_stream_response(
    request: Request,
    messages: list,
    model: str,
    room_id: str,
    db: Session,
    user_id: str,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    file_names: Optional[List[str]] = None,
    enable_thinking: bool = True,
    enable_grounding: bool = True,
    enable_code_execution: bool = True,
    thinking_budget: int = 8192
) -> AsyncGenerator[str, None]:
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")

        config = get_model_config(model)
        if not config or config.provider != ModelProvider.GOOGLE:
            raise HTTPException(status_code=400, detail="Invalid Gemini model")

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not messages or len(messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )

        # ğŸš€ ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
        # 1ë‹¨ê³„: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš© (ìµœëŒ€ 15ê°œ ë©”ì‹œì§€)
        messages = apply_sliding_window_with_summary(messages, room_id)
        
        logger.info(f"After sliding window: {len(messages)} messages")
        
        # 2ë‹¨ê³„: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í° ê³„ì‚° (tiktoken ì‚¬ìš©)
        system_tokens = 0
        if config.system_prompt:
            system_token_info = count_tokens_with_tiktoken(config.system_prompt, model)
            system_tokens = system_token_info.get("input_tokens", 0)
        
        # 3ë‹¨ê³„: íŒŒì¼ í† í° ê³„ì‚°
        file_tokens = 0
        if file_data_list and file_types:
            for file_type in file_types:
                if file_type.startswith("image/"):
                    file_tokens += 258  # Gemini 2.5 ê¸°ì¤€: ì´ë¯¸ì§€ë‹¹ 258 í† í°
                elif file_type == "application/pdf":
                    file_tokens += 258 * 10  # ì˜ˆìƒ í˜ì´ì§€ ìˆ˜ * 258 í† í°
                elif file_type.startswith("video/"):
                    file_tokens += 263 * 60  # ì˜ˆìƒ 1ë¶„ * 263 í† í°/ì´ˆ
                elif file_type.startswith("audio/"):
                    file_tokens += 32 * 60   # ì˜ˆìƒ 1ë¶„ * 32 í† í°/ì´ˆ
        
        # 4ë‹¨ê³„: ë™ì  ì»¨í…ìŠ¤íŠ¸ í•œë„ ê³„ì‚°
        MAX_CONTEXT_TOKENS = get_dynamic_context_limit(model, system_tokens, file_tokens)
        
        # 5ë‹¨ê³„: ì§€ëŠ¥í˜• ë©”ì‹œì§€ ì„ ë³„ (tiktoken ê¸°ë°˜)
        available_tokens = MAX_CONTEXT_TOKENS - system_tokens - file_tokens
        valid_messages = intelligent_message_selection(messages, available_tokens, model)
        
        # ìµœì¢… í† í° ê³„ì‚°
        total_tokens = system_tokens + file_tokens
        for msg in valid_messages:
            if msg.get("content") and msg["content"].strip():
                token_info = count_tokens_with_tiktoken(msg["content"], model)
                total_tokens += token_info["input_tokens"]
        
        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ë©”ì‹œì§€ëŠ” í¬í•¨ë˜ì–´ì•¼ í•¨
        if len(valid_messages) == 0 and len(messages) > 0:
            last_msg = messages[-1]
            if last_msg.get("content") and last_msg["content"].strip():
                valid_messages = [last_msg]
        
        if len(valid_messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="No valid message content found"
            )
        
        logger.info(f"Context management: Using {len(valid_messages)} messages with {total_tokens} tokens")

        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì ìš© (í•„ìš”í•œ ê²½ìš°)
        if len(valid_messages) > 5:  # 5ê°œ ì´ìƒ ë©”ì‹œì§€ê°€ ìˆì„ ë•Œë§Œ ì••ì¶• ê³ ë ¤
            valid_messages = await compress_context_if_needed(
                client=client,
                model=model,
                messages=valid_messages,
                max_tokens=MAX_CONTEXT_TOKENS
            )

        # ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ë©€í‹°ëª¨ë‹¬ ì»¨í…ì¸  ìƒì„±
        if file_data_list and file_types and file_names:
            for file_data, file_type, file_name in zip(file_data_list, file_types, file_names):
                # File APIë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì¸ì§€ í™•ì¸
                if file_data.startswith("FILE_API:"):
                    # File API URIì—ì„œ íŒŒì¼ ì´ë¦„ ì¶”ì¶œ
                    file_uri = file_data.replace("FILE_API:", "")
                    try:
                        # File API ê°ì²´ë¡œ ì§ì ‘ ì¶”ê°€
                        uploaded_file = client.files.get(name=file_uri)
                        contents.append(uploaded_file)
                        logger.info(f"Added File API file: {file_name} ({file_uri})")
                    except Exception as e:
                        logger.error(f"Failed to get File API file {file_uri}: {e}", exc_info=True)
                        # í´ë°±: íŒŒì¼ ì •ë³´ í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                        contents.append(f"íŒŒì¼: {file_name} (File API ì²˜ë¦¬ ì‹¤íŒ¨)")
                else:
                    # ê¸°ì¡´ base64 ë°©ì‹ ì²˜ë¦¬
                    if file_type.startswith("image/"):
                        contents.append(
                            types.Part.from_bytes(
                                data=base64.b64decode(file_data),
                                mime_type=file_type
                            )
                        )
                    elif file_type == "application/pdf":
                        # PDF íŒŒì¼ ì²˜ë¦¬
                        pdf_data = base64.b64decode(file_data)
                        contents.append(
                            types.Part.from_bytes(
                                data=pdf_data,
                                mime_type=file_type
                            )
                        )

        # ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation_text = ""
        for message in valid_messages:
            role_text = "Human" if message["role"] == "user" else "Assistant"
            conversation_text += f"{role_text}: {message['content']}\n"

        contents.append(conversation_text)

        # ë„êµ¬ ì„¤ì • - ìµœì‹  API êµ¬ì¡° ì‚¬ìš©
        tools = []
        
        if enable_grounding:
            # Google ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì¶”ê°€ (ìµœì‹  API ë°©ì‹)
            tools.append(types.Tool(google_search=types.GoogleSearch()))
        
        if enable_code_execution:
            # ì½”ë“œ ì‹¤í–‰ ì¶”ê°€
            tools.append(types.Tool(code_execution=types.ToolCodeExecution()))

        # ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬ (ì»¨í…ìŠ¤íŠ¸ ìºì‹± ëŒ€ì²´)
        chat_session = None
        if room_id:
            chat_session = await get_or_create_chat_session(
                client=client,
                model=model,
                room_id=room_id,
                system_instruction=config.system_prompt
            )

        # ìƒì„± ì„¤ì • (ìµœì í™”ëœ êµ¬ì¡°)
        generation_config = types.GenerateContentConfig(
            system_instruction=config.system_prompt,
            temperature=config.temperature,
            top_p=config.top_p,
            max_output_tokens=config.max_tokens,
            tools=tools if tools else None
        )

        # ìµœì í™”ëœ ì‚¬ê³  ê¸°ëŠ¥ ì„¤ì •
        if enable_thinking:
            optimized_thinking_config = await get_optimized_thinking_config(
                model=model,
                request_type="complex" if len(valid_messages) > 5 else "simple",
                user_preference=None  # í–¥í›„ ì‚¬ìš©ì ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
            )
            if optimized_thinking_config:
                generation_config.thinking_config = optimized_thinking_config

        # ì…ë ¥ í† í° ê³„ì‚°
        input_token_count = count_tokens_with_tiktoken(conversation_text, model)
        input_tokens = input_token_count.get("input_tokens", 0)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ë²„í¼ë§ ìµœì í™”)
        accumulated_content = ""
        accumulated_thinking = ""
        thought_time = 0.0
        citations = []
        web_search_queries = []
        streaming_completed = False  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì—¬ë¶€ ì²´í¬
        is_disconnected = False  # ì—°ê²° ì¤‘ë‹¨ í”Œë˜ê·¸ ì¶”ê°€
        
        # ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì´ˆê¸°í™”
        content_buffer = StreamingBuffer()
        thinking_buffer = StreamingBuffer()

        try:
            response = client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=generation_config
            )

            start_time = time.time()

            for chunk in response:
                # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
                if await request.is_disconnected():
                    is_disconnected = True
                    logger.warning("Client disconnected. Stopping stream.")
                    break
                if chunk.candidates and len(chunk.candidates) > 0:
                    candidate = chunk.candidates[0]
                    
                    # ì½˜í…ì¸  íŒŒíŠ¸ ì²˜ë¦¬
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            # ì‚¬ê³  ë‚´ìš©ê³¼ ì¼ë°˜ ì½˜í…ì¸  ë¶„ë¦¬
                            if hasattr(part, 'thought') and part.thought:
                                # ì‚¬ê³  ë‚´ìš©ë§Œ ì²˜ë¦¬
                                if part.text:
                                    accumulated_thinking += part.text
                                    thought_time = time.time() - start_time
                                    
                                    # ë²„í¼ë§ëœ ì‚¬ê³  ë‚´ìš© ì „ì†¡
                                    if thinking_buffer.add_chunk(part.text):
                                        buffered_content = thinking_buffer.flush()
                                        try:
                                            yield f"data: {json.dumps({'reasoning_content': buffered_content, 'thought_time': thought_time})}\n\n"
                                        except (ConnectionError, BrokenPipeError, GeneratorExit):
                                            logger.warning("Client disconnected during reasoning streaming")
                                            return
                            elif part.text:
                                # ì¼ë°˜ ì‘ë‹µ ë‚´ìš©ë§Œ ì²˜ë¦¬
                                accumulated_content += part.text
                                
                                # ë²„í¼ë§ëœ ì¼ë°˜ ë‚´ìš© ì „ì†¡
                                if content_buffer.add_chunk(part.text):
                                    buffered_content = content_buffer.flush()
                                    try:
                                        yield f"data: {json.dumps({'content': buffered_content})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        logger.warning("Client disconnected during content streaming")
                                        return

                    # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (ìµœì‹  API êµ¬ì¡°)
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding = candidate.grounding_metadata
                        
                        # ì‹¤ì œ ê°’ë“¤ í™•ì¸
                        logger.debug("=== GROUNDING VALUES DEBUG ===")
                        logger.debug(f"web_search_queries: {getattr(grounding, 'web_search_queries', None)}")
                        logger.debug(f"grounding_chunks: {getattr(grounding, 'grounding_chunks', None)}")
                        logger.debug(f"grounding_supports: {getattr(grounding, 'grounding_supports', None)}")
                        
                        # ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜ì§‘
                        if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                            logger.debug(f"Adding web_search_queries: {grounding.web_search_queries}")
                            web_search_queries.extend(grounding.web_search_queries)
                        
                        # grounding_supportsì—ì„œ citations ì¶”ì¶œ ì‹œë„
                        if hasattr(grounding, 'grounding_supports') and grounding.grounding_supports:
                            logger.debug(f"Found grounding_supports: {len(grounding.grounding_supports)} supports")
                            new_citations = []
                            for i, support in enumerate(grounding.grounding_supports):
                                logger.debug(f"Support {i}: type={type(support)}, dir={dir(support)}")
                                logger.debug(f"Support {i} content: {support}")
                                
                                # grounding_chunk_indicesê°€ ìˆëŠ”ì§€ í™•ì¸
                                if hasattr(support, 'grounding_chunk_indices') and support.grounding_chunk_indices:
                                    for chunk_idx in support.grounding_chunk_indices:
                                        if (hasattr(grounding, 'grounding_chunks') and 
                                            grounding.grounding_chunks and 
                                            chunk_idx < len(grounding.grounding_chunks)):
                                            chunk = grounding.grounding_chunks[chunk_idx]
                                            logger.debug(f"Referenced chunk {chunk_idx}: {chunk}")
                                            
                                            if hasattr(chunk, 'web') and chunk.web:
                                                citation = {
                                                    "url": getattr(chunk.web, 'uri', ''),
                                                    "title": getattr(chunk.web, 'title', '')
                                                }
                                                logger.debug(f"Extracted citation from support: {citation}")
                                                if citation['url'] and not any(c['url'] == citation['url'] for c in citations):
                                                    citations.append(citation)
                                                    new_citations.append(citation)
                        
                        # ì§ì ‘ grounding chunksì—ì„œë„ ì‹œë„
                        if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                            logger.debug(f"Found grounding_chunks: {len(grounding.grounding_chunks)} chunks")
                            new_citations = []
                            for i, chunk in enumerate(grounding.grounding_chunks):
                                logger.debug(f"Direct chunk {i}: {chunk}")
                                if hasattr(chunk, 'web') and chunk.web:
                                    citation = {
                                        "url": getattr(chunk.web, 'uri', ''),
                                        "title": getattr(chunk.web, 'title', '')
                                    }
                                    logger.debug(f"Direct extracted citation: {citation}")
                                    if citation['url'] and not any(c['url'] == citation['url'] for c in citations):
                                        citations.append(citation)
                                        new_citations.append(citation)
                            
                            # ìƒˆë¡œìš´ ì¸ìš© ì •ë³´ë§Œ ì „ì†¡
                            if new_citations:
                                logger.debug(f"Sending {len(new_citations)} new citations")
                                try:
                                    yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    logger.warning("Client disconnected during citations streaming")
                                    return
                        
                        # ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì†¡
                        if web_search_queries:
                            logger.debug(f"Sending search queries: {web_search_queries}")
                            try:
                                yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                            except (ConnectionError, BrokenPipeError, GeneratorExit):
                                logger.warning("Client disconnected during search queries streaming")
                                return

            # ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if is_disconnected:
                logger.info("Skipping post-processing due to client disconnection.")
                return
            
            # ë²„í¼ì— ë‚¨ì€ ë‚´ìš© ì²˜ë¦¬
            remaining_content = content_buffer.flush()
            if remaining_content:
                try:
                    yield f"data: {json.dumps({'content': remaining_content})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    logger.warning("Client disconnected during final content flush")
                    return
            
            remaining_thinking = thinking_buffer.flush()
            if remaining_thinking:
                try:
                    yield f"data: {json.dumps({'reasoning_content': remaining_thinking, 'thought_time': thought_time})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    logger.warning("Client disconnected during final thinking flush")
                    return
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨
            streaming_completed = True
            
            # ì¶œë ¥ í† í° ê³„ì‚°
            output_token_count = count_tokens_with_tiktoken(accumulated_content, model)
            output_tokens = output_token_count.get("input_tokens", 0)
            
            # ì‚¬ê³  í† í° ê³„ì‚°
            thinking_tokens = 0
            if accumulated_thinking:
                thinking_token_count = count_tokens_with_tiktoken(accumulated_thinking, model)
                thinking_tokens = thinking_token_count.get("input_tokens", 0)

            # í† í° ì‚¬ìš©ëŸ‰ ì €ì¥ (KST ì‹œê°„ìœ¼ë¡œ ì €ì¥)
            from pytz import timezone
            kst = timezone('Asia/Seoul')
            crud_stats.create_token_usage(
                db=db,
                user_id=user_id,
                room_id=room_id,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens + thinking_tokens,  # ì‚¬ê³  í† í° í¬í•¨
                timestamp=datetime.now(kst)
            )

        except (ConnectionError, BrokenPipeError, GeneratorExit):
            streaming_completed = False  # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            logger.warning("Client disconnected during main streaming loop")
            return
        except Exception as api_error:
            streaming_completed = False  # ì—ëŸ¬ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            error_message = f"Gemini API Error: {str(api_error)}"
            try:
                yield f"data: {json.dumps({'error': error_message})}\n\n"
            except (ConnectionError, BrokenPipeError, GeneratorExit):
                logger.warning("Client disconnected during error streaming")
                return
        
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥ (ìƒˆë¡œìš´ DB ì„¸ì…˜ ì‚¬ìš©)
        if streaming_completed and accumulated_content:
            logger.debug("=== SAVING MESSAGE DEBUG ===")
            logger.debug(f"streaming_completed: {streaming_completed}")
            logger.debug(f"accumulated_content length: {len(accumulated_content)}")
            logger.debug(f"citations count: {len(citations)}")
            logger.debug(f"citations: {citations}")
            
            # room_id ìœ íš¨ì„± ê²€ì‚¬ (ë©”ì‹œì§€ ì €ì¥ ì „)
            if not room_id or room_id.strip() == "" or room_id == "unknown":
                logger.warning(f"Invalid room_id for message saving: {room_id}, skipping save")
            else:
                # ìƒˆë¡œìš´ DB ì„¸ì…˜ìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ ì„¸ì…˜ê³¼ ë¶„ë¦¬)
                from app.db.session import SessionLocal
                new_db = SessionLocal()
                try:
                    message_create = ChatMessageCreate(
                        content=accumulated_content,
                        role="assistant",
                        room_id=room_id,
                        reasoning_content=accumulated_thinking if accumulated_thinking else None,
                        thought_time=thought_time if thought_time > 0 else None,
                        citations=citations if citations else None
                    )
                    saved_message = crud_chat.create_message(new_db, room_id, message_create)
                    logger.info(f"Message saved with ID: {saved_message.id}")
                    logger.debug(f"Saved message citations: {saved_message.citations}")
                    logger.debug("=== END SAVING DEBUG ===")
                finally:
                    new_db.close()
        else:
            logger.info("=== MESSAGE NOT SAVED ===")
            logger.info(f"streaming_completed: {streaming_completed}")
            logger.info(f"accumulated_content: {bool(accumulated_content)}")
            logger.info(f"Reason: {'Streaming was interrupted' if not streaming_completed else 'No content'}")
            logger.info("=== END NOT SAVED DEBUG ===")

    except Exception as e:
        error_message = f"Stream Generation Error: {str(e)}"
        try:
            yield f"data: {json.dumps({'error': error_message})}\n\n"
        except (ConnectionError, BrokenPipeError, GeneratorExit):
            logger.warning("Client disconnected during final error streaming (generate_gemini_stream_response)")
            return

async def generate_stream_response(
    request: Request,
    chat_request: ChatRequest,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    room_id: Optional[str] = None,
    db: Optional[Session] = None,
    user_id: Optional[str] = None,
    subscription_plan: Optional[str] = None,
    file_names: Optional[List[str]] = None
):
    try:
        config = get_model_config(chat_request.model)
        if not config:
            raise HTTPException(status_code=400, detail="Invalid model specified")

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not chat_request.messages or len(chat_request.messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )

        # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
        MAX_MESSAGES = 10
        valid_messages = [
            msg for msg in chat_request.messages[-MAX_MESSAGES:] 
            if msg.content and msg.content.strip()
        ]
        
        if len(valid_messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="No valid message content found"
            )

        # Gemini ëª¨ë¸ ì²˜ë¦¬
        formatted_messages = [
            {"role": msg.role, "content": msg.content} 
            for msg in valid_messages
        ]
        
        # ê³ ê¸‰ ê¸°ëŠ¥ ì„¤ì • (êµ¬ë… ë“±ê¸‰ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
        enable_thinking = True
        enable_grounding = True
        enable_code_execution = True
        thinking_budget = 8192
        
        if subscription_plan:
            # êµ¬ë… ë“±ê¸‰ì— ë”°ë¥¸ ê¸°ëŠ¥ ì œí•œ (ì˜ˆì‹œ)
            if subscription_plan == "FREE":
                thinking_budget = 1024
                enable_grounding = False
                enable_code_execution = False
            elif subscription_plan == "BASIC":
                thinking_budget = 4096
                enable_grounding = True
                enable_code_execution = False
        
        async for chunk in generate_gemini_stream_response(
            request,
            formatted_messages, 
            chat_request.model, 
            room_id or "", 
            db, 
            user_id or "",
            file_data_list, 
            file_types, 
            file_names,
            enable_thinking=enable_thinking,
            enable_grounding=enable_grounding,
            enable_code_execution=enable_code_execution,
            thinking_budget=thinking_budget
        ):
            yield chunk

    except Exception as e:
        error_message = f"Stream Generation Error: {str(e)}"
        try:
            yield f"data: {json.dumps({'error': error_message})}\n\n"
        except (ConnectionError, BrokenPipeError, GeneratorExit):
            logger.warning("Client disconnected during final error streaming (generate_stream_response)")
            return

async def validate_file(file: UploadFile) -> bool:
    """íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    if file.size and file.size > MAX_FILE_SIZE:
        return False
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¥
    if file.content_type.startswith("image/"):
        return True
    elif file.content_type == "application/pdf":
        return True
    elif file.content_type in ["text/plain", "text/csv", "application/json"]:
        return True
    elif file.content_type.startswith("text/"):
        return True
    elif file.content_type in ["application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                             "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                             "application/vnd.openxmlformats-officedocument.presentationml.presentation"]:
        return True
    elif file.content_type.startswith("application/"):
        return True
    
    return False

async def generate_chat_room_name(first_message: str) -> str:
    """Open-WebUI ìŠ¤íƒ€ì¼ì˜ AI ê¸°ë°˜ ì±„íŒ…ë°© ì œëª© ìƒì„±"""
    try:
        # ë¹ˆ ë©”ì‹œì§€ ì²˜ë¦¬
        if not first_message or len(first_message.strip()) == 0:
            return "ìƒˆ ì±„íŒ…"
        
        # ê°„ë‹¨í•œ fallback ë¨¼ì € ìƒì„± (AI ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        words = first_message.strip().split()
        fallback_title = " ".join(words[:3]) if len(words) >= 3 else " ".join(words)
        if len(fallback_title) > 20:
            fallback_title = fallback_title[:17] + "..."
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        client = get_gemini_client()
        if not client:
            logger.info("Gemini client not available, using fallback")
            return fallback_title
        
        # Open-WebUI ìŠ¤íƒ€ì¼ì˜ ì±„íŒ…ë°© ì œëª© ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt_template = """Generate a concise and descriptive title in Korean for this chat conversation based on the AI response content.

Requirements:
- Use 2-10 Korean words only
- No emojis or special characters
- Capture the main topic or purpose
- Be specific and informative
- Return only JSON format

Examples:
{{"title": "íŒŒì´ì¬ ê¸°ì´ˆ í•™ìŠµ"}}
{{"title": "ë ˆì‹œí”¼ ì¶”ì²œ"}}
{{"title": "í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸"}}
{{"title": "ì¼ë°˜ ëŒ€í™”"}}
{{"title": "ì¸ì‚¬"}}

AI Response Content: {message}

Generate title as JSON:"""

        # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
        limited_message = first_message[:300] if len(first_message) > 300 else first_message
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = prompt_template.format(message=limited_message)
        
        logger.info(f"Generating AI title for response: '{limited_message[:50]}...'")
        
        # Gemini API í˜¸ì¶œ
        logger.debug(f"Final prompt being sent to Gemini: {repr(prompt)}")
        try:
            logger.info(f"Calling Gemini API with model: gemini-2.0-flash-lite")
            response = client.models.generate_content(
                model="gemini-2.0-flash-lite",
                contents=[prompt],
                config=types.GenerateContentConfig(
                    temperature=0.3,  # ì°½ì˜ì ì¸ ì œëª© ìƒì„±ì„ ìœ„í•´ ì ë‹¹í•œ ì˜¨ë„
                    max_output_tokens=50  # ê°„ë‹¨í•œ JSON ì‘ë‹µìš©
                )
            )
            logger.info(f"API call completed successfully")
            logger.debug(f"Response object: {type(response)}")
            
            # text ì†ì„± í™•ì¸
            if hasattr(response, 'text'):
                logger.debug(f"Gemini raw response text: {repr(response.text)}")
            
            # candidates í™•ì¸
            if hasattr(response, 'candidates'):
                logger.debug(f"Response has {len(response.candidates) if response.candidates else 0} candidates")
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° JSON íŒŒì‹±
            response_text = None
            
            # Gemini API ë¬¸ì„œì— ë”°ë¥¸ í‘œì¤€ ì‘ë‹µ êµ¬ì¡°ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # 1. candidates[0].content.partsì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‘œì¤€ ë°©ë²•)
            if hasattr(response, 'candidates') and response.candidates and len(response.candidates) > 0:
                candidate = response.candidates[0]  # ì²« ë²ˆì§¸ í›„ë³´ ì‚¬ìš©
                logger.debug(f"Working with candidate: {type(candidate)}")
                
                if hasattr(candidate, 'content') and candidate.content:
                    content = candidate.content
                    logger.debug(f"Content type: {type(content)}")
                    
                    # content.parts í™•ì¸
                    if hasattr(content, 'parts'):
                        if content.parts and len(content.parts) > 0:
                            logger.debug(f"Found {len(content.parts)} parts")
                            for i, part in enumerate(content.parts):
                                if hasattr(part, 'text') and part.text:
                                    response_text = part.text
                                    logger.info(f"Got text from part {i}: '{response_text[:100]}...'")
                                    break
                        else:
                            logger.debug("Parts list is empty")
                            # contentë¥¼ dictë¡œ ë³€í™˜í•´ì„œ í™•ì¸
                            try:
                                content_dict = content.model_dump() if hasattr(content, 'model_dump') else content.dict()
                                if 'parts' in content_dict and content_dict['parts']:
                                    for i, part_dict in enumerate(content_dict['parts']):
                                        if 'text' in part_dict and part_dict['text']:
                                            response_text = part_dict['text']
                                            logger.info(f"Got text from dict part {i}: '{response_text[:100]}...'")
                                            break
                            except Exception as e:
                                logger.debug(f"Error converting content to dict: {e}")
                            
                            # contentì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ ì°¾ê¸°
                            if not response_text and hasattr(content, 'text') and content.text:
                                response_text = content.text
                                logger.info(f"Got text from content.text: '{response_text[:100]}...'")
                    else:
                        logger.debug("No parts attribute in content")
                else:
                    logger.debug("No content found in candidate")
            
            # 2. response.text fallback ì‹œë„
            elif hasattr(response, 'text') and response.text:
                response_text = response.text
                logger.info(f"Got text from response.text (fallback): '{response_text[:100]}...'")
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ JSON íŒŒì‹± ì‹œë„
            if response_text:
                try:
                    import json
                    import re
                    
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                    text = response_text.strip()
                    if text.startswith('```'):
                        json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', text, re.DOTALL)
                        if json_match:
                            text = json_match.group(1).strip()
                    
                    # JSON íŒŒì‹±
                    result = json.loads(text)
                    if 'title' in result and result['title']:
                        ai_title = result['title'].strip()
                        if len(ai_title) <= 25:
                            logger.info(f"Successfully generated AI title: '{ai_title}'")
                            return ai_title
                        else:
                            logger.info(f"AI title too long: '{ai_title}'")
                except json.JSONDecodeError as e:
                    logger.debug(f"JSON decode error: {e}, text: '{text[:100]}...'")
                except Exception as e:
                    logger.debug(f"Error parsing response: {e}")
            else:
                logger.debug("Could not extract text from response")
            
        except Exception as api_error:
            logger.info(f"Gemini API call failed: {api_error}")
        
        # AI ìƒì„± ì‹¤íŒ¨ ì‹œ fallback ì‚¬ìš©
        logger.info(f"Using fallback title: '{fallback_title}'")
        return fallback_title
        
            
    except Exception as e:
        logger.error(f"Chat room name generation error: {e}", exc_info=True)
        return "ìƒˆ ì±„íŒ…"

@router.post("/title/generate", summary="ì±„íŒ…ë°© ì œëª© ìƒì„±")
async def generate_title_api(
    request: Request
):
    """Open-WebUI ìŠ¤íƒ€ì¼ ì±„íŒ…ë°© ì œëª© ìƒì„± API"""
    try:
        body = await request.json()
        messages = body.get("messages", [])
        
        if not messages:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )
        
        # ì²« ë²ˆì§¸ user ë©”ì‹œì§€ ì°¾ê¸°
        first_user_message = None
        for msg in messages:
            if msg.get("role") == "user" and msg.get("content"):
                first_user_message = msg.get("content")
                break
        
        if not first_user_message:
            raise HTTPException(
                status_code=400,
                detail="No user message found"
            )
        
        # ì œëª© ìƒì„±
        title = await generate_chat_room_name(first_user_message)
        
        return {
            "title": title,
            "status": "success"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Title generation API error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error during title generation"
        )

@router.post("/rooms", response_model=ChatRoom, summary="ìƒˆ ì±„íŒ…ë°© ìƒì„±")
def create_chat_room(
    room: ChatRoomCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ìƒˆë¡œìš´ ì±„íŒ…ë°©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    - **title**: ì±„íŒ…ë°© ì œëª© (í•„ìˆ˜)
    - **model**: ì‚¬ìš©í•  AI ëª¨ë¸ (í•„ìˆ˜, ì§€ì›ë˜ëŠ” ëª¨ë¸: gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash-exp ë“±)
    - **system_prompt**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
    - **project_id**: í”„ë¡œì íŠ¸ ID (ì„ íƒ, í”„ë¡œì íŠ¸ ë‚´ ì±„íŒ…ë°©ì¸ ê²½ìš°)
    
    **ì§€ì›ë˜ëŠ” AI ëª¨ë¸:**
    - Gemini Flash (ë¹ ë¥¸ ì‘ë‹µ)
    - Gemini Pro (ê³ í’ˆì§ˆ ì‘ë‹µ)
    - Gemini Flash Thinking (ì¶”ë¡  ê³¼ì • í¬í•¨)
    
    **ì‘ë‹µ:**
    - ìƒì„±ëœ ì±„íŒ…ë°© ì •ë³´ ë°˜í™˜
    """
    return crud_chat.create_chat_room(db, room, current_user.id)

@router.post("/rooms/{room_id}/generate-name")
async def generate_room_name(
    room_id: str,
    message_content: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±„íŒ…ë°© ì´ë¦„ì„ ìƒì„±í•˜ê³  ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        logger.info(f"ğŸ” generate_room_name called - room_id: {room_id}, message: {message_content[:50]}...")
        
        # ì±„íŒ…ë°© ì†Œìœ ê¶Œ í™•ì¸
        chat_room = crud_chat.get_chat_room(db, room_id, current_user.id)
        if not chat_room:
            logger.info(f"âŒ Chat room not found: {room_id} - room may not be created yet")
            return {
                "room_id": room_id,
                "generated_name": "ìƒˆ ì±„íŒ…",
                "message": "Chat room not found - may not be created yet"
            }
        
        logger.info(f"ğŸ” Found chat room: {chat_room.id}, current name: '{chat_room.name}'")
        
        # ì±„íŒ…ë°© ì´ë¦„ì´ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        try:
            current_name = getattr(chat_room, 'name', None)
            current_name_str = str(current_name) if current_name is not None else ""
        except:
            current_name_str = ""
            
        logger.info(f"ğŸ” Current name check: '{current_name_str}'")
        
        # "ìƒˆ ì±„íŒ…"ì¸ ê²½ìš°ì—ë§Œ ì œëª© ìƒì„±
        if current_name_str and current_name_str.strip() != "" and current_name_str != "ìƒˆ ì±„íŒ…":
            logger.info(f"â­ï¸ Room already has a name: '{current_name_str}'")
            return {
                "room_id": room_id,
                "generated_name": current_name_str,
                "message": "Room already has a name"
            }
        
        # ì „ë‹¬ë°›ì€ ë©”ì‹œì§€ ë‚´ìš©ìœ¼ë¡œ ì œëª© ìƒì„±
        if not message_content or message_content.strip() == "":
            logger.info("âŒ Message content is required")
            raise HTTPException(status_code=400, detail="Message content is required")
        
        logger.info(f"ğŸ”„ Generating room name for message: '{message_content[:50]}...'")
        
        # AI ê¸°ë°˜ ì œëª© ìƒì„±
        generated_name = await generate_chat_room_name(message_content.strip())
        logger.info(f"ğŸ¯ AI generated name: '{generated_name}'")
        
        if not generated_name or generated_name == "ìƒˆ ì±„íŒ…":
            # AI ìƒì„± ì‹¤íŒ¨ ì‹œ fallback
            words = message_content.strip().split()
            fallback_title = " ".join(words[:3]) if len(words) >= 3 else message_content.strip()
            if len(fallback_title) > 20:
                fallback_title = fallback_title[:17] + "..."
            generated_name = fallback_title
            logger.info(f"ğŸ”„ Using fallback title: '{generated_name}'")
        
        # ì±„íŒ…ë°© ì´ë¦„ ì—…ë°ì´íŠ¸
        from app.schemas.chat import ChatRoomCreate
        room_update = ChatRoomCreate(name=generated_name)
        
        logger.info(f"ğŸ’¾ Updating room name to: '{generated_name}'")
        updated_room = crud_chat.update_chat_room(db, room_id, room_update, current_user.id)
        
        logger.info(f"âœ… Room name updated: '{generated_name}'")
        
        return {
            "room_id": room_id,
            "generated_name": generated_name,
            "updated_room": updated_room
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Room name generation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to generate room name")

@router.get("/rooms", response_model=ChatRoomList)
async def get_chatroom(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    rooms = crud_chat.get_chatroom(db, current_user.id)
    return ChatRoomList(rooms=rooms)

@router.delete("/rooms/{room_id}")
async def delete_chat_room(
    room_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    crud_chat.delete_chat_room(db, room_id, current_user.id)
    return {"message": "Room deleted successfully"}

@router.post("/rooms/{room_id}/messages")
async def create_message(
    room_id: str,
    message: ChatMessageCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    # ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
    if hasattr(message, 'model') and message.model:
        config = get_model_config(message.model)
        if not config:
            raise HTTPException(status_code=400, detail="Invalid model specified")
    
    # ë©”ì‹œì§€ ìƒì„±
    created_message = crud_chat.create_message(db, room_id, message)
    
    # êµ¬ë… ì •ë³´ í™•ì¸ ë° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
    if hasattr(message, 'model') and message.model:
        updated_subscription = crud_subscription.update_model_usage(
            db, current_user.id, message.model
        )
        
        if not updated_subscription:
            # ìƒì„±ëœ ë©”ì‹œì§€ ì‚­ì œ (ë¡¤ë°±)
            db.delete(created_message)
            db.commit()
            raise HTTPException(
                status_code=403, 
                detail="Usage limit exceeded for this model group"
            )
    
    return created_message

@router.get("/rooms/{room_id}/messages", response_model=ChatMessageList)
async def get_chat_messages(
    room_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    messages = crud_chat.get_room_messages(db, room_id, current_user.id)
    return ChatMessageList(messages=messages)

@router.post("/rooms/{room_id}/chat")
async def create_chat_message(
    room_id: str,
    request: Request,
    request_data: str = Form(...),
    files: List[UploadFile] = File([]),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        # JSON íŒŒì‹±
        try:
            parsed_data = json.loads(request_data)
            chat_request = ChatRequest(**parsed_data)
        except (json.JSONDecodeError, ValueError) as e:
            raise HTTPException(status_code=400, detail=f"Invalid JSON format: {str(e)}")

        # ëª¨ë¸ ì„¤ì • í™•ì¸
        config = get_model_config(chat_request.model)
        if not config:
            raise HTTPException(status_code=400, detail="Invalid model specified")

        # íŒŒì¼ ì²˜ë¦¬
        file_data_list = []
        file_types = []
        file_names = []
        
        if files and files[0].filename:
            if not config.supports_multimodal:
                raise HTTPException(
                    status_code=400,
                    detail="File upload is not supported for this model"
                )
            
            for file in files:
                if not await validate_file(file):
                    raise HTTPException(
                        status_code=400,
                        detail=f"Invalid file: {file.filename}"
                    )
                
                file_data, file_type = await process_file_to_base64(file)
                file_data_list.append(file_data)
                file_types.append(file_type)
                file_names.append(file.filename)

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        if chat_request.messages:
            last_message = chat_request.messages[-1]
            user_message = ChatMessageCreate(
                content=last_message.content,
                role="user",
                room_id=room_id,
                files=[{
                    "type": file_type,
                    "name": file_name,
                    "data": file_data
                } for file_data, file_type, file_name in zip(file_data_list, file_types, file_names)] if file_data_list else None
            )
            # room_id ê²€ì¦ í›„ ì €ì¥
            if room_id and room_id.strip() != "" and room_id != "unknown":
                crud_chat.create_message(db, room_id, user_message)
            else:
                logger.warning(f"Invalid room_id for user message saving: {room_id}, skipping save")

        # êµ¬ë… ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ (crud_subscription ì‚¬ìš©)
        updated_subscription = crud_subscription.update_model_usage(
            db, current_user.id, chat_request.model
        )
        
        if not updated_subscription:
            raise HTTPException(
                status_code=403, 
                detail="Usage limit exceeded for this model group"
            )

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        return StreamingResponse(
            generate_stream_response(
                request,
                chat_request,
                file_data_list,
                file_types,
                room_id,
                db,
                current_user.id,
                updated_subscription.plan,
                file_names
            ),
            media_type="text/plain"
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@router.patch("/rooms/{room_id}", response_model=ChatRoom)
async def update_chat_room(
    room_id: str,
    room: ChatRoomCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    return crud_chat.update_chat_room(db, room_id, room, current_user.id)

@router.get("/rooms/{room_id}", response_model=ChatRoom)
async def get_chat_room(
    room_id: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    return crud_chat.get_chat_room(db, room_id, current_user.id)

# ì»¨í…ìŠ¤íŠ¸ ìºì‹± ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@router.post("/cache")
async def create_context_cache(
    content: str = Form(...),
    model: str = Form(...),
    ttl: int = Form(3600),  # ê¸°ë³¸ 1ì‹œê°„
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìƒì„±"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # ìºì‹œ ìƒì„±
        cache = client.caches.create(
            model=model,
            config=types.CreateCachedContentConfig(
                display_name=f"cache_{current_user.id}_{int(time.time())}",
                contents=[content],
                ttl=f"{ttl}s"
            )
        )
        
        return {
            "cache_name": cache.name,
            "ttl": ttl,
            "message": "Cache created successfully"
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create cache: {str(e)}")

@router.get("/cache")
async def list_context_caches(
    current_user: User = Depends(get_current_user)
):
    """ì‚¬ìš©ìì˜ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ëª©ë¡ ì¡°íšŒ"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        caches = []
        for cache in client.caches.list():
            if cache.display_name and cache.display_name.startswith(f"cache_{current_user.id}_"):
                caches.append({
                    "name": cache.name,
                    "display_name": cache.display_name,
                    "model": cache.model,
                    "create_time": cache.create_time.isoformat() if cache.create_time else None,
                    "expire_time": cache.expire_time.isoformat() if cache.expire_time else None
                })
        
        return {"caches": caches}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list caches: {str(e)}")

@router.delete("/cache/{cache_name}")
async def delete_context_cache(
    cache_name: str,
    current_user: User = Depends(get_current_user)
):
    """ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        client.caches.delete(cache_name)
        
        return {"message": "Cache deleted successfully"}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete cache: {str(e)}")

@router.get("/stats/token-usage", response_model=List[TokenUsage])
async def get_token_usage(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    user_id: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
    start_dt = None
    end_dt = None
    
    if start_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
    if end_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
    
    return crud_stats.get_token_usage(
        db=db,
        start_date=start_dt,
        end_date=end_dt,
        user_id=user_id or current_user.id
    )

@router.get("/stats/chat-usage")
async def get_chat_usage(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    user_id: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
    start_dt = None
    end_dt = None
    
    if start_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
    if end_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
    
    return crud_stats.get_chat_statistics(
        db=db,
        start_date=start_dt,
        end_date=end_dt,
        user_id=user_id
    )

@router.get("/stats/token-usage-history")
async def get_token_usage_history(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    user_id: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
    start_dt = None
    end_dt = None
    
    if start_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
    if end_date:
        # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
        end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
    
    return crud_stats.get_token_usage_history(
        db=db,
        start=start_dt,
        end=end_dt,
        user_id=user_id
    )

# ì„ë² ë”© ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@router.post("/embeddings")
async def create_embeddings(
    texts: List[str] = Form(...),
    model: str = Form("text-embedding-004"),
    task_type: str = Form("SEMANTIC_SIMILARITY"),
    current_user: User = Depends(get_current_user)
):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        embeddings = []
        for text in texts:
            result = client.models.embed_content(
                model=model,
                contents=text,
                config=types.EmbedContentConfig(task_type=task_type)
            )
            embeddings.append({
                "text": text,
                "embedding": result.embeddings[0] if result.embeddings else []
            })
        
        return {"embeddings": embeddings}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create embeddings: {str(e)}")

# í”„ë¡¬í”„íŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸ ê°œì„ 
@router.post("/generate-prompt")
async def generate_prompt(
    category: str = Form(...),  # ì¹´í…Œê³ ë¦¬ (í•™ìŠµ, ì°½ì‘, ë¶„ì„, ë²ˆì—­, ì½”ë”© ë“±)
    task_description: str = Form(...),  # ì‘ì—… ì„¤ëª…
    style: str = Form("ì¹œê·¼í•œ"),  # ìŠ¤íƒ€ì¼ (ì¹œê·¼í•œ, ì „ë¬¸ì , ì°½ì˜ì , ê°„ê²°í•œ)
    complexity: str = Form("ì¤‘ê°„"),  # ë³µì¡ë„ (ê°„ë‹¨, ì¤‘ê°„, ê³ ê¸‰)
    output_format: str = Form("ììœ í˜•ì‹"),  # ì¶œë ¥ í˜•ì‹ (ììœ í˜•ì‹, ë‹¨ê³„ë³„, í‘œí˜•ì‹, ë¦¬ìŠ¤íŠ¸)
    include_examples: bool = Form(True),  # ì˜ˆì‹œ í¬í•¨ ì—¬ë¶€
    include_constraints: bool = Form(False),  # ì œì•½ì‚¬í•­ í¬í•¨ ì—¬ë¶€
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ê°œì„ ëœ AI í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° (ë¡œê·¸ì¸ í•„ìš”)"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ ì‹œìŠ¤í…œ ì§€ì‹œ ìƒì„±
        category_instructions = {
            "í•™ìŠµ": "êµìœ¡ ë° í•™ìŠµ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì „ë¬¸ê°€ë¡œì„œ, í•™ìŠµìì˜ ì´í•´ë„ë¥¼ ë†’ì´ê³  ë‹¨ê³„ì  í•™ìŠµì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì°½ì‘": "ì°½ì˜ì  ì½˜í…ì¸  ìƒì„± ì „ë¬¸ê°€ë¡œì„œ, ìƒìƒë ¥ì„ ìê·¹í•˜ê³  ë…ì°½ì ì¸ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¶„ì„": "ë°ì´í„° ë¶„ì„ ë° ë…¼ë¦¬ì  ì‚¬ê³  ì „ë¬¸ê°€ë¡œì„œ, ì²´ê³„ì ì´ê³  ê°ê´€ì ì¸ ë¶„ì„ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë²ˆì—­": "ë‹¤êµ­ì–´ ë²ˆì—­ ì „ë¬¸ê°€ë¡œì„œ, ë¬¸ë§¥ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ ì •í™•íˆ ì „ë‹¬í•˜ëŠ” ë²ˆì—­ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì½”ë”©": "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì „ë¬¸ê°€ë¡œì„œ, íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì½”ë“œ ì‘ì„±ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ë° ì˜ì‚¬ê²°ì • ì „ë¬¸ê°€ë¡œì„œ, ì‹¤ìš©ì ì´ê³  ê²°ê³¼ ì§€í–¥ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì¼ë°˜": "ë²”ìš© í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ì–‘í•œ ìƒí™©ì— ì ìš© ê°€ëŠ¥í•œ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        }
        
        # ìŠ¤íƒ€ì¼ë³„ í†¤ ì„¤ì •
        style_tones = {
            "ì¹œê·¼í•œ": "ì¹œê·¼í•˜ê³  ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ í†¤ìœ¼ë¡œ, ì‚¬ìš©ìì™€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ ë„",
            "ì „ë¬¸ì ": "ì „ë¬¸ì ì´ê³  ì •í™•í•œ í†¤ìœ¼ë¡œ, ì‹ ë¢°ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ì œê³µ",
            "ì°½ì˜ì ": "ì°½ì˜ì ì´ê³  ì˜ê°ì„ ì£¼ëŠ” í†¤ìœ¼ë¡œ, í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¥¼ ìê·¹",
            "ê°„ê²°í•œ": "ëª…í™•í•˜ê³  ê°„ê²°í•œ í†¤ìœ¼ë¡œ, íš¨ìœ¨ì ì¸ ì†Œí†µì„ ì¶”êµ¬"
        }
        
        # ë³µì¡ë„ë³„ ì ‘ê·¼ ë°©ì‹
        complexity_approaches = {
            "ê°„ë‹¨": "ì´ˆë³´ìë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì¸ ì ‘ê·¼",
            "ì¤‘ê°„": "ê¸°ë³¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ê· í˜•ì¡íŒ ì ‘ê·¼",
            "ê³ ê¸‰": "ì „ë¬¸ì  ì§€ì‹ê³¼ ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ìš”êµ¬í•˜ëŠ” ê³ ê¸‰ ì ‘ê·¼"
        }
        
        # ì¶œë ¥ í˜•ì‹ë³„ êµ¬ì¡°
        format_structures = {
            "ììœ í˜•ì‹": "ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ìœ ì—°í•œ ì‘ë‹µ êµ¬ì¡°",
            "ë‹¨ê³„ë³„": "1ë‹¨ê³„, 2ë‹¨ê³„ ë“± ìˆœì°¨ì  ë‹¨ê³„ë³„ ì‘ë‹µ êµ¬ì¡°",
            "í‘œí˜•ì‹": "í‘œë‚˜ ì°¨íŠ¸ í˜•íƒœë¡œ ì •ë¦¬ëœ ì²´ê³„ì  ì‘ë‹µ êµ¬ì¡°",
            "ë¦¬ìŠ¤íŠ¸": "ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ ëª©ë¡ í˜•íƒœì˜ ëª…í™•í•œ ì‘ë‹µ êµ¬ì¡°"
        }
        
        system_instruction = f"""
        ë‹¹ì‹ ì€ {category_instructions.get(category, category_instructions["ì¼ë°˜"])}
        
        í”„ë¡¬í”„íŠ¸ ìƒì„± ì›ì¹™:
        1. {style_tones.get(style, style_tones["ì¹œê·¼í•œ"])}
        2. {complexity_approaches.get(complexity, complexity_approaches["ì¤‘ê°„"])}
        3. {format_structures.get(output_format, format_structures["ììœ í˜•ì‹"])}
        4. ëª…í™•í•œ ì§€ì‹œì‚¬í•­ê³¼ êµ¬ì²´ì ì¸ ê¸°ëŒ€ ê²°ê³¼ë¥¼ í¬í•¨
        5. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ìµœì ì˜ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        ì‘ë‹µ í˜•ì‹:
        - í”„ë¡¬í”„íŠ¸ ì œëª© (ê°„ê²°í•˜ê³  ëª©ì ì´ ëª…í™•)
        - ë©”ì¸ í”„ë¡¬í”„íŠ¸ (ì‹¤ì œ ì‚¬ìš©í•  ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸)
        - ì‚¬ìš© íŒ (íš¨ê³¼ì ì¸ ì‚¬ìš© ë°©ë²•)
        - ë³€í˜• ì œì•ˆ (ìƒí™©ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ë³€í˜• ë°©ë²•)
        """
        
        user_request = f"""
        ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        ğŸ“‹ **ê¸°ë³¸ ì •ë³´**
        - ì¹´í…Œê³ ë¦¬: {category}
        - ì‘ì—… ì„¤ëª…: {task_description}
        - ìŠ¤íƒ€ì¼: {style}
        - ë³µì¡ë„: {complexity}
        - ì¶œë ¥ í˜•ì‹: {output_format}
        
        ğŸ“Œ **ì¶”ê°€ ìš”êµ¬ì‚¬í•­**
        - ì˜ˆì‹œ í¬í•¨: {'ì˜ˆ' if include_examples else 'ì•„ë‹ˆì˜¤'}
        - ì œì•½ì‚¬í•­ í¬í•¨: {'ì˜ˆ' if include_constraints else 'ì•„ë‹ˆì˜¤'}
        
        ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ ë°”ë¡œ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì™„ì„±ëœ í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[user_request],
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.8,  # ì°½ì˜ì„±ì„ ìœ„í•´ ì˜¨ë„ ì¡°ê¸ˆ ìƒìŠ¹
                max_output_tokens=3000,  # ë” ìì„¸í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•´ í† í° ì¦ê°€
            )
        )
        
        return {
            "generated_prompt": response.text,
            "category": category,
            "task_description": task_description,
            "style": style,
            "complexity": complexity,
            "output_format": output_format,
            "settings": {
                "include_examples": include_examples,
                "include_constraints": include_constraints
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate prompt: {str(e)}")

# ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
@router.post("/search")
async def search_web(
    request: Request,
    query: str = Form(...),
    room_id: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Google ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ (ìŠ¤íŠ¸ë¦¬ë°)"""
    try:
        # ì‚¬ìš©ì ê²€ìƒ‰ ì§ˆë¬¸ì„ DBì— ì €ì¥
        user_message = ChatMessageCreate(
            content=f"ğŸ” ê²€ìƒ‰: {query}",
            role="user",
            room_id=room_id
        )
        crud_chat.create_message(db, room_id, user_message)
        
        async def generate_search_stream():
            try:
                client = get_gemini_client()
                if not client:
                    yield f"data: {json.dumps({'error': 'Gemini client not available'})}\n\n"
                    return
                
                # ê²€ìƒ‰ ë„êµ¬ ì„¤ì • (ìµœì‹  API ë°©ì‹)
                tools = [types.Tool(google_search=types.GoogleSearch())]
                
                # ê²€ìƒ‰ì„ ìœ„í•œ ì‹œìŠ¤í…œ ì§€ì‹œ
                system_instruction = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬ì— ëŒ€í•´ ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
                ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³  ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”."""
                
                # ìƒì„± ì„¤ì •
                generation_config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.7,
                    max_output_tokens=2048,
                    tools=tools
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì‹¤í–‰
                response = client.models.generate_content_stream(
                    model="gemini-2.5-flash",
                    contents=[f"ë‹¤ìŒì— ëŒ€í•´ ê²€ìƒ‰í•´ì£¼ì„¸ìš”: {query}"],
                    config=generation_config
                )
                
                accumulated_content = ""
                citations = []
                web_search_queries = []
                citations_sent = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ set
                search_completed = False  # ê²€ìƒ‰ ì™„ë£Œ ì—¬ë¶€ ì²´í¬
                is_disconnected = False  # ì—°ê²° ì¤‘ë‹¨ í”Œë˜ê·¸ ì¶”ê°€
                
                for chunk in response:
                    # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
                    if await request.is_disconnected():
                        is_disconnected = True
                        logger.warning("Client disconnected during search. Stopping stream.")
                        break
                    if chunk.candidates and len(chunk.candidates) > 0:
                        candidate = chunk.candidates[0]
                        
                        # ì½˜í…ì¸  ì²˜ë¦¬
                        if candidate.content and candidate.content.parts:
                            for part in candidate.content.parts:
                                if part.text:
                                    accumulated_content += part.text
                                    try:
                                        yield f"data: {json.dumps({'content': part.text})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        logger.warning("Client disconnected during search content streaming")
                                        return
                        
                        # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (ìµœì‹  API êµ¬ì¡°)
                        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                            grounding = candidate.grounding_metadata
                            
                            # ë””ë²„ê¹…: grounding metadata êµ¬ì¡° í™•ì¸ (ê²€ìƒ‰ìš©)
                            logger.debug("=== SEARCH GROUNDING METADATA DEBUG ===")
                            logger.debug(f"grounding type: {type(grounding)}")
                            logger.debug(f"grounding dir: {dir(grounding)}")
                            
                            # ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜ì§‘
                            if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                                logger.debug(f"Found web_search_queries: {grounding.web_search_queries}")
                                web_search_queries.extend(grounding.web_search_queries)
                            
                            # grounding chunksì—ì„œ citations ì¶”ì¶œ
                            if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                                logger.debug(f"Found grounding_chunks: {len(grounding.grounding_chunks)} chunks")
                                new_citations = []
                                for i, chunk in enumerate(grounding.grounding_chunks):
                                    logger.debug(f"Search Chunk {i}: type={type(chunk)}, dir={dir(chunk)}")
                                    if hasattr(chunk, 'web') and chunk.web:
                                        logger.debug(f"Search Chunk {i} web: type={type(chunk.web)}, dir={dir(chunk.web)}")
                                        citation_url = chunk.web.uri
                                        
                                        # Mixed Content ë¬¸ì œ ë°©ì§€: HTTP URLì„ HTTPSë¡œ ë³€í™˜
                                        if citation_url.startswith('http://'):
                                            citation_url = citation_url.replace('http://', 'https://', 1)
                                        
                                        # ì¤‘ë³µ ë°©ì§€
                                        if citation_url not in citations_sent:
                                            citation = {
                                                "url": citation_url,
                                                "title": chunk.web.title if hasattr(chunk.web, 'title') else ""
                                            }
                                            logger.debug(f"Search extracted citation: {citation}")
                                            citations.append(citation)
                                            new_citations.append(citation)
                                            citations_sent.add(citation_url)
                                
                                # ìƒˆë¡œìš´ ì¸ìš© ì •ë³´ë§Œ ì „ì†¡
                                if new_citations:
                                    logger.debug(f"Search sending {len(new_citations)} new citations")
                                    try:
                                        yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        logger.warning("Client disconnected during search citations streaming")
                                        return
                
                # ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if is_disconnected:
                    logger.info("Skipping search post-processing due to client disconnection.")
                    return
                
                # ê²€ìƒ‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨
                search_completed = True
                
                # ìµœì¢… ë©”íƒ€ë°ì´í„° ì „ì†¡
                if web_search_queries:
                    try:
                        yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                        logger.warning("Client disconnected during final search queries streaming")
                        return
                
            except Exception as e:
                search_completed = False  # ì—ëŸ¬ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
                try:
                    yield f"data: {json.dumps({'error': f'Search failed: {str(e)}'})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    logger.warning("Client disconnected during search error streaming")
                    return
            
            # ê²€ìƒ‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥
            if search_completed and accumulated_content and room_id and room_id.strip() != "" and room_id != "unknown":
                logger.debug("=== SEARCH SAVING DEBUG ===")
                logger.debug(f"search_completed: {search_completed}")
                logger.debug(f"Saving search response with {len(citations)} citations: {citations}")
                ai_message = ChatMessageCreate(
                    content=accumulated_content,
                    role="assistant",
                    room_id=room_id,
                    citations=citations if citations else None
                )
                saved_message = crud_chat.create_message(db, room_id, ai_message)
                logger.debug(f"Saved message citations: {saved_message.citations}")
                logger.debug("=== END SEARCH SAVING DEBUG ===")
            else:
                logger.info("=== SEARCH MESSAGE NOT SAVED ===")
                logger.info(f"search_completed: {search_completed}")
                logger.info(f"accumulated_content: {bool(accumulated_content)}")
                logger.info(f"Reason: {'Search was interrupted' if not search_completed else 'No content'}")
                logger.info("=== END SEARCH NOT SAVED DEBUG ===")
        
        return StreamingResponse(
            generate_search_stream(),
            media_type="text/plain"
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

async def get_optimized_thinking_config(
    model: str, 
    request_type: str = "normal",
    user_preference: Optional[str] = None
) -> Optional[types.ThinkingConfig]:
    """ì ì‘í˜• ì‚¬ê³  ì„¤ì • ìƒì„±"""
    if model not in THINKING_OPTIMIZATION:
        return None
    
    config = THINKING_OPTIMIZATION[model]
    
    # ì‚¬ìš©ì ì„ í˜¸ë„ ê¸°ë°˜ ì¡°ì •
    if user_preference == "fast":
        budget = 0  # ë¹ ë¥¸ ì‘ë‹µ
    elif user_preference == "quality":
        budget = config["max_budget"]  # ìµœê³  í’ˆì§ˆ
    else:
        # ì ì‘í˜• ì˜ˆì‚° ê³„ì‚°
        if request_type == "simple":
            budget = config["default_budget"]
        elif request_type == "complex":
            budget = config["max_budget"] // 2
        else:
            budget = config["default_budget"]
    
    return types.ThinkingConfig(
        thinking_budget=budget,
        include_thoughts=budget > 0
    )

async def get_or_create_chat_session(
    client, 
    model: str, 
    room_id: str,
    system_instruction: Optional[str] = None
):
    """ì±„íŒ… ì„¸ì…˜ ìºì‹œ ê´€ë¦¬"""
    cache_key = f"{model}:{room_id}"
    
    # ê¸°ì¡´ ì„¸ì…˜ í™•ì¸
    if cache_key in CHAT_SESSION_CACHE:
        session_info = CHAT_SESSION_CACHE[cache_key]
        # ì„¸ì…˜ ë§Œë£Œ í™•ì¸ (1ì‹œê°„)
        if time.time() - session_info["created_at"] < 3600:
            return session_info["session"]
    
    # ìƒˆ ì„¸ì…˜ ìƒì„±
    try:
        chat_session = client.chats.create(
            model=model,
            config=types.GenerateContentConfig(
                system_instruction=system_instruction
            ) if system_instruction else None
        )
        
        CHAT_SESSION_CACHE[cache_key] = {
            "session": chat_session,
            "created_at": time.time()
        }
        
        return chat_session
    except Exception as e:
        logger.error(f"Chat session creation error: {e}", exc_info=True)
        return None

async def compress_context_if_needed(
    client,
    model: str,
    messages: List[dict],
    max_tokens: int
) -> List[dict]:
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (í•„ìš”í•œ ê²½ìš°)"""
    # í† í° ìˆ˜ ê³„ì‚°
    total_tokens = 0
    for msg in messages:
        token_count = count_tokens_with_tiktoken(msg["content"], model)
        total_tokens += token_count.get("input_tokens", 0)
    
    # ì••ì¶•ì´ í•„ìš”í•œì§€ í™•ì¸
    if total_tokens < max_tokens * CONTEXT_COMPRESSION_THRESHOLD:
        return messages
    
    logger.info(f"Context compression needed: {total_tokens} tokens > {max_tokens * CONTEXT_COMPRESSION_THRESHOLD}")
    
    # ìµœì‹  ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    keep_recent = 3  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ ìœ ì§€
    recent_messages = messages[-keep_recent:]
    old_messages = messages[:-keep_recent]
    
    if not old_messages:
        return recent_messages
    
    # ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    try:
        summary_content = "\n".join([
            f"{msg['role']}: {msg['content']}" 
            for msg in old_messages
        ])
        
        summary_response = client.models.generate_content(
            model="gemini-2.5-flash",  # ìš”ì•½ì€ ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            contents=[f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš” (í•µì‹¬ ë‚´ìš©ë§Œ):\n\n{summary_content}"],
            config=types.GenerateContentConfig(
                temperature=0.3,
                max_output_tokens=512,
                thinking_config=types.ThinkingConfig(thinking_budget=0)  # ìš”ì•½ì€ ì‚¬ê³  ì—†ì´
            )
        )
        
        # ìš”ì•½ëœ ë©”ì‹œì§€ë¡œ êµì²´
        compressed_messages = [
            {"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_response.text}"}
        ] + recent_messages
        
        logger.info(f"Context compressed: {len(messages)} -> {len(compressed_messages)} messages")
        return compressed_messages
        
    except Exception as e:
        logger.error(f"Context compression error: {e}", exc_info=True)
        return messages[-keep_recent:]  # ì‹¤íŒ¨ì‹œ ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€

class StreamingBuffer:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë²„í¼ë§"""
    def __init__(self, buffer_size: int = STREAMING_BUFFER_SIZE):
        self.buffer = []
        self.buffer_size = buffer_size
        self.current_size = 0
        self.last_flush = time.time()
    
    def add_chunk(self, chunk: str) -> bool:
        """ì²­í¬ ì¶”ê°€, í”ŒëŸ¬ì‹œ í•„ìš”ì‹œ True ë°˜í™˜"""
        self.buffer.append(chunk)
        self.current_size += len(chunk.encode('utf-8'))
        
        # ë²„í¼ê°€ ê°€ë“ ì°¼ê±°ë‚˜ ì¼ì • ì‹œê°„ì´ ì§€ë‚œ ê²½ìš° í”ŒëŸ¬ì‹œ
        now = time.time()
        return (self.current_size >= self.buffer_size or 
                now - self.last_flush >= STREAMING_FLUSH_INTERVAL)
    
    def flush(self) -> str:
        """ë²„í¼ ë‚´ìš© ë°˜í™˜ ë° ì´ˆê¸°í™”"""
        if not self.buffer:
            return ""
        
        content = "".join(self.buffer)
        self.buffer.clear()
        self.current_size = 0
        self.last_flush = time.time()
        return content

# ============================================================================
# ìµëª… ì‚¬ìš©ì ì±„íŒ… ê´€ë ¨ í•¨ìˆ˜ë“¤
# ============================================================================

def get_client_ip(request: Request) -> str:
    """í´ë¼ì´ì–¸íŠ¸ IP ì£¼ì†Œ ì¶”ì¶œ"""
    # X-Forwarded-For í—¤ë” í™•ì¸ (í”„ë¡ì‹œ/ë¡œë“œë°¸ëŸ°ì„œ í™˜ê²½)
    forwarded_for = request.headers.get("X-Forwarded-For")
    if forwarded_for:
        # ì²« ë²ˆì§¸ IP ì£¼ì†Œ ì‚¬ìš© (ì›ë³¸ í´ë¼ì´ì–¸íŠ¸ IP)
        return forwarded_for.split(",")[0].strip()
    
    # X-Real-IP í—¤ë” í™•ì¸ (Nginx ë“±)
    real_ip = request.headers.get("X-Real-IP")
    if real_ip:
        return real_ip.strip()
    
    # ì§ì ‘ ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ IP
    return request.client.host if request.client else "unknown"

def generate_anonymous_session_id() -> str:
    """ìµëª… ì‚¬ìš©ì ì„¸ì…˜ ID ìƒì„±"""
    return str(uuid.uuid4())

async def check_anonymous_rate_limit(
    request: Request, 
    db: Session,
    session_id: str,
    limit: int = 5
) -> tuple[bool, int, str]:
    """
    ìµëª… ì‚¬ìš©ì ì‚¬ìš©ëŸ‰ ì œí•œ í™•ì¸
    
    Returns:
        (is_limit_exceeded, current_usage, ip_address)
    """
    ip_address = get_client_ip(request)
    
    # ì‚¬ìš©ëŸ‰ í™•ì¸
    current_usage = crud_anonymous_usage.get_usage_count(db, session_id, ip_address)
    is_limit_exceeded = crud_anonymous_usage.check_usage_limit(db, session_id, ip_address, limit)
    
    return is_limit_exceeded, current_usage, ip_address

async def generate_anonymous_gemini_stream_response(
    request: Request,
    messages: list,
    model: str,
    session_id: str,
    ip_address: str,
    db: Session
) -> AsyncGenerator[str, None]:
    """ìµëª… ì‚¬ìš©ìë¥¼ ìœ„í•œ ì œí•œëœ Gemini ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    try:
        client = get_gemini_client()
        if not client:
            yield f"data: {json.dumps({'error': 'AI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'})}\n\n"
            return
        
        # ì œí•œëœ ì‹œìŠ¤í…œ ì§€ì‹œ (ê°„ë‹¨í•œ ë‹µë³€ ìœ ë„)
        system_instruction = """ë‹¹ì‹ ì€ Sungblab AI êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
í•™ìŠµìê°€ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¹œê·¼í•˜ê³  ëª…í™•í•œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ì€ 500ìë¥¼ ë„˜ì§€ ì•Šë„ë¡ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ë˜, í•µì‹¬ ë‚´ìš©ì€ ë†“ì¹˜ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”."""
        
        # ì œí•œëœ ìƒì„± ì„¤ì • (ë¹„ìš© ì ˆì•½)
        generation_config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            temperature=0.7,
            max_output_tokens=512,  # ìµëª… ì‚¬ìš©ìëŠ” ì§§ì€ ë‹µë³€ë§Œ
            # ìµëª… ì‚¬ìš©ìëŠ” function calling, grounding ë“± ê³ ê¸‰ ê¸°ëŠ¥ ì œí•œ
        )
        
        # ë©”ì‹œì§€ í¬ë§· ë³€í™˜
        formatted_messages = []
        for msg in messages:
            role = "user" if msg["role"] == "user" else "model"
            formatted_messages.append({
                "role": role,
                "parts": [{"text": msg["content"]}]
            })
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        response = client.models.generate_content_stream(
            model=model,
            contents=formatted_messages,
            config=generation_config
        )
        
        accumulated_content = ""
        
        for chunk in response:
            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
            if await request.is_disconnected():
                logger.warning("Anonymous client disconnected. Stopping stream.")
                break
                
            if chunk.candidates and len(chunk.candidates) > 0:
                candidate = chunk.candidates[0]
                
                if candidate.content and candidate.content.parts:
                    for part in candidate.content.parts:
                        if part.text:
                            accumulated_content += part.text
                            try:
                                yield f"data: {json.dumps({'content': part.text})}\n\n"
                            except (ConnectionError, BrokenPipeError, GeneratorExit):
                                logger.warning("Anonymous client disconnected during streaming")
                                return
        
        # ì‚¬ìš©ëŸ‰ ì¦ê°€ (ì„±ê³µì ìœ¼ë¡œ ì‘ë‹µì´ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ)
        if accumulated_content.strip():
            crud_anonymous_usage.increment_usage(db, session_id, ip_address)
            
    except Exception as e:
        logger.error(f"Anonymous chat error: {e}", exc_info=True)
        yield f"data: {json.dumps({'error': 'ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'})}\n\n"


# ============================================================================
# ìµëª… ì‚¬ìš©ì API ì—”ë“œí¬ì¸íŠ¸ë“¤
# ============================================================================

@router.post("/anonymous-chat")
async def anonymous_chat(
    request: Request,
    session_id: str = Form(...),
    message: str = Form(...),
    model: str = Form("gemini-2.5-flash"),  # ìµëª… ì‚¬ìš©ìëŠ” Flash ëª¨ë¸ë§Œ
    db: Session = Depends(get_db)
):
    """ìµëª… ì‚¬ìš©ìë¥¼ ìœ„í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ë¡œê·¸ì¸ ë¶ˆí•„ìš”, 5íšŒ ì œí•œ)"""
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not message.strip():
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        if len(message) > 2000:  # ìµëª… ì‚¬ìš©ìëŠ” ì§§ì€ ë©”ì‹œì§€ë§Œ
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. (ìµœëŒ€ 2000ì)")
        
        # ì„¸ì…˜ ID ê²€ì¦
        try:
            uuid.UUID(session_id)  # ìœ íš¨í•œ UUIDì¸ì§€ í™•ì¸
        except ValueError:
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ì„¸ì…˜ IDì…ë‹ˆë‹¤.")
        
        # ì‚¬ìš©ëŸ‰ ì œí•œ í™•ì¸
        is_limit_exceeded, current_usage, ip_address = await check_anonymous_rate_limit(
            request, db, session_id
        )
        
        if is_limit_exceeded:
            raise HTTPException(
                status_code=429, 
                detail={
                    "message": "ìµëª… ì±„íŒ… íšŸìˆ˜ë¥¼ ëª¨ë‘ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. íšŒì›ê°€ì…í•˜ê³  ë” ë§ì€ ê¸°ëŠ¥ì„ ì´ìš©í•´ë³´ì„¸ìš”!",
                    "current_usage": current_usage,
                    "limit": 5
                }
            )
        
        # í—ˆìš©ëœ ëª¨ë¸ì¸ì§€ í™•ì¸ (ìµëª… ì‚¬ìš©ìëŠ” Flashë§Œ)
        if model not in ["gemini-2.5-flash"]:
            model = "gemini-2.5-flash"
        
        # ê°„ë‹¨í•œ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (ìµœê·¼ 5ê°œë§Œ)
        messages = [
            {"role": "user", "content": message}
        ]
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        async def generate_anonymous_stream():
            async for chunk in generate_anonymous_gemini_stream_response(
                request, messages, model, session_id, ip_address, db
            ):
                yield chunk
        
        return StreamingResponse(
            generate_anonymous_stream(),
            media_type="text/plain",
            headers={
                "X-Anonymous-Usage": str(current_usage + 1),
                "X-Anonymous-Limit": "5"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Anonymous chat error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@router.get("/anonymous-usage/{session_id}")
@retry_db_operation(max_retries=3, delay=1.0)
async def get_anonymous_usage(
    request: Request,
    session_id: str,
    db: Session = Depends(get_robust_db)
):
    """ìµëª… ì‚¬ìš©ìì˜ í˜„ì¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    
    try:
        # ì„¸ì…˜ ID ê²€ì¦
        try:
            uuid.UUID(session_id)
        except ValueError:
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ì„¸ì…˜ IDì…ë‹ˆë‹¤.")
        
        ip_address = get_client_ip(request)
        current_usage = crud_anonymous_usage.get_usage_count(db, session_id, ip_address)
        
        return {
            "session_id": session_id,
            "current_usage": current_usage,
            "limit": 5,
            "remaining": max(0, 5 - current_usage),
            "is_limit_exceeded": current_usage >= 5
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Anonymous usage check error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@router.post("/anonymous-session")
async def create_anonymous_session(request: Request):
    """ìƒˆë¡œìš´ ìµëª… ì„¸ì…˜ ID ìƒì„±"""
    
    try:
        session_id = generate_anonymous_session_id()
        ip_address = get_client_ip(request)
        
        return {
            "session_id": session_id,
            "ip_address": ip_address,  # ë””ë²„ê¹…ìš© (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì œê±°)
            "limit": 5,
            "message": "ìµëª… ì„¸ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 5ë²ˆì˜ ë¬´ë£Œ ì±„íŒ…ì„ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        logger.error(f"Anonymous session creation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì„¸ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


# ê´€ë¦¬ììš© ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒì )
@router.get("/admin/anonymous-stats")
async def get_anonymous_stats(
    date: Optional[str] = None,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ìµëª… ì‚¬ìš©ì í†µê³„ ì¡°íšŒ (ê´€ë¦¬ìë§Œ)"""
    
    # ê´€ë¦¬ì ê¶Œí•œ í™•ì¸ (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
    if not current_user.email.endswith("@admin.com"):  # ì‹¤ì œ ê´€ë¦¬ì ì¡°ê±´ìœ¼ë¡œ ìˆ˜ì •
        raise HTTPException(status_code=403, detail="ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    try:
        target_date = None
        if date:
            target_date = datetime.fromisoformat(date)
            
        stats = crud_anonymous_usage.get_daily_stats(db, target_date)
        return stats
        
    except Exception as e:
        logger.error(f"Anonymous stats error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


# í”„ë¡¬í”„íŠ¸ ê°œì„  API (ì¼ë°˜ ì±„íŒ…ìš©)
@router.post("/improve-prompt")
async def improve_chat_prompt(
    original_prompt: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ AIê°€ ê°œì„ í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤ (ì¼ë°˜ ì±„íŒ…ìš©)."""
    try:
        # Gemini 2.0 Flash-Lite í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = genai.Client(api_key=settings.GEMINI_API_KEY)

        # í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        improvement_prompt = """ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•˜ê³ , ë” ëª…í™•í•˜ê³  íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.

## ê°œì„  ì›ì¹™:
1. **ëª…í™•ì„±**: ëª¨í˜¸í•œ í‘œí˜„ì„ êµ¬ì²´ì ìœ¼ë¡œ ê°œì„ 
2. **êµ¬ì¡°í™”**: ìš”ì²­ì‚¬í•­ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì •ë¦¬
3. **ë§¥ë½ ì œê³µ**: í•„ìš”í•œ ë°°ê²½ ì •ë³´ ì¶”ê°€
4. **êµ¬ì²´ì„±**: ì¶”ìƒì  ìš”ì²­ì„ êµ¬ì²´ì ìœ¼ë¡œ ë³€í™˜
5. **ì‹¤í–‰ ê°€ëŠ¥ì„±**: AIê°€ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì¡°ì •

## ê°œì„  ë°©ë²•:
- í•µì‹¬ ì˜ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ í‘œí˜„ ë°©ì‹ ê°œì„ 
- ë‹¨ê³„ë³„ ìš”ì²­ì´ í•„ìš”í•œ ê²½ìš° ìˆœì„œ ëª…ì‹œ
- ì˜ˆì‹œë‚˜ í˜•ì‹ì´ í•„ìš”í•œ ê²½ìš° êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ
- ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ê°„ê²°í•˜ê²Œ ìœ ì§€

ì‚¬ìš©ìì˜ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ë‚´ìš© ì—†ì´ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë§Œ ì œê³µí•˜ì„¸ìš”."""

        # ê°œì„  ìš”ì²­ ë©”ì‹œì§€ êµ¬ì„± (Gemini API v2.5 í˜•ì‹)
        content_text = f"ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:\n\n{original_prompt}"

        # Gemini 2.0 Flash-Liteë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„  ìš”ì²­
        response = client.models.generate_content(
            model="gemini-2.0-flash-lite",
            contents=[content_text],
            config=types.GenerateContentConfig(
                system_instruction=improvement_prompt,
                temperature=0.3,
                max_output_tokens=2048
            )
        )

        if not response.text:
            raise HTTPException(status_code=500, detail="Failed to improve prompt")

        improved_prompt = response.text.strip()
        
        return {
            "original_prompt": original_prompt,
            "improved_prompt": improved_prompt,
            "status": "success"
        }

    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"í”„ë¡¬í”„íŠ¸ ê°œì„ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
