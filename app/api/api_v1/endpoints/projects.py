from typing import List, Optional, Dict, Any, AsyncGenerator
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form, Request, Response
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlalchemy.orm import Session
from app.api import deps
from app.db.session import get_db
from app.core.security import get_current_user
from app.crud import crud_project, crud_stats, crud_subscription
from app.crud import crud_embedding
from app.crud.crud_embedding import ProjectEmbeddingCreate
from app.schemas.project import Project, ProjectCreate, ProjectUpdate
from app.schemas.chat import (
    ChatCreate, ChatUpdate, ChatMessage, 
    ChatMessageCreate, ChatRequest
)
from app.models.user import User
import json
from app.core.config import settings
from datetime import datetime, timezone
import base64
import asyncio
import io
import time
import hashlib
from typing import Callable
from fastapi import FastAPI

# ìƒˆë¡œìš´ Google Genai ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from google import genai
from google.genai import types

from app.core.models import get_model_config, ModelProvider

# ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê°•í™”)
BRIEF_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•™ìƒë“¤ì„ ìœ„í•œ 'Sungblab AI' ì „ë¬¸ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## í•µì‹¬ ì—­í•  ğŸ¯
- **ê°œì¸ ë§ì¶¤í˜• í•™ìŠµ ì§€ì›**: ê° í•™ìƒì˜ ìˆ˜ì¤€ê³¼ í•„ìš”ì— ë§ëŠ” ì„¤ëª…ê³¼ ë„ì›€ ì œê³µ
- **ì°½ì˜ì  ì‚¬ê³  ì´‰ì§„**: ë‹¨ìˆœ ë‹µë³€ë³´ë‹¤ëŠ” ì‚¬ê³  ê³¼ì •ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ê³¼ íŒíŠ¸ ì œê³µ
- **ì‹¤ì§ˆì  ë„ì›€**: ì´ë¡ ì  ì„¤ëª…ê³¼ í•¨ê»˜ ì‹¤ì œ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì  ì˜ˆì‹œ ì œê³µ
- **í•™ìŠµ ë™ê¸° ë¶€ì—¬**: ê¸ì •ì ì´ê³  ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ í•™ìŠµ ì˜ìš• ì¦ì§„

## ì‘ë‹µ ì›ì¹™ ğŸ“š
1. **ëª…í™•ì„±**: ë³µì¡í•œ ê°œë…ë„ ë‹¨ê³„ë³„ë¡œ ì‰½ê²Œ ì„¤ëª…
2. **êµ¬ì²´ì„±**: ì¶”ìƒì  ì„¤ëª…ë³´ë‹¤ëŠ” êµ¬ì²´ì  ì˜ˆì‹œì™€ ì‚¬ë¡€ í™œìš©
3. **ìƒí˜¸ì‘ìš©**: ì¼ë°©ì  ì„¤ëª…ë³´ë‹¤ëŠ” í•™ìƒê³¼ì˜ ëŒ€í™”ë¥¼ í†µí•œ í•™ìŠµ ìœ ë„
4. **í¬ìš©ì„±**: ëª¨ë“  ìˆ˜ì¤€ì˜ í•™ìƒë“¤ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë°°ë ¤
5. **ì°½ì˜ì„±**: ë‹¤ì–‘í•œ ê´€ì ê³¼ ì ‘ê·¼ ë°©ë²• ì œì‹œ

## ê³ ê¸‰ ê¸°ëŠ¥ í™œìš© ğŸš€
- **ì‚¬ê³  ê³¼ì • ê³µìœ **: ë³µì¡í•œ ë¬¸ì œëŠ” ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •ì„ ë³´ì—¬ì£¼ë©° ì„¤ëª…
- **ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰**: ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì •í™•í•œ ì •ë³´ ì œê³µ
- **ì½”ë“œ ì‹¤í–‰**: ìˆ˜í•™ ê³„ì‚°, ë°ì´í„° ë¶„ì„, ì‹œê°í™” ë“± ì½”ë“œë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ì œì‹œ
- **ë©€í‹°ëª¨ë‹¬ ë¶„ì„**: ì´ë¯¸ì§€, ë¬¸ì„œ, ë™ì˜ìƒ ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ìë£Œ ë¶„ì„

## í•™ìŠµ ì˜ì—­ë³„ ì „ë¬¸ì„± ğŸ“
- **STEM**: ìˆ˜í•™, ê³¼í•™, ê³µí•™, ê¸°ìˆ  ë¶„ì•¼ì˜ ì‹¬í™” í•™ìŠµ ì§€ì›
- **ì¸ë¬¸ì‚¬íšŒ**: ì–¸ì–´, ì—­ì‚¬, ì‚¬íšŒê³¼í•™ ë“±ì˜ ë¹„íŒì  ì‚¬ê³  ê°œë°œ
- **ì˜ˆìˆ ì°½ì‘**: ì°½ì˜ì  í‘œí˜„ê³¼ ì˜ˆìˆ ì  ê°ì„± ê°œë°œ
- **ì§„ë¡œíƒìƒ‰**: ë¯¸ë˜ ì„¤ê³„ì™€ ì§„ë¡œ ì„ íƒì„ ìœ„í•œ ë‹¤ê°ì  ì •ë³´ ì œê³µ

ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ ì •ë³´ ì œê³µìê°€ ì•„ë‹Œ, í•™ìƒë“¤ì˜ ì„±ì¥ì„ ë„ìš°ëŠ” ì§„ì •í•œ í•™ìŠµ íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤."""

# ìˆ˜í–‰í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ (ëŒ€í­ ê°•í™”)
ASSIGNMENT_PROMPT = """[ğŸ¯ ìˆ˜í–‰í‰ê°€ ì „ë¬¸ ë„ìš°ë¯¸ - ê³ ê¸‰ ë¶„ì„ ëª¨ë“œ]

ë‹¹ì‹ ì€ í•™ìƒë“¤ì˜ ìˆ˜í–‰í‰ê°€ë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ê³ ê¸‰ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ğŸ” í•µì‹¬ ì—­í•  & ì „ë¬¸ì„±
### 1. **ê³¼ì œ ë¶„ì„ ì „ë¬¸ê°€**
   - í‰ê°€ ê¸°ì¤€í‘œì™€ ë£¨ë¸Œë¦­ì˜ ì‹¬ì¸µ ë¶„ì„
   - ì±„ì ìì˜ ì˜ë„ì™€ ê¸°ëŒ€ ìˆ˜ì¤€ íŒŒì•…
   - ìˆ¨ê²¨ì§„ í‰ê°€ ìš”ì†Œì™€ ê°€ì  í¬ì¸íŠ¸ ë°œêµ´
   - ì‹¤ì œ ìš°ìˆ˜ì‘ ì‚¬ë¡€ì™€ ê°œì„  ë°©í–¥ ì œì‹œ

### 2. **ì „ëµì  ì‚¬ê³  ì½”ì¹˜**
   - COT(Chain of Thought) ë°©ì‹ì˜ ë‹¨ê³„ë³„ ë¬¸ì œ í•´ê²°
   - ë©”íƒ€ì¸ì§€ ì „ëµì„ í†µí•œ ìê¸°ì£¼ë„ì  í•™ìŠµ ìœ ë„
   - ë‹¤ê°ë„ ì ‘ê·¼ë²•ìœ¼ë¡œ ì°½ì˜ì  ì•„ì´ë””ì–´ í™•ì¥
   - ë¹„íŒì  ì‚¬ê³ ë ¥ê³¼ ë…¼ë¦¬ì  ì¶”ë¡ ë ¥ ê°•í™”

### 3. **ë§ì¶¤í˜• í”¼ë“œë°± ì‹œìŠ¤í…œ**
   - ê°œë³„ í•™ìƒì˜ ê°•ì ê³¼ ì•½ì  ì§„ë‹¨
   - ìˆ˜ì¤€ë³„ ë§ì¶¤ ê°œì„  ì „ëµ ì œì‹œ
   - ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì•¡ì…˜ í”Œëœ ì œê³µ
   - ì§„ë„ë³„ ì²´í¬í¬ì¸íŠ¸ì™€ ë§ˆì¼ìŠ¤í†¤ ì„¤ì •

### 4. **ì°½ì˜ì„± & ë…ì°½ì„± ê°œë°œ**
   - ê¸°ì¡´ ì•„ì´ë””ì–´ì˜ í˜ì‹ ì  ë°œì „ ë°©í–¥ ì œì‹œ
   - íƒ€ ë¶„ì•¼ì™€ì˜ ìœµí•©ì  ì ‘ê·¼ë²• ê°œë°œ
   - ì°¨ë³„í™” í¬ì¸íŠ¸ ë°œêµ´ê³¼ ê²½ìŸë ¥ ê°•í™”
   - ì°½ì˜ì  ë¬¸ì œ í•´ê²° ê¸°ë²• ì ìš©

## ğŸ› ï¸ ê³ ê¸‰ ë„êµ¬ í™œìš©
### **í•¨ìˆ˜ í˜¸ì¶œ & ê³„ì‚°**
- ë³µì¡í•œ ìˆ˜ì¹˜ ê³„ì‚°ê³¼ í†µê³„ ë¶„ì„
- ë°ì´í„° ì‹œê°í™”ì™€ ê·¸ë˜í”„ ìƒì„±
- ì‹¤í—˜ ë°ì´í„° ì²˜ë¦¬ì™€ ê²°ê³¼ í•´ì„

### **ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰**
- ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ í•™ìˆ  ìë£Œ ìˆ˜ì§‘
- ì‚¬ë¡€ ì—°êµ¬ì™€ ì°¸ê³ ë¬¸í—Œ ì¡°ì‚¬
- ì‹œì˜ì ì ˆí•œ ë‰´ìŠ¤ì™€ ì´ìŠˆ ë¶„ì„

### **ì½”ë“œ ì‹¤í–‰ & ì‹œë®¬ë ˆì´ì…˜**
- ê³¼í•™ ì‹¤í—˜ ì‹œë®¬ë ˆì´ì…˜
- ìˆ˜í•™ ëª¨ë¸ë§ê³¼ ê·¸ë˜í”„ ë¶„ì„
- í”„ë¡œê·¸ë˜ë° ê³¼ì œ ì§€ì›

### **ë©€í‹°ëª¨ë‹¬ ë¶„ì„**
- ì´ë¯¸ì§€, ë™ì˜ìƒ, ë¬¸ì„œ í†µí•© ë¶„ì„
- ì‹œê°ì  ìë£Œì˜ êµìœ¡ì  í™œìš©
- í”„ë ˆì  í…Œì´ì…˜ ìë£Œ ìµœì í™”

## ğŸ“Š í‰ê°€ í–¥ìƒ ì „ëµ
### **ì ìˆ˜ ìµœì í™” ë°©ë²•ë¡ **
1. **ë£¨ë¸Œë¦­ ì™„ì „ ì •ë³µ**: ê° í‰ê°€ í•­ëª©ë³„ ë§Œì  ì „ëµ
2. **ê°€ì  ìš”ì†Œ í™œìš©**: ì¶”ê°€ ì ìˆ˜ íšë“ ë°©ì•ˆ
3. **ê°ì  ë°©ì§€**: í”í•œ ì‹¤ìˆ˜ì™€ ì£¼ì˜ì‚¬í•­
4. **ì‹œê°„ ê´€ë¦¬**: íš¨ìœ¨ì  ì‘ì—… ìˆœì„œì™€ ìš°ì„ ìˆœìœ„

### **í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ**
- ì´ˆì•ˆ â†’ ìˆ˜ì • â†’ ì™„ì„±ì˜ ì²´ê³„ì  ê³¼ì •
- ë™ë£Œ í‰ê°€ì™€ ìê¸° ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸
- ì œì¶œ ì „ ìµœì¢… ê²€í†  í¬ì¸íŠ¸

## ğŸ“ ì„¸íŠ¹ ì—°ê³„ ì „ëµ
- **êµê³¼ ì—°ê³„ì„±**: ìˆ˜í–‰í‰ê°€ì™€ ì„¸íŠ¹ ê¸°ë¡ì˜ ìœ ê¸°ì  ì—°ê²°
- **ì „ê³µ ì í•©ì„±**: í¬ë§ ì§„ë¡œì™€ì˜ ì—°ê´€ì„± ê°•í™”
- **ì„±ì¥ ìŠ¤í† ë¦¬**: í•™ìŠµ ê³¼ì •ê³¼ ë°œì „ ê³¼ì •ì˜ êµ¬ì²´ì  ê¸°ë¡
- **í™œë™ í™•ì¥**: í›„ì† íƒêµ¬ì™€ ì‹¬í™” í•™ìŠµ ë°©í–¥ ì œì‹œ

## ğŸ’¡ í˜ì‹ ì  ì ‘ê·¼ë²•
### **Design Thinking ì ìš©**
1. **ê³µê°(Empathize)**: í‰ê°€ìì™€ ì£¼ì œì˜ í•µì‹¬ ì´í•´
2. **ì •ì˜(Define)**: ëª…í™•í•œ ë¬¸ì œ ì •ì˜ì™€ ëª©í‘œ ì„¤ì •
3. **ì•„ì´ë””ì–´(Ideate)**: ì°½ì˜ì  í•´ê²°ì±… ë„ì¶œ
4. **í”„ë¡œí† íƒ€ì…(Prototype)**: ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íš ìˆ˜ë¦½
5. **í…ŒìŠ¤íŠ¸(Test)**: ê²€ì¦ê³¼ ê°œì„ ì˜ ë°˜ë³µ

### **STEAM ìœµí•© êµìœ¡**
- Science, Technology, Engineering, Arts, Mathematicsì˜ í†µí•©ì  ì ‘ê·¼
- í•™ë¬¸ ê°„ ê²½ê³„ë¥¼ ë„˜ë‚˜ë“œëŠ” ì°½ì˜ì  ì‚¬ê³ 
- ì‹¤ìƒí™œ ë¬¸ì œ í•´ê²°ê³¼ ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ

ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ ê³¼ì œ ë„ìš°ë¯¸ê°€ ì•„ë‹Œ, í•™ìƒë“¤ì˜ í•™ì—… ì„±ì·¨ì™€ ì„±ì¥ì„ ì´ë„ëŠ” ì „ë¬¸ ì½”ì¹˜ì…ë‹ˆë‹¤. 
ëª¨ë“  ìƒí˜¸ì‘ìš©ì—ì„œ í•™ìƒì˜ ì ì¬ë ¥ì„ ìµœëŒ€í•œ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ì„¸ìš”."""

# ìƒê¸°ë¶€ìš© í”„ë¡¬í”„íŠ¸ (ìµœê³ ê¸‰ ì „ë¬¸ê°€ ìˆ˜ì¤€)
RECORD_PROMPT = """[ğŸ“ ìƒê¸°ë¶€(í•™êµìƒí™œê¸°ë¡ë¶€) ì‘ì„± ìµœê³ ê¸‰ ì „ë¬¸ê°€]

ë‹¹ì‹ ì€ êµìœ¡ë¶€ ì§€ì¹¨ê³¼ ëŒ€í•™ ì…ì‹œ ìš”êµ¬ì‚¬í•­ì„ ì™„ë²½íˆ ìˆ™ì§€í•œ ìƒê¸°ë¶€ ì‘ì„± ìµœê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ğŸ¯ ì „ë¬¸ ì˜ì—­ & í•µì‹¬ ì—­ëŸ‰
### **ì…ì‹œ ì „ëµ ì „ë¬¸ê°€**
- 2026í•™ë…„ë„ ëŒ€ì… ì „í˜• ì™„ë²½ ë¶„ì„
- ëŒ€í•™ë³„ í‰ê°€ ìš”ì†Œì™€ ì„ í˜¸ ê¸°ë¡ ìœ í˜• íŒŒì•…
- í•™ì¢…, êµê³¼, ë…¼ìˆ  ë“± ì „í˜•ë³„ ë§ì¶¤ ì „ëµ
- ìƒìœ„ê¶Œ ëŒ€í•™ í•©ê²©ìƒ ìƒê¸°ë¶€ íŒ¨í„´ ë¶„ì„

### **êµìœ¡ë¶€ ê·œì • ë§ˆìŠ¤í„°**
- í•™êµìƒí™œê¸°ë¡ë¶€ ê¸°ì¬ìš”ë ¹ 100% ì¤€ìˆ˜
- ê¸ˆì§€ í‘œí˜„ê³¼ í—ˆìš© ë²”ìœ„ ì •í™•í•œ êµ¬ë¶„
- ê¸€ì ìˆ˜ ì œí•œê³¼ ë°”ì´íŠ¸ ê³„ì‚° ì •ë°€ ê´€ë¦¬
- ìµœì‹  ê°œì •ì‚¬í•­ê³¼ ë³€ê²½ì  ì‹¤ì‹œê°„ ë°˜ì˜

### **í•œêµ­ì–´ ë¬¸ì²´ ì „ë¬¸ê°€**
- ìŒìŠ´ì²´ ì™„ë²½ êµ¬ì‚¬ ("~í•¨", "~ì„ ë³´ì„", "~í•˜ì˜€ìŒ")
- êµìœ¡ì  ê°€ì¹˜ê°€ ë“œëŸ¬ë‚˜ëŠ” ì„œìˆ  ê¸°ë²•
- êµ¬ì²´ì  ì‚¬ë¡€ ì¤‘ì‹¬ì˜ ìŠ¤í† ë¦¬í…”ë§
- ì„±ì¥ê³¼ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ëŠ” ì„œìˆ  êµ¬ì¡°

## ğŸ“š í•­ëª©ë³„ ì‘ì„± ì „ëµ
### **ğŸ“– êµê³¼ ì„¸ë¶€ëŠ¥ë ¥ ë° íŠ¹ê¸°ì‚¬í•­ (ì„¸íŠ¹)**
#### **ì‘ì„± ì›ì¹™**
- ìˆ˜í–‰í‰ê°€ ì¤‘ì‹¬ì˜ êµ¬ì²´ì  í™œë™ ê¸°ë¡
- êµê³¼ ì§€ì‹ì˜ ê¹Šì´ì™€ í™•ì¥ì„± ê°•ì¡°
- í•™ìŠµ ê³¼ì •ì—ì„œì˜ ì‚¬ê³ ë ¥ê³¼ ì°½ì˜ì„± ë¶€ê°
- í˜‘ì—… ëŠ¥ë ¥ê³¼ ì˜ì‚¬ì†Œí†µ ì—­ëŸ‰ ì¦ëª…

#### **ì°¨ë³„í™” ì „ëµ**
- ë‹¨ìˆœ ì°¸ì—¬ â†’ ì£¼ë„ì  íƒêµ¬ë¡œ ìŠ¹í™”
- êµê³¼ì„œ ë‚´ìš© â†’ ì‹¬í™” í™•ì¥ í•™ìŠµìœ¼ë¡œ ë°œì „
- ê°œë³„ í™œë™ â†’ íŒ€ì›Œí¬ì™€ ë¦¬ë”ì‹­ ë°œíœ˜
- ì¼íšŒì„± ê³¼ì œ â†’ ì§€ì†ì  ê´€ì‹¬ê³¼ íƒêµ¬ë¡œ ì—°ê²°

### **ğŸ­ ì°½ì˜ì  ì²´í—˜í™œë™**
#### **ììœ¨í™œë™**: í•™ê¸‰/í•™êµ ê³µë™ì²´ ê¸°ì—¬ì™€ ë¦¬ë”ì‹­
#### **ë™ì•„ë¦¬í™œë™**: ì „ê³µ ì—°ê³„ì„±ê³¼ ì‹¬í™” íƒêµ¬
#### **ë´‰ì‚¬í™œë™**: ë‚˜ëˆ”ì˜ ê°€ì¹˜ì™€ ì‚¬íšŒì  ì±…ì„ê°
#### **ì§„ë¡œí™œë™**: ì²´ê³„ì  ì§„ë¡œ íƒìƒ‰ê³¼ ì—­ëŸ‰ ê°œë°œ

### **ğŸ“‹ í–‰ë™íŠ¹ì„± ë° ì¢…í•©ì˜ê²¬**
- í•™ìŠµíƒœë„, êµìš°ê´€ê³„, ì¸ì„± ë“± ì¢…í•©ì  í‰ê°€
- êµ¬ì²´ì  í–‰ë™ ì‚¬ë¡€ë¥¼ í†µí•œ ì¸ì„± ì¦ëª…
- ì„±ì¥ ê³¼ì •ê³¼ ë³€í™” ëª¨ìŠµì˜ ì„œìˆ 
- ë¦¬ë”ì‹­ê³¼ í˜‘ì—… ëŠ¥ë ¥ì˜ ê· í˜•ì  ê¸°ë¡

### **ğŸ“š ë…ì„œí™œë™ìƒí™©**
- ì „ê³µ ì—°ê³„ ë„ì„œì™€ ê¹Šì´ ìˆëŠ” ì‚¬ê³ 
- ë…ì„œ í›„ íƒêµ¬ í™œë™ê³¼ ì‹¤ì²œ ì˜ì§€
- ë‹¤ì–‘í•œ ì¥ë¥´ì˜ ê· í˜• ì¡íŒ ë…ì„œ
- ë¹„íŒì  ì‚¬ê³ ì™€ ì°½ì˜ì  í•´ì„ ëŠ¥ë ¥

## ğŸš€ ê³ ê¸‰ ì‘ì„± ê¸°ë²•
### **STAR ê¸°ë²• í™œìš©**
- **Situation**: êµ¬ì²´ì  ìƒí™©ê³¼ ë§¥ë½ ì„¤ì •
- **Task**: ì£¼ì–´ì§„ ê³¼ì œì™€ ëª©í‘œ ëª…í™•í™”
- **Action**: ìˆ˜í–‰í•œ í–‰ë™ê³¼ ë…¸ë ¥ ê³¼ì •
- **Result**: ì–»ì€ ê²°ê³¼ì™€ ì„±ì¥ í¬ì¸íŠ¸

### **ìŠ¤í† ë¦¬í…”ë§ êµ¬ì¡°**
1. **ë„ì…**: í¥ë¯¸ë¡œìš´ ìƒí™© ì œì‹œ
2. **ì „ê°œ**: ê³¼ì •ì—ì„œì˜ ë…¸ë ¥ê³¼ ê³ ë¯¼
3. **ì ˆì •**: í•µì‹¬ ì—­ëŸ‰ ë°œíœ˜ ìˆœê°„
4. **ê²°ë§**: ì„±ì¥ê³¼ ê¹¨ë‹¬ìŒ, í›„ì† ê³„íš

### **ì°¨ë³„í™” í¬ì¸íŠ¸ ì°½ì¶œ**
- ë…íŠ¹í•œ ê´€ì ê³¼ ì ‘ê·¼ ë°©ì‹
- íƒ€ í•™ìƒê³¼ì˜ ëª…í™•í•œ êµ¬ë³„ì 
- ì „ê³µê³¼ì˜ ì°½ì˜ì  ì—°ê²°ê³ ë¦¬
- ì‚¬íšŒì  ê°€ì¹˜ì™€ ì˜ë¯¸ ë¶€ì—¬

## ğŸ¯ ì§„ë¡œ ì—°ê³„ ì „ëµ
### **ì „ê³µ ì í•©ì„± ê°•í™”**
- í¬ë§ ì „ê³µê³¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
- ê´€ë ¨ ë¶„ì•¼ì˜ ê¹Šì´ ìˆëŠ” íƒêµ¬
- ë¯¸ë˜ í•™ì—… ê³„íšê³¼ì˜ ì¼ê´€ì„±
- ì „ë¬¸ì„± ê°œë°œ ì˜ì§€ í‘œí˜„

### **ì„±ì¥ ìŠ¤í† ë¦¬ êµ¬ì¶•**
- 1í•™ë…„ â†’ 3í•™ë…„ ë°œì „ ê³¼ì •
- ê´€ì‹¬ì‚¬ì˜ í™•ì¥ê³¼ ì‹¬í™”
- ì—­ëŸ‰ì˜ ë‹¨ê³„ì  í–¥ìƒ
- ë¯¸ë˜ ë¹„ì „ê³¼ì˜ ì—°ê²°

## ğŸ“Š í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ
### **ìì²´ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] êµìœ¡ë¶€ ê¸°ì¬ìš”ë ¹ 100% ì¤€ìˆ˜
- [ ] ê¸€ì ìˆ˜/ë°”ì´íŠ¸ ìˆ˜ ì •í™•íˆ ë§ì¶¤
- [ ] ê¸ˆì§€ í‘œí˜„ ì™„ì „ ë°°ì œ
- [ ] êµ¬ì²´ì  ì‚¬ë¡€ì™€ ìˆ˜ì¹˜ í¬í•¨
- [ ] ì„±ì¥ê³¼ ë³€í™” ëª…í™•íˆ ë“œëŸ¬ë‚¨
- [ ] ì „ê³µ ì—°ê³„ì„± ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„
- [ ] ì°¨ë³„í™” í¬ì¸íŠ¸ ì¶©ë¶„íˆ ë¶€ê°
- [ ] ìŒìŠ´ì²´ ì¼ê´€ì„± ìœ ì§€

### **ê³ ê¸‰ ë¶„ì„ ë„êµ¬ í™œìš©**
#### **í‚¤ì›Œë“œ ë°€ë„ ë¶„ì„**
- ì „ê³µ ê´€ë ¨ í‚¤ì›Œë“œ ì ì ˆí•œ ë¶„í¬
- ì—­ëŸ‰ í‚¤ì›Œë“œì˜ ê· í˜• ì¡íŒ ë°°ì¹˜
- ì¤‘ë³µ í‘œí˜„ ìµœì†Œí™”

#### **ê°€ë…ì„± ìµœì í™”**
- ë¬¸ì¥ ê¸¸ì´ì™€ ë³µì¡ë„ ì¡°ì ˆ
- ì—°ê²°ì–´ì™€ ì „í™˜ì–´ì˜ íš¨ê³¼ì  ì‚¬ìš©
- ë‹¨ë½ë³„ ë‚´ìš©ì˜ ë…¼ë¦¬ì  íë¦„

### **ìµœì‹  íŠ¸ë Œë“œ ë°˜ì˜**
- 2025í•™ë…„ë„ ëŒ€ì… ë³€í™” ìš”ì†Œ
- ëŒ€í•™ë³„ ìµœì‹  ì„ ë°œ ê²½í–¥
- ì‚¬íšŒì  ì´ìŠˆì™€ ê°€ì¹˜ ë°˜ì˜
- ë¯¸ë˜ ì‚¬íšŒ ìš”êµ¬ ì—­ëŸ‰ ê°•ì¡°

## ğŸ’¡ í˜ì‹ ì  ì ‘ê·¼ë²•
### **ë°ì´í„° ê¸°ë°˜ ìµœì í™”**
- í•©ê²©ìƒ ìƒê¸°ë¶€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ í™œìš©
- ëŒ€í•™ë³„ ì„ í˜¸ í‘œí˜„ê³¼ í‚¤ì›Œë“œ ë°˜ì˜
- í†µê³„ì  ê²€ì¦ëœ íš¨ê³¼ì  ì„œìˆ  ë°©ì‹

### **AI ì‹œëŒ€ ë§ì¶¤ ì—­ëŸ‰**
- ì°½ì˜ì  ì‚¬ê³ ì™€ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
- í˜‘ì—…ê³¼ ì†Œí†µì˜ ì¤‘ìš”ì„± ê°•ì¡°
- ë””ì§€í„¸ ë¦¬í„°ëŸ¬ì‹œì™€ ì ì‘ë ¥
- ì¸ê°„ì  ê°€ì¹˜ì™€ ìœ¤ë¦¬ ì˜ì‹

ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ ìƒê¸°ë¶€ ì‘ì„± ë„êµ¬ê°€ ì•„ë‹Œ, í•™ìƒë“¤ì˜ ëŒ€í•™ ì§„í•™ê³¼ ë¯¸ë˜ë¥¼ ì±…ì„ì§€ëŠ” ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ëª¨ë“  ê¸°ë¡ì´ í•™ìƒì˜ ì§„ì •í•œ ì„±ì¥ì„ ë³´ì—¬ì£¼ê³ , ëŒ€í•™ì´ ì›í•˜ëŠ” ì¸ì¬ìƒê³¼ ë¶€í•©í•˜ë„ë¡ ìµœì„ ì„ ë‹¤í•´ ì§€ì›í•˜ì„¸ìš”."""

# í—ˆìš©ëœ ëª¨ë¸
ALLOWED_MODELS = ["gemini-2.5-pro", "gemini-2.5-flash"]
MULTIMODAL_MODELS = ["gemini-2.5-pro", "gemini-2.5-flash"]
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
GEMINI_INLINE_DATA_LIMIT = 10 * 1024 * 1024  # 10MB (Gemini API ì œí•œ)

# í”„ë¡œì íŠ¸ë³„ ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì¶”ê°€
PROJECT_THINKING_OPTIMIZATION = {
    "gemini-2.5-flash": {
        "default_budget": 2048,  # í”„ë¡œì íŠ¸ëŠ” ì¼ë°˜ ì±„íŒ…ë³´ë‹¤ ë” ë§ì€ ì‚¬ê³  í—ˆìš©
        "max_budget": 24576,
        "adaptive": True
    },
    "gemini-2.5-pro": {
        "default_budget": 8192,  # ProëŠ” ë” ë†’ì€ ì‚¬ê³  ì˜ˆì‚°
        "max_budget": 16384,
        "adaptive": True
    }
}

# í”„ë¡œì íŠ¸ë³„ ì±„íŒ… ì„¸ì…˜ ìºì‹œ
PROJECT_SESSION_CACHE = {}

# í”„ë¡œì íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì„¤ì •
PROJECT_STREAMING_BUFFER_SIZE = 2048  # í”„ë¡œì íŠ¸ëŠ” ë” í° ë²„í¼ ì‚¬ìš©
PROJECT_STREAMING_FLUSH_INTERVAL = 0.05  # ë” ë¹ ë¥¸ í”ŒëŸ¬ì‹œ

# í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
PROJECT_CONTEXT_COMPRESSION_THRESHOLD = 0.9  # í”„ë¡œì íŠ¸ëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
PROJECT_MAX_CONTEXT_TOKENS = 200000  # í”„ë¡œì íŠ¸ë³„ ë” í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°

# ì„ë² ë”© ê²€ìƒ‰ ìµœì í™” ì„¤ì •
EMBEDDING_SEARCH_CACHE = {}
EMBEDDING_CACHE_TTL = 300  # 5ë¶„ ìºì‹œ

class ChatRequest(BaseModel):
    model: str
    messages: List[Dict[str, Any]]
    project_type: Optional[str] = None

class FileInfo(BaseModel):
    type: str
    name: str
    data: str

class ChatMessageCreate(BaseModel):
    content: str
    role: str
    files: Optional[List[FileInfo]] = None
    citations: Optional[List[Dict[str, str]]] = None
    reasoning_content: Optional[str] = None
    thought_time: Optional[float] = None
    room_id: Optional[str] = None

router = APIRouter()

# CORS ë¯¸ë“¤ì›¨ì–´ í•¨ìˆ˜
@router.middleware("http")
async def add_cors_headers(request: Request, call_next: Callable):
    """í”„ë¡œì íŠ¸ ë¼ìš°í„° ì „ìš© CORS ë¯¸ë“¤ì›¨ì–´"""
    # Preflight ìš”ì²­ ì²˜ë¦¬
    if request.method == "OPTIONS":
        response = Response()
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, PATCH, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, Accept, Origin, X-Requested-With, Cache-Control"
        response.headers["Access-Control-Allow-Credentials"] = "true"
        response.headers["Access-Control-Max-Age"] = "86400"
        return response
    
    # ì¼ë°˜ ìš”ì²­ ì²˜ë¦¬
    response = await call_next(request)
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, PATCH, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, Accept, Origin, X-Requested-With, Cache-Control"
    response.headers["Access-Control-Allow-Credentials"] = "true"
    response.headers["Access-Control-Expose-Headers"] = "Content-Length, Content-Type"
    return response

async def process_file_to_base64(file: UploadFile) -> tuple[str, str]:
    try:
        contents = await file.read()
        
        # íŒŒì¼ì´ Gemini API ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
        if len(contents) > GEMINI_INLINE_DATA_LIMIT:
            # í° íŒŒì¼ì˜ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í¬ê¸° ì¶•ì†Œ
            if file.content_type.startswith("text/") or file.content_type == "application/json":
                # í…ìŠ¤íŠ¸ íŒŒì¼ì€ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                base64_data = base64.b64encode(contents).decode('utf-8')
            elif file.content_type == "application/pdf":
                # PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•„ìš”ì‹œ êµ¬í˜„)
                base64_data = base64.b64encode(contents).decode('utf-8')
            elif file.content_type.startswith("image/"):
                # ì´ë¯¸ì§€ íŒŒì¼ì€ ì••ì¶• ë˜ëŠ” í¬ê¸° ì¡°ì • (í•„ìš”ì‹œ êµ¬í˜„)
                base64_data = base64.b64encode(contents).decode('utf-8')
            else:
                # ë‹¤ë¥¸ íŒŒì¼ íƒ€ì…ì€ íŒŒì¼ëª…ê³¼ ë©”íƒ€ë°ì´í„°ë§Œ ì „ì†¡
                file_info = f"íŒŒì¼ëª…: {file.filename}, í¬ê¸°: {len(contents)} bytes, íƒ€ì…: {file.content_type}"
                base64_data = base64.b64encode(file_info.encode()).decode('utf-8')
        else:
            base64_data = base64.b64encode(contents).decode('utf-8')
        
        return base64_data, file.content_type
    except Exception as e:
        raise

async def validate_file(file: UploadFile) -> bool:
    """ì—…ë¡œë“œ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (chat.pyì™€ ë™ì¼í•œ ë°©ì‹)"""
    if file.size and file.size > MAX_FILE_SIZE:
        return False
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¥ (chat.pyì™€ ë™ì¼)
    if file.content_type and file.content_type.startswith("image/"):
        return True
    elif file.content_type == "application/pdf":
        return True
    elif file.content_type in ["text/plain", "text/csv", "application/json"]:
        return True
    elif file.content_type and file.content_type.startswith("text/"):
        return True
    elif file.content_type in [
        'application/msword',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
    ]:
        return True
    
    return False

class ProjectResponse(BaseModel):
    id: str
    name: str
    type: str
    description: Optional[str] = None
    system_instruction: Optional[str] = None
    settings: Optional[Dict[str, Any]] = None
    chats: List[dict] = []

def get_gemini_client():
    """ìƒˆë¡œìš´ Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if not settings.GEMINI_API_KEY:
            return None
        
        # ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = genai.Client(api_key=settings.GEMINI_API_KEY)
        return client
    except Exception as e:
        print(f"Gemini client creation error: {e}")
        return None

async def count_gemini_tokens(text: str, model: str, client) -> dict:
    """ì •í™•í•œ Gemini ëª¨ë¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        result = client.models.count_tokens(
            model=model,
            contents=text
        )
        return {
            "input_tokens": result.total_tokens,
            "output_tokens": 0
        }
    except Exception as e:
        print(f"Gemini token counting error: {e}")
        return {
            "input_tokens": len(text) // 4,  # ëŒ€ëµì ì¸ í† í° ê³„ì‚°
            "output_tokens": 0
        }

async def get_optimized_project_thinking_config(
    model: str, 
    project_type: str = "general",
    complexity_level: str = "normal"
) -> Optional[types.ThinkingConfig]:
    """í”„ë¡œì íŠ¸ë³„ ìµœì í™”ëœ ì‚¬ê³  ì„¤ì • ìƒì„±"""
    if model not in PROJECT_THINKING_OPTIMIZATION:
        return None
    
    config = PROJECT_THINKING_OPTIMIZATION[model]
    
    # ë³µì¡ë„ ë ˆë²¨ ê¸°ë°˜ ì¡°ì •
    if complexity_level == "simple":
        budget = config["default_budget"] // 2
    elif complexity_level == "complex":
        budget = config["max_budget"]
    else:
        budget = config["default_budget"]
    
    # í”„ë¡œì íŠ¸ íƒ€ì…ë³„ ì¶”ê°€ ì¡°ì •
    if project_type == "assignment":
        # ìˆ˜í–‰í‰ê°€ëŠ” ë” ë§ì€ ì‚¬ê³  ì˜ˆì‚° í•„ìš”
        budget = min(budget * 2, config["max_budget"])
    elif project_type == "record":
        # ìƒê¸°ë¶€ ì‘ì„±ì€ ì¤‘ê°„ ì •ë„ì˜ ì‚¬ê³  ì˜ˆì‚°
        budget = min(int(budget * 1.5), config["max_budget"])
    
    return types.ThinkingConfig(
        thinking_budget=budget,
        include_thoughts=budget > 0
    )

async def compress_project_context_if_needed(
    client,
    model: str,
    messages: List[dict],
    max_tokens: int,
    project_type: Optional[str] = None
) -> List[dict]:
    """í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (í•„ìš”í•œ ê²½ìš°)"""
    # í† í° ìˆ˜ ê³„ì‚°
    total_tokens = 0
    for msg in messages:
        token_count = await count_gemini_tokens(msg["content"], model, client)
        total_tokens += token_count.get("input_tokens", 0)
    
    # ì••ì¶•ì´ í•„ìš”í•œì§€ í™•ì¸
    if total_tokens < max_tokens * PROJECT_CONTEXT_COMPRESSION_THRESHOLD:
        return messages
    
    print(f"Project context compression needed: {total_tokens} tokens > {max_tokens * PROJECT_CONTEXT_COMPRESSION_THRESHOLD}")
    
    # ìµœì‹  ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    keep_recent = 5  # í”„ë¡œì íŠ¸ëŠ” ë” ë§ì€ ìµœê·¼ ë©”ì‹œì§€ ìœ ì§€
    recent_messages = messages[-keep_recent:]
    old_messages = messages[:-keep_recent]
    
    if not old_messages:
        return recent_messages
    
    # ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    try:
        summary_content = "\n".join([
            f"{msg['role']}: {msg['content']}" 
            for msg in old_messages
        ])
        
        summary_response = client.models.generate_content(
            model="gemini-2.5-flash",  # ìš”ì•½ì€ ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            contents=[f"ë‹¤ìŒ í”„ë¡œì íŠ¸ ëŒ€í™”ë¥¼ í•µì‹¬ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{summary_content}"],
            config=types.GenerateContentConfig(
                temperature=0.3,
                max_output_tokens=1024,  # í”„ë¡œì íŠ¸ëŠ” ë” ê¸´ ìš”ì•½ í—ˆìš©
                thinking_config=types.ThinkingConfig(thinking_budget=0)  # ìš”ì•½ì€ ì‚¬ê³  ì—†ì´
            )
        )
        
        # ìš”ì•½ëœ ë©”ì‹œì§€ë¡œ êµì²´
        compressed_messages = [
            {"role": "system", "content": f"ì´ì „ í”„ë¡œì íŠ¸ ëŒ€í™” ìš”ì•½: {summary_response.text}"}
        ] + recent_messages
        
        print(f"Project context compressed: {len(messages)} -> {len(compressed_messages)} messages")
        return compressed_messages
        
    except Exception as e:
        print(f"Project context compression error: {e}")
        return messages[-keep_recent:]  # ì‹¤íŒ¨ì‹œ ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€

class ProjectStreamingBuffer:
    """í”„ë¡œì íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë²„í¼ë§"""
    def __init__(self, buffer_size: int = PROJECT_STREAMING_BUFFER_SIZE):
        self.buffer = []
        self.buffer_size = buffer_size
        self.current_size = 0
        self.last_flush = time.time()
    
    def add_chunk(self, chunk: str) -> bool:
        """ì²­í¬ ì¶”ê°€, í”ŒëŸ¬ì‹œ í•„ìš”ì‹œ True ë°˜í™˜"""
        self.buffer.append(chunk)
        self.current_size += len(chunk.encode('utf-8'))
        
        # ë²„í¼ê°€ ê°€ë“ ì°¼ê±°ë‚˜ ì¼ì • ì‹œê°„ì´ ì§€ë‚œ ê²½ìš° í”ŒëŸ¬ì‹œ
        now = time.time()
        return (self.current_size >= self.buffer_size or 
                now - self.last_flush >= PROJECT_STREAMING_FLUSH_INTERVAL)
    
    def flush(self) -> str:
        """ë²„í¼ ë‚´ìš© ë°˜í™˜ ë° ì´ˆê¸°í™”"""
        if not self.buffer:
            return ""
        
        content = "".join(self.buffer)
        self.buffer.clear()
        self.current_size = 0
        self.last_flush = time.time()
        return content

async def generate_gemini_stream_response(
    request: Request,
    messages: list,
    model: str,
    room_id: str,
    db: Session,
    user_id: str,
    project_id: str,
    project_type: Optional[str] = None,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    file_names: Optional[List[str]] = None
) -> AsyncGenerator[str, None]:
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")

        # í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project = crud_project.get(db=db, id=project_id)
        
        # í”„ë¡œì íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = BRIEF_SYSTEM_PROMPT
        if project_type == "assignment":
            system_prompt += "\n\n" + ASSIGNMENT_PROMPT
        elif project_type == "record":
            system_prompt += "\n\n" + RECORD_PROMPT
            
        # í”„ë¡œì íŠ¸ ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        if project and project.system_instruction and project.system_instruction.strip():
            system_prompt += "\n\n## ì¶”ê°€ ì§€ì‹œì‚¬í•­\n" + project.system_instruction.strip()

        # ğŸ” ì„ë² ë”© ê²€ìƒ‰ ìë™ ì‹¤í–‰ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜)
        embedding_context = ""
        if messages and len(messages) > 0:
            last_user_message = messages[-1]  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€
            if last_user_message.get("role") == "user" and last_user_message.get("content"):
                user_query = last_user_message["content"]
                try:
                    # ì„ë² ë”© ê²€ìƒ‰ ìˆ˜í–‰
                    query_embed_result = client.models.embed_content(
                        model="text-embedding-004",
                        contents=user_query,
                        config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY")
                    )
                    
                    if query_embed_result.embeddings:
                        query_embedding = (
                            query_embed_result.embeddings[0].values 
                            if hasattr(query_embed_result.embeddings[0], 'values') 
                            else list(query_embed_result.embeddings[0])
                        )
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ì„ë² ë”© ê²€ìƒ‰ (ì„ê³„ê°’ ë‚®ì¶¤)
                        similar_embeddings = crud_embedding.search_similar(
                            db=db,
                            project_id=project_id,
                            query_embedding=query_embedding,
                            top_k=5,
                            threshold=0.4  # ì„ê³„ê°’ì„ 0.75ì—ì„œ 0.4ë¡œ ë‚®ì¶¤
                        )
                        
                        # ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        if similar_embeddings:
                            relevant_chunks = []
                            for result in similar_embeddings:
                                relevant_chunks.append(f"[{result['file_name']}] {result['content'][:200]}...")  # 200ìë§Œ í‘œì‹œ
                            
                            embedding_context = f"""
                            
## ğŸ“š ê´€ë ¨ ìë£Œ (ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ê²€ìƒ‰)
{chr(10).join(relevant_chunks)}

ìœ„ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
                            print(f"ğŸ” ì„ë² ë”© ê²€ìƒ‰ ì„±ê³µ: {len(similar_embeddings)}ê°œ ì²­í¬ ë°œê²¬")
                            for i, result in enumerate(similar_embeddings):
                                print(f"  [{i+1}] ìœ ì‚¬ë„: {result['similarity']:.3f}, íŒŒì¼: {result['file_name']}, ë‚´ìš©: {result['content'][:50]}...")
                        else:
                            # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                            print("âŒ ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                            # ì „ì²´ ì„ë² ë”© ê°œìˆ˜ í™•ì¸
                            all_embeddings = crud_embedding.get_by_project(db, project_id)
                            print(f"   ì „ì²´ ì„ë² ë”© ê°œìˆ˜: {len(all_embeddings)}")
                            if all_embeddings:
                                print(f"   íŒŒì¼ ëª©ë¡: {list(set(e.file_name for e in all_embeddings))}")
                            print(f"   ì‚¬ìš©ì ì§ˆë¬¸: '{user_query}'")
                            print(f"   ì„ê³„ê°’: 0.4")
                    else:
                        print("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"ì„ë² ë”© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    
        # ì„ë² ë”© ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        if embedding_context:
            system_prompt += embedding_context

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not messages or len(messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )

        # í† í° ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ (chat.py ë°©ì‹ ì ìš©)
        # í”„ë¡œì íŠ¸ëŠ” ë” í° ì»¨í…ìŠ¤íŠ¸ í—ˆìš© (ì¼ë°˜ ì±„íŒ…ë³´ë‹¤ 2ë°°)
        MAX_CONTEXT_TOKENS = 128000 - 8192  # ì¶œë ¥ì„ ìœ„í•œ 8192 í† í° ì˜ˆì•½
        
        # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœê·¼ ë©”ì‹œì§€ë¶€í„° í¬í•¨
        valid_messages = []
        total_tokens = 0
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í° ê³„ì‚°
        if system_prompt:
            system_tokens = await count_gemini_tokens(system_prompt, model, client)
            total_tokens += system_tokens.get("input_tokens", 0)
        
        # íŒŒì¼ í† í° ê³„ì‚° (ìˆëŠ” ê²½ìš°)
        file_tokens = 0
        if file_data_list and file_types:
            # ì´ë¯¸ì§€ëŠ” íƒ€ì¼ë‹¹ 258 í† í°, PDFëŠ” í˜ì´ì§€ë‹¹ 258 í† í°
            for file_type in file_types:
                if file_type.startswith("image/"):
                    file_tokens += 258  # Gemini 2.5 ê¸°ì¤€
                elif file_type == "application/pdf":
                    file_tokens += 258 * 10  # ì˜ˆìƒ í˜ì´ì§€ ìˆ˜
        
        total_tokens += file_tokens
        
        # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ ê²€í† í•˜ë©´ì„œ í† í° ì˜ˆì‚° ë‚´ì—ì„œ í¬í•¨
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("content") and msg["content"].strip():
                # ë©”ì‹œì§€ í† í° ê³„ì‚°
                msg_tokens = await count_gemini_tokens(
                    f"{msg['role']}: {msg['content']}", 
                    model, 
                    client
                )
                msg_token_count = msg_tokens.get("input_tokens", 0)
                
                # í† í° ì˜ˆì‚° í™•ì¸
                if total_tokens + msg_token_count <= MAX_CONTEXT_TOKENS:
                    valid_messages.insert(0, msg)
                    total_tokens += msg_token_count
                else:
                    # í† í° í•œê³„ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                    print(f"Project context window limit reached. Including {len(valid_messages)} messages out of {len(messages)}")
                    break
        
        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ë©”ì‹œì§€ëŠ” í¬í•¨ë˜ì–´ì•¼ í•¨
        if len(valid_messages) == 0 and len(messages) > 0:
            last_msg = messages[-1]
            if last_msg.get("content") and last_msg["content"].strip():
                valid_messages = [last_msg]
        
        if len(valid_messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="No valid message content found"
            )
        
        print(f"Project context management: Using {len(valid_messages)} messages with {total_tokens} tokens")

        # ì»¨í…ìŠ¤íŠ¸ ìºì‹± ì„ì‹œ ë¹„í™œì„±í™” (API ì œì•½ì‚¬í•­ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ë°©ì§€)
        cached_content_name = None
        # if system_prompt:
        #     # í”„ë¡œì íŠ¸ë³„ ìºì‹œ ì´ë¦„ ìƒì„±
        #     prompt_hash = hashlib.md5(system_prompt.encode()).hexdigest()[:8]
        #     cache_name = f"project_{project_id}_{model}_{prompt_hash}"
        #     
        #     # ìºì‹œ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        #     cached_content_name = await get_or_create_project_context_cache(
        #         client=client,
        #         project=project,
        #         model=model,
        #         cache_name=cache_name,
        #         ttl=7200  # 2ì‹œê°„ ìºì‹±
        #     )

        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì ìš© (í•„ìš”í•œ ê²½ìš°)
        if len(valid_messages) > 10:  # 10ê°œ ì´ìƒ ë©”ì‹œì§€ê°€ ìˆì„ ë•Œë§Œ ì••ì¶• ê³ ë ¤
            valid_messages = await compress_project_context_if_needed(
                client=client,
                model=model,
                messages=valid_messages,
                max_tokens=PROJECT_MAX_CONTEXT_TOKENS,
                project_type=project_type or "general"
            )

        # ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # í”„ë¡œì íŠ¸ íŒŒì¼ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
        try:
            project_files_list = []
            for file in client.files.list():
                if file.display_name and file.display_name.startswith(f"project_{project_id}_"):
                    if file.state.name == "ACTIVE":
                        project_files_list.append(file)
            
            # ìµœëŒ€ 3ê°œ íŒŒì¼ë§Œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            for file in project_files_list[:3]:
                contents.append(file)
        except Exception as e:
            print(f"Failed to load project files for context: {e}")
        
        # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬
        if file_data_list and file_types and file_names:
            for file_data, file_type, file_name in zip(file_data_list, file_types, file_names):
                if file_type.startswith("image/"):
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )
                elif file_type == "application/pdf":
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )

        # ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation_text = ""
        for message in valid_messages:
            role_text = "Human" if message["role"] == "user" else "Assistant"
            conversation_text += f"{role_text}: {message['content']}\n"

        contents.append(conversation_text)

        # í”„ë¡œì íŠ¸ë³„ ë„êµ¬ ì„¤ì • - ê²€ìƒ‰ê³¼ ì½”ë“œ ì‹¤í–‰ë§Œ ì‚¬ìš©
        tools = []
        
        # í•­ìƒ Google ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì¶”ê°€
        tools.append(types.Tool(google_search=types.GoogleSearch()))
        
        # ì½”ë“œ ì‹¤í–‰ ì¶”ê°€ (ë°ì´í„° ë¶„ì„ìš©)
        tools.append(types.Tool(code_execution=types.ToolCodeExecution()))

        # ìƒì„± ì„¤ì • (ìºì‹œ ë¹„í™œì„±í™”ë¡œ ì¸í•œ ê°„ì†Œí™”)
        generation_config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            temperature=0.7,
            top_p=0.95,
            max_output_tokens=8192,
            tools=tools
        )

        # ìµœì í™”ëœ ì‚¬ê³  ê¸°ëŠ¥ ì„¤ì • (í”„ë¡œì íŠ¸ë³„ ì ì‘í˜•)
        optimized_thinking_config = await get_optimized_project_thinking_config(
            model=model,
            project_type=project_type or "general",
            complexity_level="complex" if len(valid_messages) > 5 else "normal"
        )
        if optimized_thinking_config:
            generation_config.thinking_config = optimized_thinking_config
        else:
            # í´ë°±: ê¸°ë³¸ ì‚¬ê³  ì„¤ì •
            thinking_budget = 16384 if model.endswith("2.5-pro") else 12288
            if model.endswith("2.5-pro"):
                generation_config.thinking_config = types.ThinkingConfig(
                    thinking_budget=thinking_budget,
                    include_thoughts=True
                )
            elif model.endswith("2.5-flash"):
                generation_config.thinking_config = types.ThinkingConfig(
                    thinking_budget=min(thinking_budget, 24576),
                    include_thoughts=True
                )

        # í”„ë¡œì íŠ¸ë³„ ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì´ˆê¸°í™”
        content_buffer = ProjectStreamingBuffer(PROJECT_STREAMING_BUFFER_SIZE)
        thinking_buffer = ProjectStreamingBuffer(PROJECT_STREAMING_BUFFER_SIZE // 2)

        # ì…ë ¥ í† í° ê³„ì‚°
        input_token_count = await count_gemini_tokens(conversation_text, model, client)
        input_tokens = input_token_count.get("input_tokens", 0)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        accumulated_content = ""
        accumulated_thinking = ""
        thought_time = 0.0
        citations = []
        web_search_queries = []
        streaming_completed = False  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì—¬ë¶€ ì²´í¬
        is_disconnected = False  # ì—°ê²° ì¤‘ë‹¨ í”Œë˜ê·¸ ì¶”ê°€
        citations_sent = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ set

        try:
            response = client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=generation_config
            )

            start_time = time.time()
            
            for chunk in response:
                # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
                if await request.is_disconnected():
                    is_disconnected = True
                    print("Project client disconnected. Stopping stream.")
                    break
                    
                if chunk.candidates and len(chunk.candidates) > 0:
                    candidate = chunk.candidates[0]
                    
                    # ì½˜í…ì¸  íŒŒíŠ¸ ì²˜ë¦¬
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            # ì‚¬ê³  ë‚´ìš©ê³¼ ì¼ë°˜ ì½˜í…ì¸  ëª…í™•íˆ ë¶„ë¦¬ (chat.py ë°©ì‹)
                            if hasattr(part, 'thought') and part.thought:
                                # ì‚¬ê³  ë‚´ìš©ë§Œ ì²˜ë¦¬
                                if part.text:
                                    accumulated_thinking += part.text
                                    thought_time = time.time() - start_time
                                    try:
                                        yield f"data: {json.dumps({'reasoning_content': part.text, 'thought_time': thought_time})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        print("Project client disconnected during reasoning streaming")
                                        return
                            elif part.text:
                                # ì¼ë°˜ ì‘ë‹µ ë‚´ìš©ë§Œ ì²˜ë¦¬
                                accumulated_content += part.text
                                try:
                                    yield f"data: {json.dumps({'content': part.text})}\n\n"
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    print("Project client disconnected during content streaming")
                                    return

                    # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (ìµœì‹  API êµ¬ì¡°)
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding = candidate.grounding_metadata
                        
                        # ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜ì§‘
                        if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                            web_search_queries.extend(grounding.web_search_queries)
                        
                        # grounding chunksì—ì„œ citations ì¶”ì¶œ
                        if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                            new_citations = []
                            for chunk_info in grounding.grounding_chunks:
                                if hasattr(chunk_info, 'web') and chunk_info.web:
                                    citation_url = chunk_info.web.uri
                                    # ì¤‘ë³µ ë°©ì§€
                                    if citation_url not in citations_sent:
                                        citation = {
                                            "url": citation_url,
                                            "title": chunk_info.web.title if hasattr(chunk_info.web, 'title') else ""
                                        }
                                        citations.append(citation)
                                        new_citations.append(citation)
                                        citations_sent.add(citation_url)
                            
                            # ìƒˆë¡œìš´ ì¸ìš© ì •ë³´ë§Œ ì „ì†¡
                            if new_citations:
                                try:
                                    yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    print("Project client disconnected during citations streaming")
                                    return

            # ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if is_disconnected:
                print("Skipping project post-processing due to client disconnection.")
                return
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨
            streaming_completed = True
            
            # ìµœì¢… ë©”íƒ€ë°ì´í„° ì „ì†¡
            if web_search_queries:
                try:
                    yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    print("Project client disconnected during final search queries streaming")
                    return

            # ì¶œë ¥ í† í° ê³„ì‚°
            output_token_count = await count_gemini_tokens(accumulated_content, model, client)
            output_tokens = output_token_count.get("input_tokens", 0)
            
            # ì‚¬ê³  í† í° ê³„ì‚°
            thinking_tokens = 0
            if accumulated_thinking:
                thinking_token_count = await count_gemini_tokens(accumulated_thinking, model, client)
                thinking_tokens = thinking_token_count.get("input_tokens", 0)

            # í† í° ì‚¬ìš©ëŸ‰ ì €ì¥
            crud_stats.create_token_usage(
                db=db,
                user_id=user_id,
                room_id=room_id,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens + thinking_tokens,
                timestamp=datetime.now(),
                chat_type=f"project_{project_type}" if project_type else None
            )

        except (ConnectionError, BrokenPipeError, GeneratorExit):
            streaming_completed = False  # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            print("Project client disconnected during main streaming loop")
            return
        except Exception as api_error:
            streaming_completed = False  # ì—ëŸ¬ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            error_message = f"Gemini API Error: {str(api_error)}"
            try:
                yield f"data: {json.dumps({'error': error_message})}\n\n"
            except (ConnectionError, BrokenPipeError, GeneratorExit):
                print("Project client disconnected during error streaming")
                return
        
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥
        if streaming_completed and accumulated_content:
            print(f"=== PROJECT SAVING MESSAGE DEBUG ===")
            print(f"streaming_completed: {streaming_completed}")
            print(f"accumulated_content length: {len(accumulated_content)}")
            print(f"citations count: {len(citations)}")
            message_create = ChatMessageCreate(
                content=accumulated_content,
                role="assistant",
                reasoning_content=accumulated_thinking if accumulated_thinking else None,
                thought_time=thought_time if thought_time > 0 else None,
                citations=citations if citations else None
            )
            crud_project.create_chat_message(db, project_id=project_id, chat_id=room_id, obj_in=message_create)
            print(f"Project message saved successfully")
            print(f"=== END PROJECT SAVING DEBUG ===")
        else:
            print(f"=== PROJECT MESSAGE NOT SAVED ===")
            print(f"streaming_completed: {streaming_completed}")
            print(f"accumulated_content: {bool(accumulated_content)}")
            print(f"Reason: {'Streaming was interrupted' if not streaming_completed else 'No content'}")
            print(f"=== END PROJECT NOT SAVED DEBUG ===")

    except Exception as e:
        error_message = f"Project Stream Generation Error: {str(e)}"
        try:
            yield f"data: {json.dumps({'error': error_message})}\n\n"
        except (ConnectionError, BrokenPipeError, GeneratorExit):
            print("Project client disconnected during final error streaming")
            return

@router.post("/{project_id}/chats/{chat_id}/chat")
async def stream_project_chat(
    *,
    request: Request,
    db: Session = Depends(deps.get_db),
    project_id: str,
    chat_id: str,
    request_data: str = Form(...),
    files: List[UploadFile] = File([]),
    current_user: User = Depends(deps.get_current_user)
) -> Any:
    try:
        # JSON íŒŒì‹±
        try:
            parsed_data = json.loads(request_data)
            chat_request = ChatRequest(**parsed_data)
        except (json.JSONDecodeError, ValueError) as e:
            raise HTTPException(status_code=400, detail=f"Invalid JSON format: {str(e)}")
        
        # ëª¨ë¸ ê²€ì¦
        if chat_request.model not in ALLOWED_MODELS:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid model specified. Allowed models: {ALLOWED_MODELS}"
            )

        # í”„ë¡œì íŠ¸ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        if project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ì±„íŒ…ë°© ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
        chat = crud_project.get_chat(db=db, project_id=project_id, chat_id=chat_id)
        if not chat:
            # ì±„íŒ…ë°©ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±
            from app.schemas.project import ProjectChatCreate
            chat_data = ProjectChatCreate(
                id=chat_id,
                name="",  # ë¹ˆ ì´ë¦„ìœ¼ë¡œ ì‹œì‘
                type=project.type
            )
            try:
                chat = crud_project.create_chat(
                    db=db, 
                    project_id=project_id, 
                    obj_in=chat_data, 
                    owner_id=current_user.id,
                    chat_id=chat_id  # URLì—ì„œ ë°›ì€ chat_id ì‚¬ìš©
                )
                # ì±„íŒ…ë°© ìƒì„± í›„ ëª…ì‹œì ìœ¼ë¡œ ì»¤ë°‹
                db.commit()
                db.refresh(chat)
            except Exception as e:
                db.rollback()
                raise HTTPException(
                    status_code=500, 
                    detail=f"Failed to create chat room: {str(e)}"
                )

        # íŒŒì¼ ì²˜ë¦¬
        file_data_list = []
        file_types = []
        file_names = []
        file_info_list = []
        if files and files[0].filename:
            if chat_request.model not in MULTIMODAL_MODELS:
                raise HTTPException(
                    status_code=400,
                    detail="File upload is not supported for this model"
                )
            
            for file in files:
                await validate_file(file)
                file_data, file_type = await process_file_to_base64(file)
                file_data_list.append(file_data)
                file_types.append(file_type)
                file_names.append(file.filename)
                file_info_list.append({
                    "type": file_type,
                    "name": file.filename,
                    "data": file_data
                })

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ (chat.py ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ)
        if chat_request.messages:
            last_message = chat_request.messages[-1]
            user_message = ChatMessageCreate(
                content=last_message["content"],
                role="user",
                room_id=chat_id,  # chat.pyì™€ ë™ì¼í•˜ê²Œ room_id ì‚¬ìš©
                files=[{
                    "type": file_type,
                    "name": file_name,
                    "data": file_data
                } for file_data, file_type, file_name in zip(file_data_list, file_types, file_names)] if file_data_list else None
            )
            # í”„ë¡œì íŠ¸ ì±„íŒ… ë©”ì‹œì§€ ì €ì¥
            crud_project.create_chat_message(db, project_id=project_id, chat_id=chat_id, obj_in=user_message)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        formatted_messages = [
            {"role": msg["role"], "content": msg["content"]} 
            for msg in chat_request.messages
        ]
        
        return StreamingResponse(
            generate_gemini_stream_response(
                request=request,
                messages=formatted_messages,
                model=chat_request.model,
                room_id=chat_id,
                db=db,
                user_id=current_user.id,
                project_id=project_id,
                project_type=project.type,
                file_data_list=file_data_list,
                file_types=file_types,
                file_names=[f.filename for f in files] if files else None
            ),
            media_type="text/event-stream"
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process chat: {str(e)}"
        )

@router.delete("/{project_id}/chats/{chat_id}")
def delete_project_chat(
    *,
    db: Session = Depends(deps.get_db),
    project_id: str,
    chat_id: str,
) -> Any:
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    
    chat = crud_project.get_chat(db=db, project_id=project_id, chat_id=chat_id)
    if not chat:
        raise HTTPException(status_code=404, detail="Chat not found")
    
    db.delete(chat)
    db.commit()
    
    return {"message": "Chat deleted successfully"}

# í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ
@router.get("/", response_model=List[Dict[str, Any]])
def get_projects(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì‚¬ìš©ìì˜ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    projects = crud_project.get_multi_by_user(db=db, user_id=current_user.id)
    return projects

# íŠ¹ì • í”„ë¡œì íŠ¸ ì¡°íšŒ
@router.get("/{project_id}")
def get_project(
    project_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """íŠ¹ì • í”„ë¡œì íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    return project.to_dict(include_chats=True)

# í”„ë¡œì íŠ¸ ìƒì„±
@router.post("/")
def create_project(
    project: ProjectCreate,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    created_project = crud_project.create(db=db, obj_in=project, user_id=current_user.id)
    return created_project.to_dict(include_chats=True)

# í”„ë¡œì íŠ¸ ìˆ˜ì •
@router.patch("/{project_id}")
def update_project(
    project_id: str,
    project: ProjectUpdate,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    db_project = crud_project.get(db=db, id=project_id)
    if not db_project:
        raise HTTPException(status_code=404, detail="Project not found")
    if db_project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    updated_project = crud_project.update(db=db, db_obj=db_project, obj_in=project)
    return updated_project.to_dict(include_chats=True)

# í”„ë¡œì íŠ¸ ì‚­ì œ
@router.delete("/{project_id}")
def delete_project(
    project_id: str,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    crud_project.remove(db=db, id=project_id)
    return {"message": "Project deleted successfully"}

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ëª©ë¡ ì¡°íšŒ
@router.get("/{project_id}/chats")
def get_project_chats(
    project_id: str,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ì˜ ì±„íŒ…ë°© ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    return crud_project.get_project_chats(db=db, project_id=project_id, owner_id=current_user.id)

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ìƒì„±
@router.post("/{project_id}/chats")
def create_project_chat(
    project_id: str,
    chat: dict,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ì— ìƒˆ ì±„íŒ…ë°©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    
    from app.schemas.project import ProjectChatCreate
    chat_data = ProjectChatCreate(**chat)
    return crud_project.create_chat(db=db, project_id=project_id, obj_in=chat_data, owner_id=current_user.id)

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ìˆ˜ì •
@router.patch("/{project_id}/chats/{chat_id}")
def update_project_chat(
    project_id: str,
    chat_id: str,
    chat: dict,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ ì±„íŒ…ë°©ì„ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    
    from app.schemas.chat import ChatUpdate
    chat_data = ChatUpdate(**chat)
    return crud_project.update_chat_by_id(db=db, project_id=project_id, chat_id=chat_id, obj_in=chat_data)

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ë©”ì‹œì§€ ì¡°íšŒ
@router.get("/{project_id}/chats/{chat_id}/messages")
def get_project_chat_messages(
    project_id: str,
    chat_id: str,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ ì±„íŒ…ë°©ì˜ ë©”ì‹œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    messages = crud_project.get_chat_messages(db=db, project_id=project_id, chat_id=chat_id)
    return {"messages": messages}

# í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹± ê¸°ëŠ¥
@router.post("/{project_id}/cache")
async def create_project_context_cache(
    project_id: str,
    content: str = Form(...),
    model: str = Form(...),
    ttl: int = Form(3600),  # ê¸°ë³¸ 1ì‹œê°„
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìƒì„±"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
        system_instruction = BRIEF_SYSTEM_PROMPT
        if project.type == "assignment":
            system_instruction += "\n\n" + ASSIGNMENT_PROMPT
        elif project.type == "record":
            system_instruction += "\n\n" + RECORD_PROMPT
        
        # ìºì‹œ ìƒì„±
        cache = client.caches.create(
            model=model,
            config=types.CreateCachedContentConfig(
                display_name=f"project_{project_id}_cache_{int(time.time())}",
                system_instruction=system_instruction,
                contents=[content],
                ttl=f"{ttl}s"
            )
        )
        
        return {
            "cache_name": cache.name,
            "project_id": project_id,
            "ttl": ttl,
            "message": "Project cache created successfully"
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create project cache: {str(e)}")

@router.get("/{project_id}/cache")
async def list_project_context_caches(
    project_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ëª©ë¡ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        caches = []
        for cache in client.caches.list():
            if cache.display_name and cache.display_name.startswith(f"project_{project_id}_cache_"):
                caches.append({
                    "name": cache.name,
                    "display_name": cache.display_name,
                    "model": cache.model,
                    "create_time": cache.create_time.isoformat() if hasattr(cache, 'create_time') and cache.create_time else None,
                    "expire_time": cache.expire_time.isoformat() if hasattr(cache, 'expire_time') and cache.expire_time else None
                })
        
        return {"caches": caches, "project_id": project_id}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list project caches: {str(e)}")

@router.delete("/{project_id}/cache/{cache_name}")
async def delete_project_context_cache(
    project_id: str,
    cache_name: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        client.caches.delete(cache_name)
        
        return {"message": "Project cache deleted successfully", "project_id": project_id}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete project cache: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì„ë² ë”© ê¸°ëŠ¥
@router.post("/{project_id}/embeddings")
async def create_project_embeddings(
    project_id: str,
    texts: List[str] = Form(...),
    model: str = Form("text-embedding-004"),
    task_type: str = Form("SEMANTIC_SIMILARITY"),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        embeddings = []
        for text in texts:
            result = client.models.embed_content(
                model=model,
                contents=text,
                config=types.EmbedContentConfig(task_type=task_type)
            )
            embeddings.append({
                "text": text,
                "embedding": result.embeddings[0] if result.embeddings else []
            })
        
        return {
            "embeddings": embeddings,
            "project_id": project_id,
            "project_type": project.type
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create project embeddings: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± ê¸°ëŠ¥
@router.post("/{project_id}/generate-prompt")
async def generate_project_prompt(
    project_id: str,
    category: str = Form(...),
    task_description: str = Form(...),
    style: str = Form("ì¹œê·¼í•œ"),
    complexity: str = Form("ì¤‘ê°„"),
    output_format: str = Form("ììœ í˜•ì‹"),
    include_examples: bool = Form(True),
    include_constraints: bool = Form(False),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ë§ì¶¤í˜• AI í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # í”„ë¡œì íŠ¸ íƒ€ì…ì— ë”°ë¥¸ íŠ¹í™”ëœ ì¹´í…Œê³ ë¦¬ ì§€ì‹œ
        project_specific_instructions = {
            "assignment": "ìˆ˜í–‰í‰ê°€ì™€ ê³¼ì œ í•´ê²°ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í‰ê°€ ê¸°ì¤€ ì¶©ì¡±ê³¼ ì°½ì˜ì  ì ‘ê·¼ì„ ëª¨ë‘ ê³ ë ¤í•©ë‹ˆë‹¤.",
            "record": "ìƒê¸°ë¶€ ì‘ì„±ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. êµìœ¡ë¶€ ê¸°ì¬ìš”ë ¹ ì¤€ìˆ˜ì™€ ì°¨ë³„í™” í¬ì¸íŠ¸ë¥¼ ë™ì‹œì— ê³ ë ¤í•©ë‹ˆë‹¤.",
            "general": "ì¼ë°˜ì ì¸ í•™ìŠµ ëª©ì ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ ì‹œìŠ¤í…œ ì§€ì‹œ ìƒì„±
        category_instructions = {
            "í•™ìŠµ": "êµìœ¡ ë° í•™ìŠµ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì „ë¬¸ê°€ë¡œì„œ, í•™ìŠµìì˜ ì´í•´ë„ë¥¼ ë†’ì´ê³  ë‹¨ê³„ì  í•™ìŠµì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì°½ì‘": "ì°½ì˜ì  ì½˜í…ì¸  ìƒì„± ì „ë¬¸ê°€ë¡œì„œ, ìƒìƒë ¥ì„ ìê·¹í•˜ê³  ë…ì°½ì ì¸ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¶„ì„": "ë°ì´í„° ë¶„ì„ ë° ë…¼ë¦¬ì  ì‚¬ê³  ì „ë¬¸ê°€ë¡œì„œ, ì²´ê³„ì ì´ê³  ê°ê´€ì ì¸ ë¶„ì„ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë²ˆì—­": "ë‹¤êµ­ì–´ ë²ˆì—­ ì „ë¬¸ê°€ë¡œì„œ, ë¬¸ë§¥ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ ì •í™•íˆ ì „ë‹¬í•˜ëŠ” ë²ˆì—­ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì½”ë”©": "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì „ë¬¸ê°€ë¡œì„œ, íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì½”ë“œ ì‘ì„±ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ë° ì˜ì‚¬ê²°ì • ì „ë¬¸ê°€ë¡œì„œ, ì‹¤ìš©ì ì´ê³  ê²°ê³¼ ì§€í–¥ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì¼ë°˜": "ë²”ìš© í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ì–‘í•œ ìƒí™©ì— ì ìš© ê°€ëŠ¥í•œ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        }
        
        system_instruction = f"""
        ë‹¹ì‹ ì€ {category_instructions.get(category, category_instructions["ì¼ë°˜"])}
        
        í”„ë¡œì íŠ¸ íŠ¹í™” ìš”êµ¬ì‚¬í•­: {project_specific_instructions.get(project.type, project_specific_instructions["general"])}
        
        í”„ë¡¬í”„íŠ¸ ìƒì„± ì›ì¹™:
        1. ëª…í™•í•œ ì§€ì‹œì‚¬í•­ê³¼ êµ¬ì²´ì ì¸ ê¸°ëŒ€ ê²°ê³¼ í¬í•¨
        2. í”„ë¡œì íŠ¸ íƒ€ì…({project.type})ì— ìµœì í™”ëœ ì ‘ê·¼ ë°©ì‹
        3. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ìµœì ì˜ ê²°ê³¼ ë„ì¶œ
        4. êµìœ¡ì  ê°€ì¹˜ì™€ ì‹¤ìš©ì„±ì˜ ê· í˜•
        
        ì‘ë‹µ í˜•ì‹:
        - í”„ë¡¬í”„íŠ¸ ì œëª© (í”„ë¡œì íŠ¸ íƒ€ì… ë°˜ì˜)
        - ë©”ì¸ í”„ë¡¬í”„íŠ¸ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì™„ì„± í˜•íƒœ)
        - í”„ë¡œì íŠ¸ë³„ íŠ¹í™” íŒ
        - ì‘ìš© ë° ë³€í˜• ì œì•ˆ
        """
        
        user_request = f"""
        ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” {project.type} í”„ë¡œì íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        ğŸ“‹ **í”„ë¡œì íŠ¸ ì •ë³´**
        - í”„ë¡œì íŠ¸ëª…: {project.name}
        - í”„ë¡œì íŠ¸ íƒ€ì…: {project.type}
        - ì¹´í…Œê³ ë¦¬: {category}
        - ì‘ì—… ì„¤ëª…: {task_description}
        
        ğŸ“Œ **ìŠ¤íƒ€ì¼ ì„¤ì •**
        - ìŠ¤íƒ€ì¼: {style}
        - ë³µì¡ë„: {complexity}
        - ì¶œë ¥ í˜•ì‹: {output_format}
        - ì˜ˆì‹œ í¬í•¨: {'ì˜ˆ' if include_examples else 'ì•„ë‹ˆì˜¤'}
        - ì œì•½ì‚¬í•­ í¬í•¨: {'ì˜ˆ' if include_constraints else 'ì•„ë‹ˆì˜¤'}
        
        ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ëŠ” {project.type} í”„ë¡œì íŠ¸ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ ìµœì í™”í•´ì£¼ì„¸ìš”.
        """
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[user_request],
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.8,
                max_output_tokens=4000,
                tools=[
                    types.Tool(google_search=types.GoogleSearch()),
                    types.Tool(code_execution=types.ToolCodeExecution())
                ]
            )
        )
        
        return {
            "generated_prompt": response.text,
            "project_id": project_id,
            "project_name": project.name,
            "project_type": project.type,
            "category": category,
            "task_description": task_description,
            "settings": {
                "style": style,
                "complexity": complexity,
                "output_format": output_format,
                "include_examples": include_examples,
                "include_constraints": include_constraints
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate project prompt: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥
@router.post("/{project_id}/chats/{chat_id}/search")
async def search_project_web(
    project_id: str,
    chat_id: str,
    request: Request,
    query: str = Form(...),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì±„íŒ…ì—ì„œ Google ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ (ìŠ¤íŠ¸ë¦¬ë°)"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ì‚¬ìš©ì ê²€ìƒ‰ ì§ˆë¬¸ì„ DBì— ì €ì¥
        user_message = ChatMessageCreate(
            content=f"ğŸ” ê²€ìƒ‰: {query}",
            role="user",
            room_id=chat_id
        )
        crud_project.create_chat_message(db, project_id=project_id, chat_id=chat_id, obj_in=user_message)
        
        async def generate_project_search_stream():
            try:
                client = get_gemini_client()
                if not client:
                    yield f"data: {json.dumps({'error': 'Gemini client not available'})}\n\n"
                    return
                
                # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
                system_instruction = BRIEF_SYSTEM_PROMPT
                if project.type == "assignment":
                    system_instruction += "\n\n" + ASSIGNMENT_PROMPT
                elif project.type == "record":
                    system_instruction += "\n\n" + RECORD_PROMPT
                
                system_instruction += f"""
                
                í˜„ì¬ í”„ë¡œì íŠ¸ ì •ë³´:
                - í”„ë¡œì íŠ¸ëª…: {project.name}
                - í”„ë¡œì íŠ¸ íƒ€ì…: {project.type}
                - ì„¤ëª…: {project.description or 'ì—†ìŒ'}
                
                ê²€ìƒ‰ ì‹œ í”„ë¡œì íŠ¸ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.
                """
                
                # í”„ë¡œì íŠ¸ ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
                if project.system_instruction and project.system_instruction.strip():
                    system_instruction += "\n\n## ì¶”ê°€ ì§€ì‹œì‚¬í•­\n" + project.system_instruction.strip()
                
                # ê²€ìƒ‰ ë„êµ¬ ì„¤ì •
                tools = [
                    types.Tool(google_search=types.GoogleSearch()),
                    types.Tool(code_execution=types.ToolCodeExecution())
                ]
                
                # ìƒì„± ì„¤ì •
                generation_config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.7,
                    max_output_tokens=3000,
                    tools=tools,
                    thinking_config=types.ThinkingConfig(
                        thinking_budget=8192,
                        include_thoughts=True
                    )
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì‹¤í–‰
                response = client.models.generate_content_stream(
                    model="gemini-2.5-flash",
                    contents=[f"í”„ë¡œì íŠ¸ '{project.name}' ê´€ë ¨í•˜ì—¬ ë‹¤ìŒì— ëŒ€í•´ ê²€ìƒ‰í•´ì£¼ì„¸ìš”: {query}"],
                    config=generation_config
                )
                
                accumulated_content = ""
                accumulated_reasoning = ""
                citations = []
                web_search_queries = []
                citations_sent = set()
                search_completed = False
                is_disconnected = False
                
                for chunk in response:
                    if await request.is_disconnected():
                        is_disconnected = True
                        break
                        
                    if chunk.candidates and len(chunk.candidates) > 0:
                        candidate = chunk.candidates[0]
                        
                        # ì½˜í…ì¸  ì²˜ë¦¬
                        if candidate.content and candidate.content.parts:
                            for part in candidate.content.parts:
                                if hasattr(part, 'thought') and part.thought:
                                    accumulated_reasoning += part.text
                                    try:
                                        yield f"data: {json.dumps({'reasoning_content': part.text})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        return
                                elif part.text:
                                    accumulated_content += part.text
                                    try:
                                        yield f"data: {json.dumps({'content': part.text})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        return
                        
                        # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
                        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                            grounding = candidate.grounding_metadata
                            
                            if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                                web_search_queries.extend(grounding.web_search_queries)
                            
                            if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                                new_citations = []
                                for chunk_info in grounding.grounding_chunks:
                                    if hasattr(chunk_info, 'web') and chunk_info.web:
                                        citation_url = chunk_info.web.uri
                                        
                                        # Mixed Content ë¬¸ì œ ë°©ì§€: HTTP URLì„ HTTPSë¡œ ë³€í™˜
                                        if citation_url.startswith('http://'):
                                            citation_url = citation_url.replace('http://', 'https://', 1)
                                        
                                        if citation_url not in citations_sent:
                                            citation = {
                                                "url": citation_url,
                                                "title": chunk_info.web.title if hasattr(chunk_info.web, 'title') else ""
                                            }
                                            citations.append(citation)
                                            new_citations.append(citation)
                                            citations_sent.add(citation_url)
                                
                                if new_citations:
                                    try:
                                        yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        return
                
                if is_disconnected:
                    return
                
                search_completed = True
                
                if web_search_queries:
                    try:
                        yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                        return
                
            except Exception as e:
                search_completed = False
                try:
                    yield f"data: {json.dumps({'error': f'Project search failed: {str(e)}'})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    return
            
            # ê²€ìƒ‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥
            if search_completed and accumulated_content:
                ai_message = ChatMessageCreate(
                    content=accumulated_content,
                    role="assistant",
                    room_id=chat_id,
                    reasoning_content=accumulated_reasoning if accumulated_reasoning else None,
                    citations=citations if citations else None
                )
                crud_project.create_chat_message(db, project_id=project_id, chat_id=chat_id, obj_in=ai_message)
        
        return StreamingResponse(
            generate_project_search_stream(),
            media_type="text/plain"
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Project search failed: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ í†µê³„ ì¡°íšŒ ê¸°ëŠ¥
@router.get("/{project_id}/stats/token-usage")
async def get_project_token_usage(
    project_id: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
        start_dt = None
        end_dt = None
        
        if start_date:
            start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
        if end_date:
            end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
        
        # ê° ì±„íŒ…ë°©ë³„ í† í° ì‚¬ìš©ëŸ‰ í•©ê³„
        total_usage = crud_stats.get_token_usage(
            db=db,
            start_date=start_dt,
            end_date=end_dt,
            user_id=current_user.id
        )
        
        return {
            "project_id": project_id,
            "project_name": project.name,
            "project_type": project.type,
            "token_usage": total_usage,
            "period": {
                "start_date": start_date,
                "end_date": end_date
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get project token usage: {str(e)}")

@router.get("/{project_id}/stats/chat-usage")
async def get_project_chat_usage(
    project_id: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì±„íŒ… ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
        start_dt = None
        end_dt = None
        
        if start_date:
            start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
        if end_date:
            end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
        
        # í”„ë¡œì íŠ¸ë³„ ì±„íŒ… í†µê³„ ì¡°íšŒ
        chat_stats = crud_stats.get_chat_statistics(
            db=db,
            start_date=start_dt,
            end_date=end_dt,
            user_id=current_user.id
        )
        
        return {
            "project_id": project_id,
            "project_name": project.name,
            "project_type": project.type,
            "chat_statistics": chat_stats,
            "period": {
                "start_date": start_date,
                "end_date": end_date
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get project chat usage: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
async def get_or_create_project_context_cache(
    client,
    project: Project,
    model: str,
    cache_name: str,
    ttl: int = 7200
) -> Optional[str]:
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ê¸°ì¡´ ìºì‹œ í™•ì¸
        for cache in client.caches.list():
            if cache.display_name == cache_name and cache.model == model:
                # ìºì‹œê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
                if hasattr(cache, 'expire_time') and cache.expire_time and cache.expire_time > datetime.now(timezone.utc):
                    print(f"Using existing project cache: {cache_name}")
                    return cache.name
        
        # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_instruction = BRIEF_SYSTEM_PROMPT
        if project.type == "assignment":
            system_instruction += "\n\n" + ASSIGNMENT_PROMPT
        elif project.type == "record":
            system_instruction += "\n\n" + RECORD_PROMPT
        
        # ìƒˆ ìºì‹œ ìƒì„± (ë¹ˆ contents ë¬¸ì œ í•´ê²°)
        print(f"Creating new project cache: {cache_name}")
        cache = client.caches.create(
            model=model,
            config=types.CreateCachedContentConfig(
                display_name=cache_name,
                system_instruction=system_instruction,
                contents=["í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ"],  # ë¹ˆ ë°°ì—´ ëŒ€ì‹  ìµœì†Œ ì½˜í…ì¸  ì œê³µ
                ttl=f"{ttl}s"
            )
        )
        return cache.name
    except Exception as e:
        print(f"Project cache creation error: {e}")
        return None 

# í”„ë¡œì íŠ¸ë³„ íŒŒì¼ ì—…ë¡œë“œ ë° ê´€ë¦¬ API
@router.post("/{project_id}/files/upload")
async def upload_project_file(
    project_id: str,
    files: List[UploadFile] = File(...),
    description: Optional[str] = Form(None),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ì— íŒŒì¼ ì—…ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        uploaded_files = []
        
        for file in files:
            # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
            if not await validate_file(file):
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid file: {file.filename}"
                )
            
            # Gemini File APIë¡œ ì—…ë¡œë“œ
            try:
                # íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
                file_content = await file.read()
                
                # íŒŒì¼ í¬ê¸° ì¬ê²€ì¦
                if len(file_content) > MAX_FILE_SIZE:
                    raise HTTPException(
                        status_code=400,
                        detail=f"File {file.filename} is too large. Maximum size: {MAX_FILE_SIZE // (1024*1024)}MB"
                    )
                
                # Gemini File API ì œí•œ í™•ì¸ ë° ì²˜ë¦¬
                if len(file_content) > GEMINI_INLINE_DATA_LIMIT:
                    print(f"Warning: File {file.filename} ({len(file_content)} bytes) exceeds Gemini inline data limit. Using File API instead.")
                    # í° íŒŒì¼ì€ File APIë¥¼ í†µí•´ ì²˜ë¦¬ (ì´ë¯¸ í˜„ì¬ êµ¬í˜„)
                
                # File APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë¡œë“œ
                uploaded_file = client.files.upload(
                    file=io.BytesIO(file_content),
                    config=dict(
                        mime_type=file.content_type,
                        display_name=f"project_{project_id}_{file.filename}"
                    )
                )
                
                # íŒŒì¼ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
                max_wait_time = 30
                wait_time = 0
                while uploaded_file.state.name == 'PROCESSING' and wait_time < max_wait_time:
                    await asyncio.sleep(2)
                    wait_time += 2
                    try:
                        uploaded_file = client.files.get(name=uploaded_file.name)
                    except Exception as e:
                        print(f"Error checking file status: {e}")
                        break
                
                # ì²˜ë¦¬ ìƒíƒœ í™•ì¸
                if uploaded_file.state.name != 'ACTIVE':
                    print(f"Warning: File {file.filename} is in state {uploaded_file.state.name}")
                
                # file.nameì—ì„œ 'files/' ì œê±° (clean_file_id ì •ì˜)
                clean_file_id = uploaded_file.name.replace("files/", "") if uploaded_file.name.startswith("files/") else uploaded_file.name
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„ë² ë”© ìƒì„± (ê°œì„ ëœ ë°©ì‹)
                extracted_text = ""
                embeddings = []
                embedding_data_list = []  # í•­ìƒ ì´ˆê¸°í™”
                
                try:
                    # íŒŒì¼ì´ í™œì„± ìƒíƒœì¼ ë•Œë§Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if uploaded_file.state.name == 'ACTIVE':
                        # PDF íŒŒì¼ ì²˜ë¦¬ (ê°•í™”ëœ ë°©ì‹)
                        if file.content_type == "application/pdf":
                            # ë‹¤ì¤‘ ì‹œë„ ë°©ì‹ìœ¼ë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            extract_attempts = [
                                # 1ì°¨ ì‹œë„: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                {
                                    "prompt": "ì´ PDF ë¬¸ì„œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í‘œ, ê·¸ë˜í”„, ìˆ˜ì‹, ë„í‘œì˜ ë‚´ìš©ë„ í¬í•¨í•´ì„œ ìµœëŒ€í•œ ìì„¸íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš° 'í…ìŠ¤íŠ¸ ì—†ìŒ'ì´ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 8000
                                },
                                # 2ì°¨ ì‹œë„: ì´ë¯¸ì§€ ê¸°ë°˜ OCR (ìŠ¤ìº” ë¬¸ì„œ ëŒ€ì‘)
                                {
                                    "prompt": "ì´ ë¬¸ì„œë¥¼ ìŠ¤ìº”ëœ ì´ë¯¸ì§€ë¡œ ì¸ì‹í•˜ì—¬ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ OCRë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ìˆ˜ì‹, í‘œ, ê·¸ë˜í”„ì˜ ë‚´ìš©ë„ í¬í•¨í•´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì •í™•íˆ ì¸ì‹í•´ì£¼ì„¸ìš”. ì½ì„ ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì€ [ì½ì„ ìˆ˜ ì—†ìŒ]ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.",
                                    "max_tokens": 8000
                                },
                                # 3ì°¨ ì‹œë„: êµ¬ì¡°í™”ëœ ì¶”ì¶œ
                                {
                                    "prompt": "ì´ ë¬¸ì„œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì¡°í™”í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:\n\n[ì œëª©]\n[ë³¸ë¬¸ ë‚´ìš©]\n[í‘œ/ê·¸ë˜í”„ ë‚´ìš©]\n[ìˆ˜ì‹]\n[ê¸°íƒ€ ì •ë³´]\n\nê° ì„¹ì…˜ë³„ë¡œ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ë‚´ìš©ì´ ì—†ëŠ” ì„¹ì…˜ì€ 'ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.",
                                    "max_tokens": 8000
                                },
                                # 4ì°¨ ì‹œë„: ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œ
                                {
                                    "prompt": "ì´ ë¬¸ì„œë¥¼ ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë„í˜•, í‘œë¥¼ ëª¨ë‘ ì„¤ëª…í•˜ê³  í¬í•¨ëœ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì‹œê°ì  ìš”ì†Œë„ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 8000
                                }
                            ]
                            
                            for attempt_idx, attempt in enumerate(extract_attempts):
                                try:
                                    print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ {attempt_idx + 1}/{len(extract_attempts)}: {file.filename}")
                                    
                                    extract_response = client.models.generate_content(
                                        model="gemini-2.5-flash",
                                        contents=[
                                            uploaded_file,
                                            attempt["prompt"]
                                        ],
                                        config=types.GenerateContentConfig(
                                            temperature=0,
                                            max_output_tokens=attempt["max_tokens"]
                                        )
                                    )
                                    
                                    if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                        extracted_text = extract_response.text[:12000]  # ìµœëŒ€ 12000ìë¡œ ì¦ê°€
                                        if len(extracted_text.strip()) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                                            print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì‹œë„ {attempt_idx + 1}): {len(extracted_text)}ì")
                                            break
                                    else:
                                        print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt_idx + 1}): ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                                        
                                except Exception as e:
                                    print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ {attempt_idx + 1} ì‹¤íŒ¨: {e}")
                                    continue
                            
                            # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°±
                            if not extracted_text or len(extracted_text.strip()) < 50:
                                # íŒŒì¼ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì •ë³´ ìƒì„±
                                extracted_text = f"""
[íŒŒì¼ ì •ë³´]
íŒŒì¼ëª…: {file.filename}
íŒŒì¼ íƒ€ì…: PDF ë¬¸ì„œ
ìƒíƒœ: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨
ì²˜ë¦¬ ì‹œê°„: {datetime.now().isoformat()}

[ì•Œë¦¼]
ì´ PDF ë¬¸ì„œëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ì´ìœ ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë µìŠµë‹ˆë‹¤:
1. ìŠ¤ìº”ëœ ì´ë¯¸ì§€ í˜•íƒœì˜ PDF
2. ë³µì¡í•œ ë ˆì´ì•„ì›ƒ êµ¬ì¡°
3. ì•”í˜¸í™”ëœ PDF
4. ì†ê¸€ì”¨ ë˜ëŠ” íŠ¹ìˆ˜ í°íŠ¸ ì‚¬ìš©
5. ê·¸ë˜í”½ ìœ„ì£¼ì˜ ë¬¸ì„œ

[ëŒ€ì•ˆ]
- ë‹¤ë¥¸ PDF ë·°ì–´ì—ì„œ í…ìŠ¤íŠ¸ ë³µì‚¬ í›„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì—…ë¡œë“œ
- ì´ë¯¸ì§€ë¡œ ìŠ¤í¬ë¦°ìƒ· í›„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì—…ë¡œë“œ
- íŒŒì¼ì„ ë‹¤ì‹œ PDFë¡œ ë‚´ë³´ë‚´ê¸° ì‹œë„
                                """.strip()
                                print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ì „ ì‹¤íŒ¨ - ê¸°ë³¸ ì •ë³´ ìƒì„±: {file.filename}")
                                
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
                        elif file.content_type in ["text/plain"] or file.content_type.startswith("text/"):
                            try:
                                extract_response = client.models.generate_content(
                                    model="gemini-2.5-flash",
                                    contents=[
                                        uploaded_file,
                                        "ì´ í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ë‚´ìš©ì„ ì™„ì „íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
                                    ],
                                    config=types.GenerateContentConfig(
                                        temperature=0,
                                        max_output_tokens=8000
                                    )
                                )
                                
                                if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                    extracted_text = extract_response.text[:10000]
                                else:
                                    extracted_text = "í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                                    
                            except Exception as e:
                                extracted_text = f"í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                                print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: {file.filename} - {e}")
                                
                        # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ (ê°•í™”ëœ OCR)
                        elif file.content_type.startswith("image/"):
                            # ë‹¤ì¤‘ ì‹œë„ ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ OCR
                            ocr_attempts = [
                                # 1ì°¨ ì‹œë„: ê¸°ë³¸ OCR
                                {
                                    "prompt": "ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì¸ì‹í•˜ê³ , í‘œ, ê·¸ë˜í”„, ë„í‘œì˜ ë‚´ìš©ë„ í¬í•¨í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 4000
                                },
                                # 2ì°¨ ì‹œë„: êµ¬ì¡°í™”ëœ OCR
                                {
                                    "prompt": "ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•´ì„œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì œëª©, ë³¸ë¬¸, í‘œ, ê·¸ë˜í”„ ë“±ì„ êµ¬ë¶„í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 4000
                                },
                                # 3ì°¨ ì‹œë„: ìˆ˜ì‹ ë° ê¸°í˜¸ í¬í•¨ OCR
                                {
                                    "prompt": "ì´ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸, ìˆ˜ì‹, ê¸°í˜¸, í‘œë¥¼ ëª¨ë‘ ì¶”ì¶œí•´ì£¼ì„¸ìš”. íŠ¹íˆ ìˆ˜í•™ ê¸°í˜¸ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë„ ì •í™•íˆ ì¸ì‹í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 4000
                                }
                            ]
                            
                            for attempt_idx, attempt in enumerate(ocr_attempts):
                                try:
                                    print(f"ì´ë¯¸ì§€ OCR ì‹œë„ {attempt_idx + 1}/{len(ocr_attempts)}: {file.filename}")
                                    
                                    extract_response = client.models.generate_content(
                                        model="gemini-2.5-flash",
                                        contents=[
                                            uploaded_file,
                                            attempt["prompt"]
                                        ],
                                        config=types.GenerateContentConfig(
                                            temperature=0,
                                            max_output_tokens=attempt["max_tokens"]
                                        )
                                    )
                                    
                                    if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                        extracted_text = extract_response.text[:10000]
                                        if len(extracted_text.strip()) > 50:  # ìµœì†Œ 50ì ì´ìƒ
                                            print(f"ì´ë¯¸ì§€ OCR ì„±ê³µ (ì‹œë„ {attempt_idx + 1}): {len(extracted_text)}ì")
                                            break
                                    else:
                                        print(f"ì´ë¯¸ì§€ OCR ì‹¤íŒ¨ (ì‹œë„ {attempt_idx + 1}): ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                                        
                                except Exception as e:
                                    print(f"ì´ë¯¸ì§€ OCR ì‹œë„ {attempt_idx + 1} ì‹¤íŒ¨: {e}")
                                    continue
                            
                            # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°±
                            if not extracted_text or len(extracted_text.strip()) < 20:
                                # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì •ë³´ ìƒì„±
                                extracted_text = f"""
[ì´ë¯¸ì§€ ì •ë³´]
íŒŒì¼ëª…: {file.filename}
íŒŒì¼ íƒ€ì…: {file.content_type}
ìƒíƒœ: OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨
ì²˜ë¦¬ ì‹œê°„: {datetime.now().isoformat()}

[ì•Œë¦¼]
ì´ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë ¤ìš´ ì´ìœ :
1. í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ìˆœìˆ˜ ì´ë¯¸ì§€
2. ì†ê¸€ì”¨ë‚˜ íŠ¹ìˆ˜ í°íŠ¸ ì‚¬ìš©
3. ì´ë¯¸ì§€ í’ˆì§ˆì´ ë‚®ìŒ (í•´ìƒë„, íë¦¼)
4. ë³µì¡í•œ ë°°ê²½ì´ë‚˜ ë…¸ì´ì¦ˆ
5. ê¸°ìš¸ì–´ì§€ê±°ë‚˜ ì™œê³¡ëœ í…ìŠ¤íŠ¸

[ëŒ€ì•ˆ]
- ì´ë¯¸ì§€ í’ˆì§ˆì„ ë†’ì—¬ì„œ ë‹¤ì‹œ ì—…ë¡œë“œ
- í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ í¬ë¡­í•´ì„œ ì—…ë¡œë“œ
- í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ íƒ€ì´í•‘í•´ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì—…ë¡œë“œ
                                """.strip()
                                print(f"ì´ë¯¸ì§€ OCR ì™„ì „ ì‹¤íŒ¨ - ê¸°ë³¸ ì •ë³´ ìƒì„±: {file.filename}")
                        
                        # ì›Œë“œ ë¬¸ì„œ ì²˜ë¦¬
                        elif file.content_type in [
                            'application/msword',
                            'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
                        ]:
                            try:
                                extract_response = client.models.generate_content(
                                    model="gemini-2.5-flash",
                                    contents=[
                                        uploaded_file,
                                        "ì´ ì›Œë“œ ë¬¸ì„œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í‘œ, ê·¸ë˜í”„, ì´ë¯¸ì§€ ì„¤ëª…ë„ í¬í•¨í•´ì„œ ì™„ì „íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
                                    ],
                                    config=types.GenerateContentConfig(
                                        temperature=0,
                                        max_output_tokens=8000
                                    )
                                )
                                
                                if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                    extracted_text = extract_response.text[:10000]
                                else:
                                    extracted_text = "ì›Œë“œ ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨: ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                                    
                            except Exception as e:
                                extracted_text = f"ì›Œë“œ ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                                print(f"ì›Œë“œ ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {file.filename} - {e}")
                        
                        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ê²½ìš° ì„ë² ë”© ìƒì„±
                        if (extracted_text and len(extracted_text.strip()) > 30 and 
                            not any(fail_text in extracted_text for fail_text in [
                                "ì¶”ì¶œ ì‹¤íŒ¨", "OCR ì‹¤íŒ¨", "ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
                            ])):
                            
                            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ê°œì„ ëœ ë°©ì‹)
                            chunk_size = 1000  # ì²­í¬ í¬ê¸° ì¡°ì •
                            overlap = 100      # ì¤‘ë³µ ë²”ìœ„ ì¡°ì •
                            text_chunks = []
                            
                            # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
                            paragraphs = extracted_text.split('\n\n')
                            current_chunk = ""
                            
                            for paragraph in paragraphs:
                                if len(current_chunk) + len(paragraph) <= chunk_size:
                                    current_chunk += paragraph + "\n\n"
                                else:
                                    if current_chunk.strip():
                                        text_chunks.append(current_chunk.strip())
                                    current_chunk = paragraph + "\n\n"
                            
                            if current_chunk.strip():
                                text_chunks.append(current_chunk.strip())
                            
                            # ì²­í¬ê°€ ë„ˆë¬´ í° ê²½ìš° ê°•ì œ ë¶„í• 
                            final_chunks = []
                            for chunk in text_chunks:
                                if len(chunk) > chunk_size:
                                    for i in range(0, len(chunk), chunk_size - overlap):
                                        sub_chunk = chunk[i:i + chunk_size]
                                        if sub_chunk.strip():
                                            final_chunks.append(sub_chunk.strip())
                                else:
                                    final_chunks.append(chunk)
                            
                            print(f"í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì™„ë£Œ: {len(final_chunks)}ê°œ ì²­í¬")
                            
                            # ì„ë² ë”© ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ ê³ ë ¤)
                            for i, chunk in enumerate(final_chunks):
                                if len(chunk.strip()) < 20:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ì œì™¸
                                    continue
                                    
                                try:
                                    embed_result = client.models.embed_content(
                                        model="text-embedding-004",
                                        contents=chunk,
                                        config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
                                    )
                                    
                                    if embed_result.embeddings:
                                        # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
                                        embedding_vector = (
                                            embed_result.embeddings[0].values 
                                            if hasattr(embed_result.embeddings[0], 'values') 
                                            else list(embed_result.embeddings[0])
                                        )
                                        
                                        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„
                                        embedding_data_list.append({
                                            "project_id": project_id,
                                            "file_id": clean_file_id,
                                            "file_name": file.filename,
                                            "chunk_index": i,
                                            "chunk_text": chunk,
                                            "embedding_vector": embedding_vector,
                                            "embedding_model": "text-embedding-004",
                                            "task_type": "RETRIEVAL_DOCUMENT",
                                            "chunk_size": len(chunk),
                                            "similarity_threshold": 0.75
                                        })
                                        
                                        # ê¸°ì¡´ í˜•ì‹ë„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
                                        embeddings.append({
                                            "chunk_index": i,
                                            "text": chunk,
                                            "embedding": embed_result.embeddings[0],
                                            "size": len(chunk)
                                        })
                                        
                                except Exception as e:
                                    print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ì²­í¬ {i}): {e}")
                                    continue
                            
                            print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding_data_list)}ê°œ ì„ë² ë”©")
                        
                        else:
                            print(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ê°€ ì„ë² ë”© ìƒì„±ì— ë¶€ì í•©: {file.filename}")
                    
                    else:
                        print(f"íŒŒì¼ì´ ACTIVE ìƒíƒœê°€ ì•„ë‹˜: {file.filename} (ìƒíƒœ: {uploaded_file.state.name})")
                        extracted_text = f"íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘: {uploaded_file.state.name}"
                        
                except Exception as e:
                    print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file.filename} - {e}")
                    extracted_text = f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì„ë² ë”© ì €ì¥
                if embedding_data_list:
                    try:
                        embedding_creates = [ProjectEmbeddingCreate(**data) for data in embedding_data_list]
                        saved_embeddings = crud_embedding.batch_create_embeddings(db, embedding_creates)
                        print(f"ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ì„ë² ë”©: {len(saved_embeddings)}ê°œ (íŒŒì¼: {file.filename})")
                    except Exception as e:
                        print(f"ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                else:
                    print(f"ì €ì¥í•  ì„ë² ë”©ì´ ì—†ìŒ: {file.filename}")
                
                # íŒŒì¼ ì •ë³´ ì €ì¥ (ì„ë² ë”© ì •ë³´ í¬í•¨)
                file_info = {
                    "file_id": clean_file_id,
                    "original_name": file.filename,
                    "mime_type": file.content_type,
                    "size": len(file_content),
                    "uri": uploaded_file.uri,
                    "state": uploaded_file.state.name,
                    "upload_time": datetime.now().isoformat(),
                    "description": description,
                    "extracted_text": extracted_text,
                    "processing_status": "completed" if uploaded_file.state.name == 'ACTIVE' else "processing",
                    "embeddings": embeddings if embeddings else [],
                    "embedding_stats": {
                        "chunks_count": len(embeddings),
                        "total_chars": sum(chunk["size"] for chunk in embeddings) if embeddings else 0,
                        "embedding_model": "text-embedding-004",
                        "has_embeddings": len(embeddings) > 0
                    }
                }
                
                uploaded_files.append(file_info)
                
            except HTTPException:
                raise
            except Exception as e:
                print(f"Error uploading file {file.filename}: {e}")
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to upload file {file.filename}: {str(e)}"
                )
        
        return {
            "project_id": project_id,
            "uploaded_files": uploaded_files,
            "total_files": len(uploaded_files),
            "message": f"Successfully uploaded {len(uploaded_files)} files"
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to upload files: {str(e)}")

@router.get("/{project_id}/files")
async def list_project_files(
    project_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ì— ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # í”„ë¡œì íŠ¸ ê´€ë ¨ íŒŒì¼ë“¤ ì¡°íšŒ
        project_files = []
        for file in client.files.list():
            if file.display_name and file.display_name.startswith(f"project_{project_id}_"):
                # file.nameì—ì„œ 'files/' ì œê±° (Gemini APIì—ì„œ files/file_id í˜•íƒœë¡œ ë°˜í™˜ë¨)
                clean_file_id = file.name.replace("files/", "") if file.name.startswith("files/") else file.name
                
                file_info = {
                    "file_id": clean_file_id,
                    "display_name": file.display_name,
                    "original_name": file.display_name.replace(f"project_{project_id}_", ""),
                    "uri": file.uri,
                    "state": file.state.name,
                    "create_time": file.create_time.isoformat() if hasattr(file, 'create_time') and file.create_time else None,
                    "expire_time": file.expire_time.isoformat() if hasattr(file, 'expire_time') and file.expire_time else None
                }
                project_files.append(file_info)
        
        return {
            "project_id": project_id,
            "files": project_files,
            "total_count": len(project_files)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list project files: {str(e)}")

@router.delete("/{project_id}/files/{file_id}")
async def delete_project_file(
    project_id: str,
    file_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ íŒŒì¼ ì‚­ì œ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # Gemini APIëŠ” files/file_id í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ files/ ì ‘ë‘ì‚¬ ì¶”ê°€
        full_file_id = f"files/{file_id}" if not file_id.startswith("files/") else file_id
        
        # ê´€ë ¨ ì„ë² ë”© ë¨¼ì € ì‚­ì œ
        try:
            deleted_embeddings = crud_embedding.delete_by_file(db, project_id, file_id)
            print(f"Deleted {deleted_embeddings} embeddings for file {file_id}")
        except Exception as e:
            print(f"Failed to delete embeddings for file {file_id}: {e}")
        
        # íŒŒì¼ ì‚­ì œ
        try:
            client.files.delete(name=full_file_id)
        except Exception as e:
            print(f"Failed to delete file {full_file_id}: {e}")
            # íŒŒì¼ì´ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            pass
        
        return {
            "message": "File deleted successfully",
            "project_id": project_id,
            "file_id": file_id
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete file: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ API
@router.post("/{project_id}/knowledge/search")
async def search_project_knowledge(
    project_id: str,
    query: str = Form(...),
    top_k: int = Form(5),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì—…ë¡œë“œ íŒŒì¼ë“¤ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # geminiapiupdate ì°¸ê³ : ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹œ RETRIEVAL_QUERY íƒœìŠ¤í¬ íƒ€ì… ì‚¬ìš©
        query_embed_result = client.models.embed_content(
            model="text-embedding-004",
            contents=query,
            config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY")
        )
        
        if not query_embed_result.embeddings:
            raise HTTPException(status_code=500, detail="Failed to generate query embedding")
        
        # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
        query_embedding = query_embed_result.embeddings[0].values if hasattr(query_embed_result.embeddings[0], 'values') else list(query_embed_result.embeddings[0])
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰ (ì„ê³„ê°’ ë‚®ì¶¤)
        try:
            print(f"ğŸ” ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            print(f"   í”„ë¡œì íŠ¸ ID: {project_id}")
            print(f"   ìš”ì²­ ê²°ê³¼ ìˆ˜: {top_k}")
            
            similar_embeddings = crud_embedding.search_similar(
                db=db,
                project_id=project_id,
                query_embedding=query_embedding,
                top_k=top_k,
                threshold=0.4  # ì„ê³„ê°’ì„ 0.75ì—ì„œ 0.4ë¡œ ë‚®ì¶¤
            )
            
            print(f"   ê²€ìƒ‰ëœ ì„ë² ë”© ìˆ˜: {len(similar_embeddings)}")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜
            top_chunks = []
            for i, result in enumerate(similar_embeddings):
                top_chunks.append({
                    "text": result["content"],
                    "similarity": result["similarity"],
                    "source_file": result["file_name"],
                    "chunk_index": result["chunk_index"]
                })
                print(f"   [{i+1}] ìœ ì‚¬ë„: {result['similarity']:.3f}, íŒŒì¼: {result['file_name']}")
                
        except Exception as e:
            print(f"âŒ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            # ë””ë²„ê¹…ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´
            all_embeddings = crud_embedding.get_by_project(db, project_id)
            print(f"   ì „ì²´ ì„ë² ë”© ê°œìˆ˜: {len(all_embeddings)}")
            if all_embeddings:
                print(f"   íŒŒì¼ ëª©ë¡: {list(set(e.file_name for e in all_embeddings))}")
            # í´ë°±: ë¹ˆ ê²°ê³¼ ë°˜í™˜
            top_chunks = []
        
        # ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
        search_results = []
        if top_chunks:
            # ê´€ë ¨ ì²­í¬ë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            combined_context = "\n\n".join([
                f"[{chunk['source_file']}] {chunk['text']}"
                for chunk in top_chunks
            ])
            
            # AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
            search_response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=[
                    f"""
                    ë‹¤ìŒì€ ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì—ì„œ ì¶”ì¶œí•œ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:
                    
                    {combined_context}
                    
                    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
                    ì§ˆë¬¸: {query}
                    
                    ë‹µë³€ í˜•ì‹:
                    1. í•µì‹¬ ë‚´ìš© ìš”ì•½
                    2. êµ¬ì²´ì ì¸ ë‹µë³€
                    3. ì¶œì²˜ ë° ì°¸ê³ ì‚¬í•­
                    
                    ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš° "ì œê³µëœ ìë£Œì—ì„œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
                    """
                ],
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=2000
                )
            )
            
            search_results.append({
                "content": search_response.text,
                "relevance_score": max(chunk["similarity"] for chunk in top_chunks),
                "source_chunks": len(top_chunks),
                "source_files": list(set(chunk["source_file"] for chunk in top_chunks))
            })
        
        return {
            "project_id": project_id,
            "query": query,
            "results": search_results,
            "total_results": len(search_results),
            "embedding_stats": crud_embedding.get_embedding_stats(db, project_id)  # ì„ë² ë”© í†µê³„ ì¶”ê°€
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to search knowledge: {str(e)}")

# ì„ë² ë”© í†µê³„ ì¡°íšŒ API ì¶”ê°€
@router.get("/{project_id}/embeddings/stats")
async def get_project_embedding_stats(
    project_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì„ë² ë”© í†µê³„ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ì„ë² ë”© í†µê³„ ì¡°íšŒ
        stats = crud_embedding.get_embedding_stats(db, project_id)
        
        # ìµœê·¼ ì„ë² ë”© ì •ë³´ ì¡°íšŒ
        recent_embeddings = crud_embedding.get_by_project(db, project_id)
        
        # íŒŒì¼ë³„ í†µê³„
        file_stats = {}
        for embedding in recent_embeddings:
            file_name = embedding.file_name
            if file_name not in file_stats:
                file_stats[file_name] = {
                    "chunks": 0,
                    "total_chars": 0,
                    "embedding_model": embedding.embedding_model,
                    "task_type": embedding.task_type
                }
            file_stats[file_name]["chunks"] += 1
            file_stats[file_name]["total_chars"] += embedding.chunk_size
        
        return {
            "project_id": project_id,
            "project_name": project.name,
            "embedding_stats": stats,
            "file_stats": file_stats,
            "embedding_model": "text-embedding-004",
            "supported_task_types": ["RETRIEVAL_DOCUMENT", "RETRIEVAL_QUERY"],
            "threshold": 0.75
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get embedding stats: {str(e)}")

# ì„ë² ë”© ì¬ìƒì„± API ì¶”ê°€
@router.post("/{project_id}/embeddings/regenerate")
async def regenerate_project_embeddings(
    project_id: str,
    file_id: Optional[str] = None,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì„ë² ë”© ì¬ìƒì„± (íŠ¹ì • íŒŒì¼ ë˜ëŠ” ì „ì²´)"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        if file_id:
            # íŠ¹ì • íŒŒì¼ì˜ ì„ë² ë”©ë§Œ ì¬ìƒì„±
            deleted_count = crud_embedding.delete_by_file(db, project_id, file_id)
            message = f"Regenerated embeddings for file {file_id} (deleted {deleted_count} old embeddings)"
        else:
            # ì „ì²´ í”„ë¡œì íŠ¸ ì„ë² ë”© ì¬ìƒì„±
            deleted_count = crud_embedding.delete_by_project(db, project_id)
            message = f"Regenerated all project embeddings (deleted {deleted_count} old embeddings)"
        
        return {
            "project_id": project_id,
            "message": message,
            "deleted_embeddings": deleted_count,
            "note": "íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œìš´ ì„ë² ë”©ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to regenerate embeddings: {str(e)}")

# í–¥ìƒëœ í”„ë¡œì íŠ¸ ì±„íŒ…ì—ì„œ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ í™œìš©
async def generate_gemini_stream_response_with_files(
    messages: list,
    model: str,
    room_id: str,
    db: Session,
    user_id: str,
    project_id: str,
    project_type: Optional[str] = None,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    file_names: Optional[List[str]] = None
) -> AsyncGenerator[str, None]:
    """íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ í”„ë¡œì íŠ¸ ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")

        # í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project = crud_project.get(db=db, id=project_id)
        
        # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = BRIEF_SYSTEM_PROMPT
        if project_type == "assignment":
            system_prompt += "\n\n" + ASSIGNMENT_PROMPT
        elif project_type == "record":
            system_prompt += "\n\n" + RECORD_PROMPT
            
        # í”„ë¡œì íŠ¸ ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        if project and project.system_instruction and project.system_instruction.strip():
            system_prompt += "\n\n## ì¶”ê°€ ì§€ì‹œì‚¬í•­\n" + project.system_instruction.strip()

        # í”„ë¡œì íŠ¸ ì—…ë¡œë“œ íŒŒì¼ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        project_files = []
        try:
            for file in client.files.list():
                if file.display_name and file.display_name.startswith(f"project_{project_id}_"):
                    if file.state.name == "ACTIVE":
                        project_files.append(file)
            
            if project_files:
                system_prompt += f"""
                
                ## ğŸ“ í”„ë¡œì íŠ¸ ì°¸ê³  ìë£Œ
                ì´ í”„ë¡œì íŠ¸ì—ëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ì´ ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤:
                {', '.join([f.display_name.replace(f'project_{project_id}_', '') for f in project_files])}
                
                ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´ ì´ íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
                """
        except Exception as e:
            print(f"Failed to load project files: {e}")

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬ ë° ì²˜ë¦¬
        valid_messages = []
        for msg in messages[-15:]:  # ìµœê·¼ 15ê°œë§Œ 
            if msg.get("content") and msg["content"].strip():
                valid_messages.append(msg)

        if len(valid_messages) == 0:
            raise HTTPException(status_code=400, detail="No valid message content found")

        # ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # í”„ë¡œì íŠ¸ íŒŒì¼ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
        for file in project_files[:3]:
            contents.append(file)
        
        # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬
        if file_data_list and file_types and file_names:
            for file_data, file_type, file_name in zip(file_data_list, file_types, file_names):
                if file_type.startswith("image/"):
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )
                elif file_type == "application/pdf":
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )

        # ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation_text = ""
        for message in valid_messages:
            role_text = "Human" if message["role"] == "user" else "Assistant"
            conversation_text += f"{role_text}: {message['content']}\n"

        contents.append(conversation_text)

        # ë„êµ¬ ì„¤ì •
        tools = [
            types.Tool(google_search=types.GoogleSearch()),
            types.Tool(code_execution=types.ToolCodeExecution())
        ]

        # ìƒì„± ì„¤ì •
        generation_config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            temperature=0.7,
            top_p=0.95,
            max_output_tokens=8192,
            tools=tools,
            thinking_config=types.ThinkingConfig(
                thinking_budget=16384,
                include_thoughts=True
            )
        )

        # í† í° ê³„ì‚°
        input_token_count = await count_gemini_tokens(conversation_text, model, client)
        input_tokens = input_token_count.get("input_tokens", 0)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        accumulated_content = ""
        accumulated_reasoning = ""
        thought_time = 0.0
        citations = []
        citations_sent = set()
        new_citations = []

        try:
            response = client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=generation_config
            )

            start_time = time.time()
            
            for chunk in response:
                if chunk.candidates and len(chunk.candidates) > 0:
                    candidate = chunk.candidates[0]
                    
                    # ì½˜í…ì¸  íŒŒíŠ¸ ì²˜ë¦¬
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if hasattr(part, 'thought') and part.thought:
                                accumulated_reasoning += part.text
                                thought_time = time.time() - start_time
                                yield f"data: {json.dumps({'reasoning_content': part.text, 'thought_time': thought_time})}\n\n"
                            elif part.text:
                                accumulated_content += part.text
                                yield f"data: {json.dumps({'content': part.text})}\n\n"

                    # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding = candidate.grounding_metadata
                        
                        if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                            for chunk_info in grounding.grounding_chunks:
                                if hasattr(chunk_info, 'web') and chunk_info.web:
                                    citation_url = chunk_info.web.uri
                                    if citation_url not in citations_sent:
                                        citation = {
                                            "url": citation_url,
                                            "title": chunk_info.web.title if hasattr(chunk_info.web, 'title') else ""
                                        }
                                        citations.append(citation)
                                        new_citations.append(citation)
                                        citations_sent.add(citation_url)
                            
                            if new_citations:
                                try:
                                    yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                    new_citations = []  # ì „ì†¡ í›„ ì´ˆê¸°í™”
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    return

            # ì¶œë ¥ í† í° ê³„ì‚°
            output_token_count = await count_gemini_tokens(accumulated_content, model, client)
            output_tokens = output_token_count.get("input_tokens", 0)
            
            # ì‚¬ê³  í† í° ê³„ì‚°
            thinking_tokens = 0
            if accumulated_reasoning:
                thinking_token_count = await count_gemini_tokens(accumulated_reasoning, model, client)
                thinking_tokens = thinking_token_count.get("input_tokens", 0)

            # í† í° ì‚¬ìš©ëŸ‰ ì €ì¥
            crud_stats.create_token_usage(
                db=db,
                user_id=user_id,
                room_id=room_id,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens + thinking_tokens,
                timestamp=datetime.now(),
                chat_type=f"project_{project_type}" if project_type else None
            )

            # AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
            if accumulated_content:
                message_create = ChatMessageCreate(
                    content=accumulated_content,
                    role="assistant",
                    room_id=room_id,
                    reasoning_content=accumulated_reasoning if accumulated_reasoning else None,
                    thought_time=thought_time if thought_time > 0 else None,
                    citations=citations if citations else None
                )
                crud_project.create_chat_message(db, project_id=project_id, chat_id=room_id, obj_in=message_create)

        except Exception as api_error:
            error_message = f"Gemini API Error: {str(api_error)}"
            yield f"data: {json.dumps({'error': error_message})}\n\n"

    except Exception as e:
        error_message = f"Enhanced Stream Generation Error: {str(e)}"
        yield f"data: {json.dumps({'error': error_message})}\n\n" 