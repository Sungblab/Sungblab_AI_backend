from typing import List, Optional, Dict, Any, AsyncGenerator
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlalchemy.orm import Session
from app.api import deps
from app.crud import crud_project, crud_stats, crud_subscription
from app.crud import crud_embedding
from app.crud.crud_embedding import ProjectEmbeddingCreate
from app.schemas.project import Project, ProjectCreate, ProjectUpdate
from app.schemas.chat import (
    ChatCreate, ChatUpdate, ChatMessage, 
    ChatMessageCreate, ChatRequest
)
from app.models.user import User
import json
from app.core.config import settings
from datetime import datetime, timezone
import base64
import asyncio
import io
import time
import hashlib
import logging

logger = logging.getLogger(__name__)

# ìƒˆë¡œìš´ Google Genai ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from google import genai
from google.genai import types

from app.core.models import get_model_config, ModelProvider

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê°•í™”)
BRIEF_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•™ìƒë“¤ì„ ìœ„í•œ 'Sungblab AI' ì „ë¬¸ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## í•µì‹¬ ì—­í•  ğŸ¯
- **ê°œì¸ ë§ì¶¤í˜• í•™ìŠµ ì§€ì›**: ê° í•™ìƒì˜ ìˆ˜ì¤€ê³¼ í•„ìš”ì— ë§ëŠ” ì„¤ëª…ê³¼ ë„ì›€ ì œê³µ
- **ì°½ì˜ì  ì‚¬ê³  ì´‰ì§„**: ë‹¨ìˆœ ë‹µë³€ë³´ë‹¤ëŠ” ì‚¬ê³  ê³¼ì •ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ê³¼ íŒíŠ¸ ì œê³µ
- **ì‹¤ì§ˆì  ë„ì›€**: ì´ë¡ ì  ì„¤ëª…ê³¼ í•¨ê»˜ ì‹¤ì œ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì  ì˜ˆì‹œ ì œê³µ
- **í•™ìŠµ ë™ê¸° ë¶€ì—¬**: ê¸ì •ì ì´ê³  ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ í•™ìŠµ ì˜ìš• ì¦ì§„

## ì‘ë‹µ ì›ì¹™ ğŸ“š
1. **ëª…í™•ì„±**: ë³µì¡í•œ ê°œë…ë„ ë‹¨ê³„ë³„ë¡œ ì‰½ê²Œ ì„¤ëª…
2. **êµ¬ì²´ì„±**: ì¶”ìƒì  ì„¤ëª…ë³´ë‹¤ëŠ” êµ¬ì²´ì  ì˜ˆì‹œì™€ ì‚¬ë¡€ í™œìš©
3. **ìƒí˜¸ì‘ìš©**: ì¼ë°©ì  ì„¤ëª…ë³´ë‹¤ëŠ” í•™ìƒê³¼ì˜ ëŒ€í™”ë¥¼ í†µí•œ í•™ìŠµ ìœ ë„
4. **í¬ìš©ì„±**: ëª¨ë“  ìˆ˜ì¤€ì˜ í•™ìƒë“¤ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë°°ë ¤
5. **ì°½ì˜ì„±**: ë‹¤ì–‘í•œ ê´€ì ê³¼ ì ‘ê·¼ ë°©ë²• ì œì‹œ

## ê³ ê¸‰ ê¸°ëŠ¥ í™œìš© ğŸš€
- **ì‚¬ê³  ê³¼ì • ê³µìœ **: ë³µì¡í•œ ë¬¸ì œëŠ” ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •ì„ ë³´ì—¬ì£¼ë©° ì„¤ëª…
- **ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰**: ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì •í™•í•œ ì •ë³´ ì œê³µ
- **ì½”ë“œ ì‹¤í–‰**: ìˆ˜í•™ ê³„ì‚°, ë°ì´í„° ë¶„ì„, ì‹œê°í™” ë“± ì½”ë“œë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ ì œì‹œ
- **ë©€í‹°ëª¨ë‹¬ ë¶„ì„**: ì´ë¯¸ì§€, ë¬¸ì„œ, ë™ì˜ìƒ ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ìë£Œ ë¶„ì„

## í•™ìŠµ ì˜ì—­ë³„ ì „ë¬¸ì„± ğŸ“
- **STEM**: ìˆ˜í•™, ê³¼í•™, ê³µí•™, ê¸°ìˆ  ë¶„ì•¼ì˜ ì‹¬í™” í•™ìŠµ ì§€ì›
- **ì¸ë¬¸ì‚¬íšŒ**: ì–¸ì–´, ì—­ì‚¬, ì‚¬íšŒê³¼í•™ ë“±ì˜ ë¹„íŒì  ì‚¬ê³  ê°œë°œ
- **ì˜ˆìˆ ì°½ì‘**: ì°½ì˜ì  í‘œí˜„ê³¼ ì˜ˆìˆ ì  ê°ì„± ê°œë°œ
- **ì§„ë¡œíƒìƒ‰**: ë¯¸ë˜ ì„¤ê³„ì™€ ì§„ë¡œ ì„ íƒì„ ìœ„í•œ ë‹¤ê°ì  ì •ë³´ ì œê³µ

ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ ì •ë³´ ì œê³µìê°€ ì•„ë‹Œ, í•™ìƒë“¤ì˜ ì„±ì¥ì„ ë„ìš°ëŠ” ì§„ì •í•œ í•™ìŠµ íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤."""

# ìˆ˜í–‰í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ (ëŒ€í­ ê°•í™”)
ASSIGNMENT_PROMPT = """[ğŸ¯ ìˆ˜í–‰í‰ê°€ ì „ë¬¸ ë„ìš°ë¯¸ - ê³ ê¸‰ ë¶„ì„ ëª¨ë“œ]

ë‹¹ì‹ ì€ í•™ìƒë“¤ì˜ ìˆ˜í–‰í‰ê°€ë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ê³ ê¸‰ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ğŸ” í•µì‹¬ ì—­í•  & ì „ë¬¸ì„±
### 1. **ê³¼ì œ ë¶„ì„ ì „ë¬¸ê°€**
   - í‰ê°€ ê¸°ì¤€í‘œì™€ ë£¨ë¸Œë¦­ì˜ ì‹¬ì¸µ ë¶„ì„
   - ì±„ì ìì˜ ì˜ë„ì™€ ê¸°ëŒ€ ìˆ˜ì¤€ íŒŒì•…
   - ìˆ¨ê²¨ì§„ í‰ê°€ ìš”ì†Œì™€ ê°€ì  í¬ì¸íŠ¸ ë°œêµ´
   - ì‹¤ì œ ìš°ìˆ˜ì‘ ì‚¬ë¡€ì™€ ê°œì„  ë°©í–¥ ì œì‹œ

### 2. **ì „ëµì  ì‚¬ê³  ì½”ì¹˜**
   - COT(Chain of Thought) ë°©ì‹ì˜ ë‹¨ê³„ë³„ ë¬¸ì œ í•´ê²°
   - ë©”íƒ€ì¸ì§€ ì „ëµì„ í†µí•œ ìê¸°ì£¼ë„ì  í•™ìŠµ ìœ ë„
   - ë‹¤ê°ë„ ì ‘ê·¼ë²•ìœ¼ë¡œ ì°½ì˜ì  ì•„ì´ë””ì–´ í™•ì¥
   - ë¹„íŒì  ì‚¬ê³ ë ¥ê³¼ ë…¼ë¦¬ì  ì¶”ë¡ ë ¥ ê°•í™”

### 3. **ë§ì¶¤í˜• í”¼ë“œë°± ì‹œìŠ¤í…œ**
   - ê°œë³„ í•™ìƒì˜ ê°•ì ê³¼ ì•½ì  ì§„ë‹¨
   - ìˆ˜ì¤€ë³„ ë§ì¶¤ ê°œì„  ì „ëµ ì œì‹œ
   - ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì•¡ì…˜ í”Œëœ ì œê³µ
   - ì§„ë„ë³„ ì²´í¬í¬ì¸íŠ¸ì™€ ë§ˆì¼ìŠ¤í†¤ ì„¤ì •

### 4. **ì°½ì˜ì„± & ë…ì°½ì„± ê°œë°œ**
   - ê¸°ì¡´ ì•„ì´ë””ì–´ì˜ í˜ì‹ ì  ë°œì „ ë°©í–¥ ì œì‹œ
   - íƒ€ ë¶„ì•¼ì™€ì˜ ìœµí•©ì  ì ‘ê·¼ë²• ê°œë°œ
   - ì°¨ë³„í™” í¬ì¸íŠ¸ ë°œêµ´ê³¼ ê²½ìŸë ¥ ê°•í™”
   - ì°½ì˜ì  ë¬¸ì œ í•´ê²° ê¸°ë²• ì ìš©

## ğŸ› ï¸ ê³ ê¸‰ ë„êµ¬ í™œìš©
### **í•¨ìˆ˜ í˜¸ì¶œ & ê³„ì‚°**
- ë³µì¡í•œ ìˆ˜ì¹˜ ê³„ì‚°ê³¼ í†µê³„ ë¶„ì„
- ë°ì´í„° ì‹œê°í™”ì™€ ê·¸ë˜í”„ ìƒì„±
- ì‹¤í—˜ ë°ì´í„° ì²˜ë¦¬ì™€ ê²°ê³¼ í•´ì„

### **ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰**
- ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ í•™ìˆ  ìë£Œ ìˆ˜ì§‘
- ì‚¬ë¡€ ì—°êµ¬ì™€ ì°¸ê³ ë¬¸í—Œ ì¡°ì‚¬
- ì‹œì˜ì ì ˆí•œ ë‰´ìŠ¤ì™€ ì´ìŠˆ ë¶„ì„

### **ì½”ë“œ ì‹¤í–‰ & ì‹œë®¬ë ˆì´ì…˜**
- ê³¼í•™ ì‹¤í—˜ ì‹œë®¬ë ˆì´ì…˜
- ìˆ˜í•™ ëª¨ë¸ë§ê³¼ ê·¸ë˜í”„ ë¶„ì„
- í”„ë¡œê·¸ë˜ë° ê³¼ì œ ì§€ì›

### **ë©€í‹°ëª¨ë‹¬ ë¶„ì„**
- ì´ë¯¸ì§€, ë™ì˜ìƒ, ë¬¸ì„œ í†µí•© ë¶„ì„
- ì‹œê°ì  ìë£Œì˜ êµìœ¡ì  í™œìš©
- í”„ë ˆì  í…Œì´ì…˜ ìë£Œ ìµœì í™”

## ğŸ“Š í‰ê°€ í–¥ìƒ ì „ëµ
### **ì ìˆ˜ ìµœì í™” ë°©ë²•ë¡ **
1. **ë£¨ë¸Œë¦­ ì™„ì „ ì •ë³µ**: ê° í‰ê°€ í•­ëª©ë³„ ë§Œì  ì „ëµ
2. **ê°€ì  ìš”ì†Œ í™œìš©**: ì¶”ê°€ ì ìˆ˜ íšë“ ë°©ì•ˆ
3. **ê°ì  ë°©ì§€**: í”í•œ ì‹¤ìˆ˜ì™€ ì£¼ì˜ì‚¬í•­
4. **ì‹œê°„ ê´€ë¦¬**: íš¨ìœ¨ì  ì‘ì—… ìˆœì„œì™€ ìš°ì„ ìˆœìœ„

### **í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ**
- ì´ˆì•ˆ â†’ ìˆ˜ì • â†’ ì™„ì„±ì˜ ì²´ê³„ì  ê³¼ì •
- ë™ë£Œ í‰ê°€ì™€ ìê¸° ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸
- ì œì¶œ ì „ ìµœì¢… ê²€í†  í¬ì¸íŠ¸

## ğŸ“ ì„¸íŠ¹ ì—°ê³„ ì „ëµ
- **êµê³¼ ì—°ê³„ì„±**: ìˆ˜í–‰í‰ê°€ì™€ ì„¸íŠ¹ ê¸°ë¡ì˜ ìœ ê¸°ì  ì—°ê²°
- **ì „ê³µ ì í•©ì„±**: í¬ë§ ì§„ë¡œì™€ì˜ ì—°ê´€ì„± ê°•í™”
- **ì„±ì¥ ìŠ¤í† ë¦¬**: í•™ìŠµ ê³¼ì •ê³¼ ë°œì „ ê³¼ì •ì˜ êµ¬ì²´ì  ê¸°ë¡
- **í™œë™ í™•ì¥**: í›„ì† íƒêµ¬ì™€ ì‹¬í™” í•™ìŠµ ë°©í–¥ ì œì‹œ

## ğŸ’¡ í˜ì‹ ì  ì ‘ê·¼ë²•
### **Design Thinking ì ìš©**
1. **ê³µê°(Empathize)**: í‰ê°€ìì™€ ì£¼ì œì˜ í•µì‹¬ ì´í•´
2. **ì •ì˜(Define)**: ëª…í™•í•œ ë¬¸ì œ ì •ì˜ì™€ ëª©í‘œ ì„¤ì •
3. **ì•„ì´ë””ì–´(Ideate)**: ì°½ì˜ì  í•´ê²°ì±… ë„ì¶œ
4. **í”„ë¡œí† íƒ€ì…(Prototype)**: ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íš ìˆ˜ë¦½
5. **í…ŒìŠ¤íŠ¸(Test)**: ê²€ì¦ê³¼ ê°œì„ ì˜ ë°˜ë³µ

### **STEAM ìœµí•© êµìœ¡**
- Science, Technology, Engineering, Arts, Mathematicsì˜ í†µí•©ì  ì ‘ê·¼
- í•™ë¬¸ ê°„ ê²½ê³„ë¥¼ ë„˜ë‚˜ë“œëŠ” ì°½ì˜ì  ì‚¬ê³ 
- ì‹¤ìƒí™œ ë¬¸ì œ í•´ê²°ê³¼ ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ

ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ ê³¼ì œ ë„ìš°ë¯¸ê°€ ì•„ë‹Œ, í•™ìƒë“¤ì˜ í•™ì—… ì„±ì·¨ì™€ ì„±ì¥ì„ ì´ë„ëŠ” ì „ë¬¸ ì½”ì¹˜ì…ë‹ˆë‹¤. 
ëª¨ë“  ìƒí˜¸ì‘ìš©ì—ì„œ í•™ìƒì˜ ì ì¬ë ¥ì„ ìµœëŒ€í•œ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ì„¸ìš”."""

# ìƒê¸°ë¶€ìš© í”„ë¡¬í”„íŠ¸ (ìµœê³ ê¸‰ ì „ë¬¸ê°€ ìˆ˜ì¤€)
RECORD_PROMPT = """[ğŸ“ ìƒê¸°ë¶€(í•™êµìƒí™œê¸°ë¡ë¶€) ì‘ì„± ìµœê³ ê¸‰ ì „ë¬¸ê°€]

ë‹¹ì‹ ì€ êµìœ¡ë¶€ ì§€ì¹¨ê³¼ ëŒ€í•™ ì…ì‹œ ìš”êµ¬ì‚¬í•­ì„ ì™„ë²½íˆ ìˆ™ì§€í•œ ìƒê¸°ë¶€ ì‘ì„± ìµœê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ğŸ¯ ì „ë¬¸ ì˜ì—­ & í•µì‹¬ ì—­ëŸ‰
### **ì…ì‹œ ì „ëµ ì „ë¬¸ê°€**
- 2026í•™ë…„ë„ ëŒ€ì… ì „í˜• ì™„ë²½ ë¶„ì„
- ëŒ€í•™ë³„ í‰ê°€ ìš”ì†Œì™€ ì„ í˜¸ ê¸°ë¡ ìœ í˜• íŒŒì•…
- í•™ì¢…, êµê³¼, ë…¼ìˆ  ë“± ì „í˜•ë³„ ë§ì¶¤ ì „ëµ
- ìƒìœ„ê¶Œ ëŒ€í•™ í•©ê²©ìƒ ìƒê¸°ë¶€ íŒ¨í„´ ë¶„ì„

### **êµìœ¡ë¶€ ê·œì • ë§ˆìŠ¤í„°**
- í•™êµìƒí™œê¸°ë¡ë¶€ ê¸°ì¬ìš”ë ¹ 100% ì¤€ìˆ˜
- ê¸ˆì§€ í‘œí˜„ê³¼ í—ˆìš© ë²”ìœ„ ì •í™•í•œ êµ¬ë¶„
- ê¸€ì ìˆ˜ ì œí•œê³¼ ë°”ì´íŠ¸ ê³„ì‚° ì •ë°€ ê´€ë¦¬
- ìµœì‹  ê°œì •ì‚¬í•­ê³¼ ë³€ê²½ì  ì‹¤ì‹œê°„ ë°˜ì˜

### **í•œêµ­ì–´ ë¬¸ì²´ ì „ë¬¸ê°€**
- ìŒìŠ´ì²´ ì™„ë²½ êµ¬ì‚¬ ("~í•¨", "~ì„ ë³´ì„", "~í•˜ì˜€ìŒ")
- êµìœ¡ì  ê°€ì¹˜ê°€ ë“œëŸ¬ë‚˜ëŠ” ì„œìˆ  ê¸°ë²•
- êµ¬ì²´ì  ì‚¬ë¡€ ì¤‘ì‹¬ì˜ ìŠ¤í† ë¦¬í…”ë§
- ì„±ì¥ê³¼ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ëŠ” ì„œìˆ  êµ¬ì¡°

## ğŸ“š í•­ëª©ë³„ ì‘ì„± ì „ëµ
### **ğŸ“– êµê³¼ ì„¸ë¶€ëŠ¥ë ¥ ë° íŠ¹ê¸°ì‚¬í•­ (ì„¸íŠ¹)**
#### **ì‘ì„± ì›ì¹™**
- ìˆ˜í–‰í‰ê°€ ì¤‘ì‹¬ì˜ êµ¬ì²´ì  í™œë™ ê¸°ë¡
- êµê³¼ ì§€ì‹ì˜ ê¹Šì´ì™€ í™•ì¥ì„± ê°•ì¡°
- í•™ìŠµ ê³¼ì •ì—ì„œì˜ ì‚¬ê³ ë ¥ê³¼ ì°½ì˜ì„± ë¶€ê°
- í˜‘ì—… ëŠ¥ë ¥ê³¼ ì˜ì‚¬ì†Œí†µ ì—­ëŸ‰ ì¦ëª…

#### **ì°¨ë³„í™” ì „ëµ**
- ë‹¨ìˆœ ì°¸ì—¬ â†’ ì£¼ë„ì  íƒêµ¬ë¡œ ìŠ¹í™”
- êµê³¼ì„œ ë‚´ìš© â†’ ì‹¬í™” í™•ì¥ í•™ìŠµìœ¼ë¡œ ë°œì „
- ê°œë³„ í™œë™ â†’ íŒ€ì›Œí¬ì™€ ë¦¬ë”ì‹­ ë°œíœ˜
- ì¼íšŒì„± ê³¼ì œ â†’ ì§€ì†ì  ê´€ì‹¬ê³¼ íƒêµ¬ë¡œ ì—°ê²°

### **ğŸ­ ì°½ì˜ì  ì²´í—˜í™œë™**
#### **ììœ¨í™œë™**: í•™ê¸‰/í•™êµ ê³µë™ì²´ ê¸°ì—¬ì™€ ë¦¬ë”ì‹­
#### **ë™ì•„ë¦¬í™œë™**: ì „ê³µ ì—°ê³„ì„±ê³¼ ì‹¬í™” íƒêµ¬
#### **ë´‰ì‚¬í™œë™**: ë‚˜ëˆ”ì˜ ê°€ì¹˜ì™€ ì‚¬íšŒì  ì±…ì„ê°
#### **ì§„ë¡œí™œë™**: ì²´ê³„ì  ì§„ë¡œ íƒìƒ‰ê³¼ ì—­ëŸ‰ ê°œë°œ

### **ğŸ“‹ í–‰ë™íŠ¹ì„± ë° ì¢…í•©ì˜ê²¬**
- í•™ìŠµíƒœë„, êµìš°ê´€ê³„, ì¸ì„± ë“± ì¢…í•©ì  í‰ê°€
- êµ¬ì²´ì  í–‰ë™ ì‚¬ë¡€ë¥¼ í†µí•œ ì¸ì„± ì¦ëª…
- ì„±ì¥ ê³¼ì •ê³¼ ë³€í™” ëª¨ìŠµì˜ ì„œìˆ 
- ë¦¬ë”ì‹­ê³¼ í˜‘ì—… ëŠ¥ë ¥ì˜ ê· í˜•ì  ê¸°ë¡

### **ğŸ“š ë…ì„œí™œë™ìƒí™©**
- ì „ê³µ ì—°ê³„ ë„ì„œì™€ ê¹Šì´ ìˆëŠ” ì‚¬ê³ 
- ë…ì„œ í›„ íƒêµ¬ í™œë™ê³¼ ì‹¤ì²œ ì˜ì§€
- ë‹¤ì–‘í•œ ì¥ë¥´ì˜ ê· í˜• ì¡íŒ ë…ì„œ
- ë¹„íŒì  ì‚¬ê³ ì™€ ì°½ì˜ì  í•´ì„ ëŠ¥ë ¥

## ğŸš€ ê³ ê¸‰ ì‘ì„± ê¸°ë²•
### **STAR ê¸°ë²• í™œìš©**
- **Situation**: êµ¬ì²´ì  ìƒí™©ê³¼ ë§¥ë½ ì„¤ì •
- **Task**: ì£¼ì–´ì§„ ê³¼ì œì™€ ëª©í‘œ ëª…í™•í™”
- **Action**: ìˆ˜í–‰í•œ í–‰ë™ê³¼ ë…¸ë ¥ ê³¼ì •
- **Result**: ì–»ì€ ê²°ê³¼ì™€ ì„±ì¥ í¬ì¸íŠ¸

### **ìŠ¤í† ë¦¬í…”ë§ êµ¬ì¡°**
1. **ë„ì…**: í¥ë¯¸ë¡œìš´ ìƒí™© ì œì‹œ
2. **ì „ê°œ**: ê³¼ì •ì—ì„œì˜ ë…¸ë ¥ê³¼ ê³ ë¯¼
3. **ì ˆì •**: í•µì‹¬ ì—­ëŸ‰ ë°œíœ˜ ìˆœê°„
4. **ê²°ë§**: ì„±ì¥ê³¼ ê¹¨ë‹¬ìŒ, í›„ì† ê³„íš

### **ì°¨ë³„í™” í¬ì¸íŠ¸ ì°½ì¶œ**
- ë…íŠ¹í•œ ê´€ì ê³¼ ì ‘ê·¼ ë°©ì‹
- íƒ€ í•™ìƒê³¼ì˜ ëª…í™•í•œ êµ¬ë³„ì 
- ì „ê³µê³¼ì˜ ì°½ì˜ì  ì—°ê²°ê³ ë¦¬
- ì‚¬íšŒì  ê°€ì¹˜ì™€ ì˜ë¯¸ ë¶€ì—¬

## ğŸ¯ ì§„ë¡œ ì—°ê³„ ì „ëµ
### **ì „ê³µ ì í•©ì„± ê°•í™”**
- í¬ë§ ì „ê³µê³¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
- ê´€ë ¨ ë¶„ì•¼ì˜ ê¹Šì´ ìˆëŠ” íƒêµ¬
- ë¯¸ë˜ í•™ì—… ê³„íšê³¼ì˜ ì¼ê´€ì„±
- ì „ë¬¸ì„± ê°œë°œ ì˜ì§€ í‘œí˜„

### **ì„±ì¥ ìŠ¤í† ë¦¬ êµ¬ì¶•**
- 1í•™ë…„ â†’ 3í•™ë…„ ë°œì „ ê³¼ì •
- ê´€ì‹¬ì‚¬ì˜ í™•ì¥ê³¼ ì‹¬í™”
- ì—­ëŸ‰ì˜ ë‹¨ê³„ì  í–¥ìƒ
- ë¯¸ë˜ ë¹„ì „ê³¼ì˜ ì—°ê²°

## ğŸ“Š í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ
### **ìì²´ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] êµìœ¡ë¶€ ê¸°ì¬ìš”ë ¹ 100% ì¤€ìˆ˜
- [ ] ê¸€ì ìˆ˜/ë°”ì´íŠ¸ ìˆ˜ ì •í™•íˆ ë§ì¶¤
- [ ] ê¸ˆì§€ í‘œí˜„ ì™„ì „ ë°°ì œ
- [ ] êµ¬ì²´ì  ì‚¬ë¡€ì™€ ìˆ˜ì¹˜ í¬í•¨
- [ ] ì„±ì¥ê³¼ ë³€í™” ëª…í™•íˆ ë“œëŸ¬ë‚¨
- [ ] ì „ê³µ ì—°ê³„ì„± ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„
- [ ] ì°¨ë³„í™” í¬ì¸íŠ¸ ì¶©ë¶„íˆ ë¶€ê°
- [ ] ìŒìŠ´ì²´ ì¼ê´€ì„± ìœ ì§€

### **ê³ ê¸‰ ë¶„ì„ ë„êµ¬ í™œìš©**
#### **í‚¤ì›Œë“œ ë°€ë„ ë¶„ì„**
- ì „ê³µ ê´€ë ¨ í‚¤ì›Œë“œ ì ì ˆí•œ ë¶„í¬
- ì—­ëŸ‰ í‚¤ì›Œë“œì˜ ê· í˜• ì¡íŒ ë°°ì¹˜
- ì¤‘ë³µ í‘œí˜„ ìµœì†Œí™”

#### **ê°€ë…ì„± ìµœì í™”**
- ë¬¸ì¥ ê¸¸ì´ì™€ ë³µì¡ë„ ì¡°ì ˆ
- ì—°ê²°ì–´ì™€ ì „í™˜ì–´ì˜ íš¨ê³¼ì  ì‚¬ìš©
- ë‹¨ë½ë³„ ë‚´ìš©ì˜ ë…¼ë¦¬ì  íë¦„

### **ìµœì‹  íŠ¸ë Œë“œ ë°˜ì˜**
- 2025í•™ë…„ë„ ëŒ€ì… ë³€í™” ìš”ì†Œ
- ëŒ€í•™ë³„ ìµœì‹  ì„ ë°œ ê²½í–¥
- ì‚¬íšŒì  ì´ìŠˆì™€ ê°€ì¹˜ ë°˜ì˜
- ë¯¸ë˜ ì‚¬íšŒ ìš”êµ¬ ì—­ëŸ‰ ê°•ì¡°

## ğŸ’¡ í˜ì‹ ì  ì ‘ê·¼ë²•
### **ë°ì´í„° ê¸°ë°˜ ìµœì í™”**
- í•©ê²©ìƒ ìƒê¸°ë¶€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ í™œìš©
- ëŒ€í•™ë³„ ì„ í˜¸ í‘œí˜„ê³¼ í‚¤ì›Œë“œ ë°˜ì˜
- í†µê³„ì  ê²€ì¦ëœ íš¨ê³¼ì  ì„œìˆ  ë°©ì‹

### **AI ì‹œëŒ€ ë§ì¶¤ ì—­ëŸ‰**
- ì°½ì˜ì  ì‚¬ê³ ì™€ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
- í˜‘ì—…ê³¼ ì†Œí†µì˜ ì¤‘ìš”ì„± ê°•ì¡°
- ë””ì§€í„¸ ë¦¬í„°ëŸ¬ì‹œì™€ ì ì‘ë ¥
- ì¸ê°„ì  ê°€ì¹˜ì™€ ìœ¤ë¦¬ ì˜ì‹

ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ ìƒê¸°ë¶€ ì‘ì„± ë„êµ¬ê°€ ì•„ë‹Œ, í•™ìƒë“¤ì˜ ëŒ€í•™ ì§„í•™ê³¼ ë¯¸ë˜ë¥¼ ì±…ì„ì§€ëŠ” ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ëª¨ë“  ê¸°ë¡ì´ í•™ìƒì˜ ì§„ì •í•œ ì„±ì¥ì„ ë³´ì—¬ì£¼ê³ , ëŒ€í•™ì´ ì›í•˜ëŠ” ì¸ì¬ìƒê³¼ ë¶€í•©í•˜ë„ë¡ ìµœì„ ì„ ë‹¤í•´ ì§€ì›í•˜ì„¸ìš”."""

# í—ˆìš©ëœ ëª¨ë¸
ALLOWED_MODELS = ["gemini-2.5-pro", "gemini-2.5-flash"]
MULTIMODAL_MODELS = ["gemini-2.5-pro", "gemini-2.5-flash"]
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
GEMINI_INLINE_DATA_LIMIT = 10 * 1024 * 1024  # 10MB (Gemini API ì œí•œ)

# í”„ë¡œì íŠ¸ë³„ ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì¶”ê°€
PROJECT_THINKING_OPTIMIZATION = {
    "gemini-2.5-flash": {
        "default_budget": 2048,  # í”„ë¡œì íŠ¸ëŠ” ì¼ë°˜ ì±„íŒ…ë³´ë‹¤ ë” ë§ì€ ì‚¬ê³  í—ˆìš©
        "max_budget": 24576,
        "adaptive": True
    },
    "gemini-2.5-pro": {
        "default_budget": 8192,  # ProëŠ” ë” ë†’ì€ ì‚¬ê³  ì˜ˆì‚°
        "max_budget": 16384,
        "adaptive": True
    }
}

# í”„ë¡œì íŠ¸ë³„ ì±„íŒ… ì„¸ì…˜ ìºì‹œ
PROJECT_SESSION_CACHE = {}

# í”„ë¡œì íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì„¤ì •
PROJECT_STREAMING_BUFFER_SIZE = 2048  # í”„ë¡œì íŠ¸ëŠ” ë” í° ë²„í¼ ì‚¬ìš©
PROJECT_STREAMING_FLUSH_INTERVAL = 0.05  # ë” ë¹ ë¥¸ í”ŒëŸ¬ì‹œ

# í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
PROJECT_CONTEXT_COMPRESSION_THRESHOLD = 0.9  # í”„ë¡œì íŠ¸ëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
PROJECT_MAX_CONTEXT_TOKENS = 200000  # í”„ë¡œì íŠ¸ë³„ ë” í° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°

# ì„ë² ë”© ê²€ìƒ‰ ìµœì í™” ì„¤ì •
EMBEDDING_SEARCH_CACHE = {}
EMBEDDING_CACHE_TTL = 300  # 5ë¶„ ìºì‹œ

class ChatRequest(BaseModel):
    model: str
    messages: List[Dict[str, Any]]
    project_type: Optional[str] = None

class FileInfo(BaseModel):
    type: str
    name: str
    data: str

class ChatMessageCreate(BaseModel):
    content: str
    role: str
    files: Optional[List[FileInfo]] = None
    citations: Optional[List[Dict[str, str]]] = None
    reasoning_content: Optional[str] = None
    thought_time: Optional[float] = None
    room_id: Optional[str] = None

router = APIRouter()

async def process_file_to_base64(file: UploadFile) -> tuple[str, str]:
    try:
        contents = await file.read()
        
        # íŒŒì¼ì´ Gemini API ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
        if len(contents) > GEMINI_INLINE_DATA_LIMIT:
            # í° íŒŒì¼ì˜ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í¬ê¸° ì¶•ì†Œ
            if file.content_type.startswith("text/") or file.content_type == "application/json":
                # í…ìŠ¤íŠ¸ íŒŒì¼ì€ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                base64_data = base64.b64encode(contents).decode('utf-8')
            elif file.content_type == "application/pdf":
                # PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•„ìš”ì‹œ êµ¬í˜„)
                base64_data = base64.b64encode(contents).decode('utf-8')
            elif file.content_type.startswith("image/"):
                # ì´ë¯¸ì§€ íŒŒì¼ì€ ì••ì¶• ë˜ëŠ” í¬ê¸° ì¡°ì • (í•„ìš”ì‹œ êµ¬í˜„)
                base64_data = base64.b64encode(contents).decode('utf-8')
            else:
                # ë‹¤ë¥¸ íŒŒì¼ íƒ€ì…ì€ íŒŒì¼ëª…ê³¼ ë©”íƒ€ë°ì´í„°ë§Œ ì „ì†¡
                file_info = f"íŒŒì¼ëª…: {file.filename}, í¬ê¸°: {len(contents)} bytes, íƒ€ì…: {file.content_type}"
                base64_data = base64.b64encode(file_info.encode()).decode('utf-8')
        else:
            base64_data = base64.b64encode(contents).decode('utf-8')
        
        return base64_data, file.content_type
    except Exception as e:
        raise

async def validate_file(file: UploadFile) -> bool:
    """ì—…ë¡œë“œ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ (chat.pyì™€ ë™ì¼í•œ ë°©ì‹)"""
    if file.size and file.size > MAX_FILE_SIZE:
        return False
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¥ (chat.pyì™€ ë™ì¼)
    if file.content_type and file.content_type.startswith("image/"):
        return True
    elif file.content_type == "application/pdf":
        return True
    elif file.content_type in ["text/plain", "text/csv", "application/json"]:
        return True
    elif file.content_type and file.content_type.startswith("text/"):
        return True
    elif file.content_type in [
        'application/msword',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
    ]:
        return True
    
    return False

class ProjectResponse(BaseModel):
    id: str
    name: str
    type: str
    description: Optional[str] = None
    system_instruction: Optional[str] = None
    settings: Optional[Dict[str, Any]] = None
    chats: List[dict] = []

def get_gemini_client():
    """ìƒˆë¡œìš´ Gemini í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if not settings.GEMINI_API_KEY:
            return None
        
        # ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = genai.Client(api_key=settings.GEMINI_API_KEY)
        return client
    except Exception as e:
        logger.error(f"Gemini client creation error: {e}", exc_info=True)
        return None

def count_tokens_with_tiktoken(text: str, model: str = "gpt-4") -> dict:
    """tiktokenì„ ì‚¬ìš©í•œ ì •í™•í•œ í† í° ê³„ì‚°"""
    import tiktoken
    
    try:
        # Gemini ëª¨ë¸ì€ OpenAIì™€ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, 
        # tiktokenì˜ cl100k_baseëŠ” ë¹„êµì  ì •í™•í•œ ì¶”ì •ì„ ì œê³µ
        if "gemini" in model.lower():
            # Geminiìš©ìœ¼ë¡œ cl100k_base ì‚¬ìš© (GPT-4ì™€ ìœ ì‚¬í•œ í† í°í™”)
            encoding_name = "cl100k_base"
        else:
            # ê¸°íƒ€ ëª¨ë¸ìš© ê¸°ë³¸ê°’
            encoding_name = "cl100k_base"
            
        encoding = tiktoken.get_encoding(encoding_name)
        token_count = len(encoding.encode(text))
        
        return {
            "input_tokens": token_count,
            "output_tokens": 0,
            "method": "tiktoken",
            "encoding": encoding_name
        }
    except Exception as e:
        logger.warning(f"tiktoken calculation failed: {e}, using fallback")
        return fallback_token_calculation(text)

def fallback_token_calculation(text: str) -> dict:
    """tiktoken ì‹¤íŒ¨ ì‹œ fallback ê³„ì‚°"""
    import re
    
    # í•œêµ­ì–´/ì˜ì–´ í˜¼í•© í…ìŠ¤íŠ¸ ì •í™•í•œ ì¶”ì •
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    numbers_symbols = len(re.findall(r'[0-9\s\.,;:!?\-\(\)\[\]{}]', text))
    other_chars = len(text) - korean_chars - english_chars - numbers_symbols
    
    # í•œêµ­ì–´ 1.3ì/í† í°, ì˜ì–´ 3.5ì/í† í° (tiktoken ê¸°ì¤€ ì¡°ì •)
    estimated_tokens = (
        korean_chars / 1.3 + 
        english_chars / 3.5 + 
        numbers_symbols / 2.5 + 
        other_chars / 2
    )
    
    return {
        "input_tokens": max(1, int(estimated_tokens)),
        "output_tokens": 0,
        "method": "fallback",
        "encoding": "estimated"
    }

# ìµœì‹  Gemini ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í•œë„ ì„¤ì • (chat.pyì™€ ë™ì¼)
MODEL_CONTEXT_LIMITS = {
    "gemini-2.5-pro": {
        "total_tokens": 2_000_000,  # 2M í† í°
        "output_reserve": 4096,     # ì¶œë ¥ìš© ì˜ˆì•½
        "system_reserve": 2048,     # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš© ì˜ˆì•½
        "file_reserve": 10000       # íŒŒì¼ìš© ì˜ˆì•½
    },
    "gemini-2.5-flash": {
        "total_tokens": 1_000_000,  # 1M í† í°
        "output_reserve": 2048,     # ì¶œë ¥ìš© ì˜ˆì•½
        "system_reserve": 1024,     # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš© ì˜ˆì•½
        "file_reserve": 5000        # íŒŒì¼ìš© ì˜ˆì•½
    },
    "gemini-2.0-flash": {
        "total_tokens": 1_000_000,
        "output_reserve": 2048,
        "system_reserve": 1024,
        "file_reserve": 5000
    },
    "gemini-1.5-pro": {
        "total_tokens": 2_000_000,
        "output_reserve": 4096,
        "system_reserve": 2048,
        "file_reserve": 10000
    },
    "gemini-1.5-flash": {
        "total_tokens": 1_000_000,
        "output_reserve": 2048,
        "system_reserve": 1024,
        "file_reserve": 5000
    }
}

def get_dynamic_context_limit(model: str, system_tokens: int = 0, file_tokens: int = 0) -> int:
    """ëª¨ë¸ë³„ ë™ì  ì»¨í…ìŠ¤íŠ¸ í•œë„ ê³„ì‚° (ìµœì‹  API ê¸°ì¤€)"""
    # ê¸°ë³¸ê°’ ì„¤ì • (í˜¸í™˜ì„±ì„ ìœ„í•´)
    default_config = {
        "total_tokens": 1_000_000,
        "output_reserve": 2048,
        "system_reserve": 1024,
        "file_reserve": 5000
    }
    
    config = MODEL_CONTEXT_LIMITS.get(model, default_config)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ ê³„ì‚°
    available_tokens = (
        config["total_tokens"] 
        - config["output_reserve"] 
        - system_tokens 
        - file_tokens
    )
    
    # ìµœì†Œ í•œë„ ë³´ì¥ (ë„ˆë¬´ ì‘ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    min_context = 10000
    if available_tokens < min_context:
        logger.warning(f"Calculated context too small ({available_tokens}), using minimum: {min_context}")
        return min_context
    
    logger.info(f"Dynamic context limit for {model}: {available_tokens} tokens")
    return available_tokens

async def get_optimized_project_thinking_config(
    model: str, 
    project_type: str = "general",
    complexity_level: str = "normal"
) -> Optional[types.ThinkingConfig]:
    """í”„ë¡œì íŠ¸ë³„ ìµœì í™”ëœ ì‚¬ê³  ì„¤ì • ìƒì„±"""
    if model not in PROJECT_THINKING_OPTIMIZATION:
        return None
    
    config = PROJECT_THINKING_OPTIMIZATION[model]
    
    # ë³µì¡ë„ ë ˆë²¨ ê¸°ë°˜ ì¡°ì •
    if complexity_level == "simple":
        budget = config["default_budget"] // 2
    elif complexity_level == "complex":
        budget = config["max_budget"]
    else:
        budget = config["default_budget"]
    
    # í”„ë¡œì íŠ¸ íƒ€ì…ë³„ ì¶”ê°€ ì¡°ì •
    if project_type == "assignment":
        # ìˆ˜í–‰í‰ê°€ëŠ” ë” ë§ì€ ì‚¬ê³  ì˜ˆì‚° í•„ìš”
        budget = min(budget * 2, config["max_budget"])
    elif project_type == "record":
        # ìƒê¸°ë¶€ ì‘ì„±ì€ ì¤‘ê°„ ì •ë„ì˜ ì‚¬ê³  ì˜ˆì‚°
        budget = min(int(budget * 1.5), config["max_budget"])
    
    return types.ThinkingConfig(
        thinking_budget=budget,
        include_thoughts=budget > 0
    )

async def compress_project_context_if_needed(
    client,
    model: str,
    messages: List[dict],
    max_tokens: int,
    project_type: Optional[str] = None
) -> List[dict]:
    """í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (í•„ìš”í•œ ê²½ìš°)"""
    # í† í° ìˆ˜ ê³„ì‚°
    total_tokens = 0
    for msg in messages:
        token_count = count_tokens_with_tiktoken(msg["content"], model)
        total_tokens += token_count.get("input_tokens", 0)
    
    # ì••ì¶•ì´ í•„ìš”í•œì§€ í™•ì¸
    if total_tokens < max_tokens * PROJECT_CONTEXT_COMPRESSION_THRESHOLD:
        return messages
    
    logger.info(f"Project context compression needed: {total_tokens} tokens > {max_tokens * PROJECT_CONTEXT_COMPRESSION_THRESHOLD}")
    
    # ìµœì‹  ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    keep_recent = 5  # í”„ë¡œì íŠ¸ëŠ” ë” ë§ì€ ìµœê·¼ ë©”ì‹œì§€ ìœ ì§€
    recent_messages = messages[-keep_recent:]
    old_messages = messages[:-keep_recent]
    
    if not old_messages:
        return recent_messages
    
    # ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ ìš”ì•½
    try:
        summary_content = "\n".join([
            f"{msg['role']}: {msg['content']}" 
            for msg in old_messages
        ])
        
        summary_response = client.models.generate_content(
            model="gemini-2.5-flash",  # ìš”ì•½ì€ ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            contents=[f"ë‹¤ìŒ í”„ë¡œì íŠ¸ ëŒ€í™”ë¥¼ í•µì‹¬ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{summary_content}"],
            config=types.GenerateContentConfig(
                temperature=0.3,
                max_output_tokens=1024,  # í”„ë¡œì íŠ¸ëŠ” ë” ê¸´ ìš”ì•½ í—ˆìš©
                thinking_config=types.ThinkingConfig(thinking_budget=0)  # ìš”ì•½ì€ ì‚¬ê³  ì—†ì´
            )
        )
        
        # ìš”ì•½ëœ ë©”ì‹œì§€ë¡œ êµì²´
        compressed_messages = [
            {"role": "system", "content": f"ì´ì „ í”„ë¡œì íŠ¸ ëŒ€í™” ìš”ì•½: {summary_response.text}"}
        ] + recent_messages
        
        logger.info(f"Project context compressed: {len(messages)} -> {len(compressed_messages)} messages")
        return compressed_messages
        
    except Exception as e:
        logger.error(f"Project context compression error: {e}", exc_info=True)
        return messages[-keep_recent:]  # ì‹¤íŒ¨ì‹œ ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€

class ProjectStreamingBuffer:
    """í”„ë¡œì íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë²„í¼ë§"""
    def __init__(self, buffer_size: int = PROJECT_STREAMING_BUFFER_SIZE):
        self.buffer = []
        self.buffer_size = buffer_size
        self.current_size = 0
        self.last_flush = time.time()
    
    def add_chunk(self, chunk: str) -> bool:
        """ì²­í¬ ì¶”ê°€, í”ŒëŸ¬ì‹œ í•„ìš”ì‹œ True ë°˜í™˜"""
        self.buffer.append(chunk)
        self.current_size += len(chunk.encode('utf-8'))
        
        # ë²„í¼ê°€ ê°€ë“ ì°¼ê±°ë‚˜ ì¼ì • ì‹œê°„ì´ ì§€ë‚œ ê²½ìš° í”ŒëŸ¬ì‹œ
        now = time.time()
        return (self.current_size >= self.buffer_size or 
                now - self.last_flush >= PROJECT_STREAMING_FLUSH_INTERVAL)
    
    def flush(self) -> str:
        """ë²„í¼ ë‚´ìš© ë°˜í™˜ ë° ì´ˆê¸°í™”"""
        if not self.buffer:
            return ""
        
        content = "".join(self.buffer)
        self.buffer.clear()
        self.current_size = 0
        self.last_flush = time.time()
        return content

async def generate_gemini_stream_response(
    request: Request,
    messages: list,
    model: str,
    room_id: str,
    db: Session,
    user_id: str,
    project_id: str,
    project_type: Optional[str] = None,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    file_names: Optional[List[str]] = None
) -> AsyncGenerator[str, None]:
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")

        # í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project = crud_project.get(db=db, id=project_id)
        
        # í”„ë¡œì íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = BRIEF_SYSTEM_PROMPT
        if project_type == "assignment":
            system_prompt += "\n\n" + ASSIGNMENT_PROMPT
        elif project_type == "record":
            system_prompt += "\n\n" + RECORD_PROMPT
            
        # í”„ë¡œì íŠ¸ ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        if project and project.system_instruction and project.system_instruction.strip():
            system_prompt += "\n\n## ì¶”ê°€ ì§€ì‹œì‚¬í•­\n" + project.system_instruction.strip()

        # ğŸ” ì„ë² ë”© ê²€ìƒ‰ ìë™ ì‹¤í–‰ (ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜)
        embedding_context = ""
        if messages and len(messages) > 0:
            last_user_message = messages[-1]  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€
            if last_user_message.get("role") == "user" and last_user_message.get("content"):
                user_query = last_user_message["content"]
                try:
                    # ì„ë² ë”© ê²€ìƒ‰ ìˆ˜í–‰
                    query_embed_result = client.models.embed_content(
                        model="text-embedding-004",
                        contents=user_query,
                        config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY")
                    )
                    
                    if query_embed_result.embeddings:
                        query_embedding = (
                            query_embed_result.embeddings[0].values 
                            if hasattr(query_embed_result.embeddings[0], 'values') 
                            else list(query_embed_result.embeddings[0])
                        )
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ì„ë² ë”© ê²€ìƒ‰ (ì„ê³„ê°’ ë‚®ì¶¤)
                        similar_embeddings = crud_embedding.search_similar(
                            db=db,
                            project_id=project_id,
                            query_embedding=query_embedding,
                            top_k=5,
                            threshold=0.4  # ì„ê³„ê°’ì„ 0.75ì—ì„œ 0.4ë¡œ ë‚®ì¶¤
                        )
                        
                        # ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        if similar_embeddings:
                            relevant_chunks = []
                            for result in similar_embeddings:
                                relevant_chunks.append(f"[{result['file_name']}] {result['content'][:200]}...")  # 200ìë§Œ í‘œì‹œ
                            
                            embedding_context = f"""
                            
## ğŸ“š ê´€ë ¨ ìë£Œ (ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ê²€ìƒ‰)
{chr(10).join(relevant_chunks)}

ìœ„ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
                            logger.info(f"ğŸ” ì„ë² ë”© ê²€ìƒ‰ ì„±ê³µ: {len(similar_embeddings)}ê°œ ì²­í¬ ë°œê²¬")
                            for i, result in enumerate(similar_embeddings):
                                logger.info(f"  [{i+1}] ìœ ì‚¬ë„: {result['similarity']:.3f}, íŒŒì¼: {result['file_name']}, ë‚´ìš©: {result['content'][:50]}...")
                        else:
                            # ì „ì²´ ì„ë² ë”© ê°œìˆ˜ í™•ì¸
                            all_embeddings = crud_embedding.get_by_project(db, project_id)
                            logger.debug(f"   ì „ì²´ ì„ë² ë”© ê°œìˆ˜: {len(all_embeddings)}")
                            if all_embeddings:
                                logger.debug(f"   íŒŒì¼ ëª©ë¡: {list(set(e.file_name for e in all_embeddings))}")
                            logger.debug(f"   ì‚¬ìš©ì ì§ˆë¬¸: '{user_query}'")
                            logger.debug(f"   ì„ê³„ê°’: 0.4")
                    else:
                        logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                        
                except Exception as e:
                    logger.error(f"ì„ë² ë”© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                    
        # ì„ë² ë”© ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        if embedding_context:
            system_prompt += embedding_context

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        if not messages or len(messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="At least one message is required"
            )

        # í† í° ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ (chat.py ë°©ì‹ ì ìš©)
        # í”„ë¡œì íŠ¸ëŠ” ë” í° ì»¨í…ìŠ¤íŠ¸ í—ˆìš© (ì¼ë°˜ ì±„íŒ…ë³´ë‹¤ 2ë°°)
        # í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ - ë™ì  í•œë„ ê³„ì‚°
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í° ë¨¼ì € ê³„ì‚°
        system_tokens = 0
        if system_prompt:
            system_token_info = count_tokens_with_tiktoken(system_prompt, model)
            system_tokens = system_token_info.get("input_tokens", 0)
        
        # íŒŒì¼ í† í° ê³„ì‚° (ìµœì‹  API ê¸°ì¤€)
        file_tokens = 0
        if file_data_list and file_types:
            for file_type in file_types:
                if file_type.startswith("image/"):
                    file_tokens += 258  # Gemini 2.5 ê¸°ì¤€: ì´ë¯¸ì§€ë‹¹ 258 í† í°
                elif file_type == "application/pdf":
                    file_tokens += 258 * 10  # ì˜ˆìƒ í˜ì´ì§€ ìˆ˜ * 258 í† í°
                elif file_type.startswith("video/"):
                    file_tokens += 263 * 60  # ì˜ˆìƒ 1ë¶„ * 263 í† í°/ì´ˆ
                elif file_type.startswith("audio/"):
                    file_tokens += 32 * 60   # ì˜ˆìƒ 1ë¶„ * 32 í† í°/ì´ˆ
        
        # ë™ì  ì»¨í…ìŠ¤íŠ¸ í•œë„ ê³„ì‚° (ìµœì‹  API ê¸°ì¤€)
        MAX_CONTEXT_TOKENS = get_dynamic_context_limit(model, system_tokens, file_tokens)
        
        # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœê·¼ ë©”ì‹œì§€ë¶€í„° í¬í•¨
        valid_messages = []
        total_tokens = system_tokens + file_tokens  # ì´ë¯¸ ê³„ì‚°ëœ ì‹œìŠ¤í…œ + íŒŒì¼ í† í° ì‚¬ìš©
        
        # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ ê²€í† í•˜ë©´ì„œ í† í° ì˜ˆì‚° ë‚´ì—ì„œ í¬í•¨
        for i in range(len(messages) - 1, -1, -1):
            msg = messages[i]
            if msg.get("content") and msg["content"].strip():
                # ë©”ì‹œì§€ í† í° ê³„ì‚° (ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©)
                msg_tokens = count_tokens_with_tiktoken(
                    f"{msg['role']}: {msg['content']}", 
                    model
                )
                msg_token_count = msg_tokens.get("input_tokens", 0)
                
                # í† í° ì˜ˆì‚° í™•ì¸
                if total_tokens + msg_token_count <= MAX_CONTEXT_TOKENS:
                    valid_messages.insert(0, msg)
                    total_tokens += msg_token_count
                else:
                    # í† í° í•œê³„ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                    logger.info(f"Project context window limit reached. Including {len(valid_messages)} messages out of {len(messages)}")
                    break
        
        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ë©”ì‹œì§€ëŠ” í¬í•¨ë˜ì–´ì•¼ í•¨
        if len(valid_messages) == 0 and len(messages) > 0:
            last_msg = messages[-1]
            if last_msg.get("content") and last_msg["content"].strip():
                valid_messages = [last_msg]
        
        if len(valid_messages) == 0:
            raise HTTPException(
                status_code=400,
                detail="No valid message content found"
            )
        
        logger.info(f"Project context management: Using {len(valid_messages)} messages with {total_tokens} tokens")

        # ì»¨í…ìŠ¤íŠ¸ ìºì‹± ì„ì‹œ ë¹„í™œì„±í™” (API ì œì•½ì‚¬í•­ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ë°©ì§€)
        cached_content_name = None
        # if system_prompt:
        #     # í”„ë¡œì íŠ¸ë³„ ìºì‹œ ì´ë¦„ ìƒì„±
        #     prompt_hash = hashlib.md5(system_prompt.encode()).hexdigest()[:8]
        #     cache_name = f"project_{project_id}_{model}_{prompt_hash}"
        #     
        #     # ìºì‹œ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        #     cached_content_name = await get_or_create_project_context_cache(
        #         client=client,
        #         project=project,
        #         model=model,
        #         cache_name=cache_name,
        #         ttl=7200  # 2ì‹œê°„ ìºì‹±
        #     )

        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì ìš© (í•„ìš”í•œ ê²½ìš°)
        if len(valid_messages) > 10:  # 10ê°œ ì´ìƒ ë©”ì‹œì§€ê°€ ìˆì„ ë•Œë§Œ ì••ì¶• ê³ ë ¤
            valid_messages = await compress_project_context_if_needed(
                client=client,
                model=model,
                messages=valid_messages,
                max_tokens=MAX_CONTEXT_TOKENS,
                project_type=project_type or "general"
            )

        # ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # í”„ë¡œì íŠ¸ íŒŒì¼ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
        try:
            project_files_list = []
            for file in client.files.list():
                if file.display_name and file.display_name.startswith(f"project_{project_id}_"):
                    if file.state.name == "ACTIVE":
                        project_files_list.append(file)
            
            # ìµœëŒ€ 3ê°œ íŒŒì¼ë§Œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
            for file in project_files_list[:3]:
                contents.append(file)
        except Exception as e:
            logger.error(f"Failed to load project files for context: {e}", exc_info=True)
        
        # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬
        if file_data_list and file_types and file_names:
            for file_data, file_type, file_name in zip(file_data_list, file_types, file_names):
                if file_type.startswith("image/"):
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )
                elif file_type == "application/pdf":
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )

        # ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation_text = ""
        for message in valid_messages:
            role_text = "Human" if message["role"] == "user" else "Assistant"
            conversation_text += f"{role_text}: {message['content']}\n"

        contents.append(conversation_text)

        # í”„ë¡œì íŠ¸ë³„ ë„êµ¬ ì„¤ì • - ê²€ìƒ‰ê³¼ ì½”ë“œ ì‹¤í–‰ë§Œ ì‚¬ìš©
        tools = []
        
        # í•­ìƒ Google ê²€ìƒ‰ ê·¸ë¼ìš´ë”© ì¶”ê°€
        tools.append(types.Tool(google_search=types.GoogleSearch()))
        
        # ì½”ë“œ ì‹¤í–‰ ì¶”ê°€ (ë°ì´í„° ë¶„ì„ìš©)
        tools.append(types.Tool(code_execution=types.ToolCodeExecution()))

        # ìƒì„± ì„¤ì • (ìºì‹œ ë¹„í™œì„±í™”ë¡œ ì¸í•œ ê°„ì†Œí™”)
        generation_config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            temperature=0.7,
            top_p=0.95,
            max_output_tokens=8192,
            tools=tools
        )

        # ìµœì í™”ëœ ì‚¬ê³  ê¸°ëŠ¥ ì„¤ì • (í”„ë¡œì íŠ¸ë³„ ì ì‘í˜•)
        optimized_thinking_config = await get_optimized_project_thinking_config(
            model=model,
            project_type=project_type or "general",
            complexity_level="complex" if len(valid_messages) > 5 else "normal"
        )
        if optimized_thinking_config:
            generation_config.thinking_config = optimized_thinking_config
        else:
            # í´ë°±: ê¸°ë³¸ ì‚¬ê³  ì„¤ì •
            thinking_budget = 16384 if model.endswith("2.5-pro") else 12288
            if model.endswith("2.5-pro"):
                generation_config.thinking_config = types.ThinkingConfig(
                    thinking_budget=thinking_budget,
                    include_thoughts=True
                )
            elif model.endswith("2.5-flash"):
                generation_config.thinking_config = types.ThinkingConfig(
                    thinking_budget=min(thinking_budget, 24576),
                    include_thoughts=True
                )

        # í”„ë¡œì íŠ¸ë³„ ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ì´ˆê¸°í™”
        content_buffer = ProjectStreamingBuffer(PROJECT_STREAMING_BUFFER_SIZE)
        thinking_buffer = ProjectStreamingBuffer(PROJECT_STREAMING_BUFFER_SIZE // 2)

        # ì…ë ¥ í† í° ê³„ì‚°
        input_token_count = count_tokens_with_tiktoken(conversation_text, model)
        input_tokens = input_token_count.get("input_tokens", 0)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        accumulated_content = ""
        accumulated_thinking = ""
        thought_time = 0.0
        citations = []
        web_search_queries = []
        streaming_completed = False  # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì—¬ë¶€ ì²´í¬
        is_disconnected = False  # ì—°ê²° ì¤‘ë‹¨ í”Œë˜ê·¸ ì¶”ê°€
        citations_sent = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ set

        try:
            response = client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=generation_config
            )

            start_time = time.time()
            
            for chunk in response:
                # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
                if await request.is_disconnected():
                    is_disconnected = True
                    logger.warning("Project client disconnected. Stopping stream.")
                    break
                    
                if chunk.candidates and len(chunk.candidates) > 0:
                    candidate = chunk.candidates[0]
                    
                    # ì½˜í…ì¸  íŒŒíŠ¸ ì²˜ë¦¬
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            # ì‚¬ê³  ë‚´ìš©ê³¼ ì¼ë°˜ ì½˜í…ì¸  ëª…í™•íˆ ë¶„ë¦¬ (chat.py ë°©ì‹)
                            if hasattr(part, 'thought') and part.thought:
                                # ì‚¬ê³  ë‚´ìš©ë§Œ ì²˜ë¦¬
                                if part.text:
                                    accumulated_thinking += part.text
                                    thought_time = time.time() - start_time
                                    try:
                                        yield f"data: {json.dumps({'reasoning_content': part.text, 'thought_time': thought_time})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        logger.warning("Project client disconnected during reasoning streaming")
                                        return
                            elif part.text:
                                # ì¼ë°˜ ì‘ë‹µ ë‚´ìš©ë§Œ ì²˜ë¦¬
                                accumulated_content += part.text
                                try:
                                    yield f"data: {json.dumps({'content': part.text})}\n\n"
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    logger.warning("Project client disconnected during content streaming")
                                    return

                    # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (ìµœì‹  API êµ¬ì¡°)
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding = candidate.grounding_metadata
                        
                        # ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜ì§‘
                        if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                            web_search_queries.extend(grounding.web_search_queries)
                        
                        # grounding chunksì—ì„œ citations ì¶”ì¶œ
                        if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                            new_citations = []
                            for chunk_info in grounding.grounding_chunks:
                                if hasattr(chunk_info, 'web') and chunk_info.web:
                                    citation_url = chunk_info.web.uri
                                    # ì¤‘ë³µ ë°©ì§€
                                    if citation_url not in citations_sent:
                                        citation = {
                                            "url": citation_url,
                                            "title": chunk_info.web.title if hasattr(chunk_info.web, 'title') else ""
                                        }
                                        citations.append(citation)
                                        new_citations.append(citation)
                                        citations_sent.add(citation_url)
                            
                            # ìƒˆë¡œìš´ ì¸ìš© ì •ë³´ë§Œ ì „ì†¡
                            if new_citations:
                                try:
                                    yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    logger.warning("Project client disconnected during citations streaming")
                                    return

            # ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if is_disconnected:
                logger.info("Skipping project post-processing due to client disconnection.")
                return
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë¨
            streaming_completed = True
            
            # ìµœì¢… ë©”íƒ€ë°ì´í„° ì „ì†¡
            if web_search_queries:
                try:
                    yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    logger.warning("Project client disconnected during final search queries streaming")
                    return

            # ì¶œë ¥ í† í° ê³„ì‚°
            output_token_count = count_tokens_with_tiktoken(accumulated_content, model)
            output_tokens = output_token_count.get("input_tokens", 0)
            
            # ì‚¬ê³  í† í° ê³„ì‚°
            thinking_tokens = 0
            if accumulated_thinking:
                thinking_token_count = count_tokens_with_tiktoken(accumulated_thinking, model)
                thinking_tokens = thinking_token_count.get("input_tokens", 0)

            # í† í° ì‚¬ìš©ëŸ‰ ì €ì¥ (KST ì‹œê°„ìœ¼ë¡œ ì €ì¥)
            from pytz import timezone
            kst = timezone('Asia/Seoul')
            crud_stats.create_token_usage(
                db=db,
                user_id=user_id,
                room_id=room_id,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens + thinking_tokens,
                timestamp=datetime.now(kst),
                chat_type=f"project_{project_type}" if project_type else None
            )

        except (ConnectionError, BrokenPipeError, GeneratorExit):
            streaming_completed = False  # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            logger.warning("Project client disconnected during main streaming loop")
            return
        except Exception as api_error:
            streaming_completed = False  # ì—ëŸ¬ ë°œìƒ ì‹œ ì™„ë£Œë˜ì§€ ì•ŠìŒ
            error_message = f"Gemini API Error: {str(api_error)}"
            try:
                yield f"data: {json.dumps({'error': error_message})}\n\n"
            except (ConnectionError, BrokenPipeError, GeneratorExit):
                logger.warning("Project client disconnected during error streaming")
                return
        
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥
        if streaming_completed and accumulated_content:
            logger.debug("=== PROJECT SAVING MESSAGE DEBUG ===")
            logger.debug(f"streaming_completed: {streaming_completed}")
            logger.debug(f"accumulated_content length: {len(accumulated_content)}")
            logger.debug(f"citations count: {len(citations)}")
            message_create = ChatMessageCreate(
                content=accumulated_content,
                role="assistant",
                reasoning_content=accumulated_thinking if accumulated_thinking else None,
                thought_time=thought_time if thought_time > 0 else None,
                citations=citations if citations else None
            )
            crud_project.create_chat_message(db, project_id=project_id, chat_id=room_id, obj_in=message_create)
            logger.info("Project message saved successfully")
            logger.debug("=== END PROJECT SAVING DEBUG ===")
        else:
            logger.info("=== PROJECT MESSAGE NOT SAVED ===")
            logger.info(f"streaming_completed: {streaming_completed}")
            logger.info(f"accumulated_content: {bool(accumulated_content)}")
            logger.info(f"Reason: {'Streaming was interrupted' if not streaming_completed else 'No content'}")
            logger.info("=== END PROJECT NOT SAVED DEBUG ===")

    except Exception as e:
        error_message = f"Project Stream Generation Error: {str(e)}"
        try:
            yield f"data: {json.dumps({'error': error_message})}\n\n"
        except (ConnectionError, BrokenPipeError, GeneratorExit):
            logger.warning("Project client disconnected during final error streaming")
            return

@router.post("/{project_id}/chats/{chat_id}/chat")
async def stream_project_chat(
    *,
    request: Request,
    db: Session = Depends(deps.get_db),
    project_id: str,
    chat_id: str,
    request_data: str = Form(...),
    files: List[UploadFile] = File([]),
    current_user: User = Depends(deps.get_current_user)
) -> Any:
    try:
        # JSON íŒŒì‹±
        try:
            parsed_data = json.loads(request_data)
            chat_request = ChatRequest(**parsed_data)
        except (json.JSONDecodeError, ValueError) as e:
            raise HTTPException(status_code=400, detail=f"Invalid JSON format: {str(e)}")
        
        # ëª¨ë¸ ê²€ì¦
        if chat_request.model not in ALLOWED_MODELS:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid model specified. Allowed models: {ALLOWED_MODELS}"
            )

        # í”„ë¡œì íŠ¸ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        if project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ì±„íŒ…ë°© ì¡´ì¬ í™•ì¸ ë° ìë™ ìƒì„±
        chat = crud_project.get_chat(db=db, project_id=project_id, chat_id=chat_id)
        if not chat:
            # ì±„íŒ…ë°©ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±
            from app.schemas.project import ProjectChatCreate
            chat_data = ProjectChatCreate(
                id=chat_id,
                name="ìƒˆ ì±„íŒ…",  # ì¼ë°˜ ì±„íŒ…ê³¼ ë™ì¼í•˜ê²Œ "ìƒˆ ì±„íŒ…"ìœ¼ë¡œ ì‹œì‘
                type=project.type
            )
            try:
                chat = crud_project.create_chat(
                    db=db, 
                    project_id=project_id, 
                    obj_in=chat_data, 
                    owner_id=current_user.id,
                    chat_id=chat_id  # URLì—ì„œ ë°›ì€ chat_id ì‚¬ìš©
                )
                # ì±„íŒ…ë°© ìƒì„± í›„ ëª…ì‹œì ìœ¼ë¡œ ì»¤ë°‹
                db.commit()
                db.refresh(chat)
            except Exception as e:
                db.rollback()
                raise HTTPException(
                    status_code=500, 
                    detail=f"Failed to create chat room: {str(e)}"
                )

        # íŒŒì¼ ì²˜ë¦¬
        file_data_list = []
        file_types = []
        file_names = []
        file_info_list = []
        if files and files[0].filename:
            if chat_request.model not in MULTIMODAL_MODELS:
                raise HTTPException(
                    status_code=400,
                    detail="File upload is not supported for this model"
                )
            
            for file in files:
                await validate_file(file)
                file_data, file_type = await process_file_to_base64(file)
                file_data_list.append(file_data)
                file_types.append(file_type)
                file_names.append(file.filename)
                file_info_list.append({
                    "type": file_type,
                    "name": file.filename,
                    "data": file_data
                })

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ (chat.py ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ)
        if chat_request.messages:
            last_message = chat_request.messages[-1]
            user_message = ChatMessageCreate(
                content=last_message["content"],
                role="user",
                room_id=chat_id,  # chat.pyì™€ ë™ì¼í•˜ê²Œ room_id ì‚¬ìš©
                files=[{
                    "type": file_type,
                    "name": file_name,
                    "data": file_data
                } for file_data, file_type, file_name in zip(file_data_list, file_types, file_names)] if file_data_list else None
            )
            # í”„ë¡œì íŠ¸ ì±„íŒ… ë©”ì‹œì§€ ì €ì¥
            crud_project.create_chat_message(db, project_id=project_id, chat_id=chat_id, obj_in=user_message)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        formatted_messages = [
            {"role": msg["role"], "content": msg["content"]} 
            for msg in chat_request.messages
        ]
        
        return StreamingResponse(
            generate_gemini_stream_response(
                request=request,
                messages=formatted_messages,
                model=chat_request.model,
                room_id=chat_id,
                db=db,
                user_id=current_user.id,
                project_id=project_id,
                project_type=project.type,
                file_data_list=file_data_list,
                file_types=file_types,
                file_names=[f.filename for f in files] if files else None
            ),
            media_type="text/event-stream"
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process chat: {str(e)}"
        )

@router.delete("/{project_id}/chats/{chat_id}")
def delete_project_chat(
    *,
    db: Session = Depends(deps.get_db),
    project_id: str,
    chat_id: str,
) -> Any:
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    
    chat = crud_project.get_chat(db=db, project_id=project_id, chat_id=chat_id)
    if not chat:
        raise HTTPException(status_code=404, detail="Chat not found")
    
    db.delete(chat)
    db.commit()
    
    return {"message": "Chat deleted successfully"}

# í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ
@router.get("/", response_model=List[Dict[str, Any]])
def get_projects(
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """ì‚¬ìš©ìì˜ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    projects = crud_project.get_multi_by_user(db=db, user_id=current_user.id)
    return projects

# íŠ¹ì • í”„ë¡œì íŠ¸ ì¡°íšŒ
@router.get("/{project_id}")
def get_project(
    project_id: str,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """íŠ¹ì • í”„ë¡œì íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    return project.to_dict(include_chats=True)

# í”„ë¡œì íŠ¸ ìƒì„±
@router.post("/")
def create_project(
    project: ProjectCreate,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    created_project = crud_project.create(db=db, obj_in=project, user_id=current_user.id)
    return created_project.to_dict(include_chats=True)

# í”„ë¡œì íŠ¸ ìˆ˜ì •
@router.patch("/{project_id}")
def update_project(
    project_id: str,
    project: ProjectUpdate,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    db_project = crud_project.get(db=db, id=project_id)
    if not db_project:
        raise HTTPException(status_code=404, detail="Project not found")
    if db_project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    updated_project = crud_project.update(db=db, db_obj=db_project, obj_in=project)
    return updated_project.to_dict(include_chats=True)

# í”„ë¡¬í”„íŠ¸ ê°œì„  API
@router.post("/{project_id}/improve-prompt")
async def improve_prompt(
    project_id: str,
    original_prompt: str = Form(...),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ AIê°€ ê°œì„ í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # í”„ë¡œì íŠ¸ ì¡´ì¬ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        if project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")

        # Gemini 2.0 Flash-Lite í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = genai.Client(api_key=settings.GEMINI_API_KEY)

        # í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        improvement_prompt = """ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•˜ê³ , ë” ëª…í™•í•˜ê³  íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.

## ê°œì„  ì›ì¹™:
1. **ëª…í™•ì„±**: ëª¨í˜¸í•œ í‘œí˜„ì„ êµ¬ì²´ì ìœ¼ë¡œ ê°œì„ 
2. **êµ¬ì¡°í™”**: ìš”ì²­ì‚¬í•­ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì •ë¦¬
3. **ë§¥ë½ ì œê³µ**: í•„ìš”í•œ ë°°ê²½ ì •ë³´ ì¶”ê°€
4. **êµ¬ì²´ì„±**: ì¶”ìƒì  ìš”ì²­ì„ êµ¬ì²´ì ìœ¼ë¡œ ë³€í™˜
5. **ì‹¤í–‰ ê°€ëŠ¥ì„±**: AIê°€ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì¡°ì •

## ê°œì„  ë°©ë²•:
- í•µì‹¬ ì˜ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ í‘œí˜„ ë°©ì‹ ê°œì„ 
- ë‹¨ê³„ë³„ ìš”ì²­ì´ í•„ìš”í•œ ê²½ìš° ìˆœì„œ ëª…ì‹œ
- ì˜ˆì‹œë‚˜ í˜•ì‹ì´ í•„ìš”í•œ ê²½ìš° êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ
- ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ê°„ê²°í•˜ê²Œ ìœ ì§€

ì‚¬ìš©ìì˜ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ë‚´ìš© ì—†ì´ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë§Œ ì œê³µí•˜ì„¸ìš”."""

        # ê°œì„  ìš”ì²­ ë©”ì‹œì§€ êµ¬ì„± (Gemini API v2.5 í˜•ì‹)
        content_text = f"ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•´ì£¼ì„¸ìš”:\n\n{original_prompt}"

        # Gemini 2.0 Flash-Liteë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„  ìš”ì²­
        response = client.models.generate_content(
            model="gemini-2.0-flash-lite",
            contents=[content_text],
            config=types.GenerateContentConfig(
                system_instruction=improvement_prompt,
                temperature=0.3,
                max_output_tokens=2048
            )
        )

        if not response.text:
            raise HTTPException(status_code=500, detail="Failed to improve prompt")

        improved_prompt = response.text.strip()
        
        return {
            "original_prompt": original_prompt,
            "improved_prompt": improved_prompt,
            "status": "success"
        }

    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"í”„ë¡¬í”„íŠ¸ ê°œì„ ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# í”„ë¡œì íŠ¸ ì‚­ì œ
@router.delete("/{project_id}")
def delete_project(
    project_id: str,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    crud_project.remove(db=db, id=project_id)
    return {"message": "Project deleted successfully"}

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ëª©ë¡ ì¡°íšŒ
@router.get("/{project_id}/chats")
def get_project_chats(
    project_id: str,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ì˜ ì±„íŒ…ë°© ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    return crud_project.get_project_chats(db=db, project_id=project_id, owner_id=current_user.id)

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ìƒì„±
@router.post("/{project_id}/chats")
def create_project_chat(
    project_id: str,
    chat: dict,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ì— ìƒˆ ì±„íŒ…ë°©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    
    from app.schemas.project import ProjectChatCreate
    
    # ì´ë¦„ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ "ìƒˆ ì±„íŒ…"ìœ¼ë¡œ ì„¤ì •
    if not chat.get("name") or chat.get("name").strip() == "":
        chat["name"] = "ìƒˆ ì±„íŒ…"
    
    chat_data = ProjectChatCreate(**chat)
    return crud_project.create_chat(db=db, project_id=project_id, obj_in=chat_data, owner_id=current_user.id)

async def generate_project_chat_room_name(first_message: str) -> str:
    """í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ì „ìš© AI ê¸°ë°˜ ì œëª© ìƒì„±"""
    try:
        # ë¹ˆ ë©”ì‹œì§€ ì²˜ë¦¬
        if not first_message or len(first_message.strip()) == 0:
            return "í”„ë¡œì íŠ¸ ì±„íŒ…"
        
        # ê°„ë‹¨í•œ fallback ë¨¼ì € ìƒì„± (AI ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        words = first_message.strip().split()
        fallback_title = " ".join(words[:3]) if len(words) >= 3 else " ".join(words)
        if len(fallback_title) > 20:
            fallback_title = fallback_title[:17] + "..."
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        from app.api.api_v1.endpoints.chat import get_gemini_client
        from google.genai import types
        
        client = get_gemini_client()
        if not client:
            logger.info("Gemini client not available, using fallback")
            return fallback_title
        
        # ì±„íŒ…ë°© ì œëª© ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt_template = """Generate a concise and descriptive title in Korean for this chat conversation based on the AI response content.

Requirements:
- Use 2-10 Korean words only
- No emojis or special characters
- Capture the main topic or purpose
- Be specific and informative
- Return only JSON format

Examples:
{{"title": "íŒŒì´ì¬ ê¸°ì´ˆ í•™ìŠµ"}}
{{"title": "ë ˆì‹œí”¼ ì¶”ì²œ"}}
{{"title": "í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸"}}
{{"title": "ì¼ë°˜ ëŒ€í™”"}}
{{"title": "ì¸ì‚¬"}}

AI Response Content: {message}

Generate title as JSON:"""

        # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
        limited_message = first_message[:300] if len(first_message) > 300 else first_message
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = prompt_template.format(message=limited_message)
        
        logger.info(f"Generating AI title for project chat: '{limited_message[:50]}...'")
        
        # Gemini API í˜¸ì¶œ
        try:
            logger.info(f"Calling Gemini API for project chat with model: gemini-2.0-flash-lite")
            response = client.models.generate_content(
                model="gemini-2.0-flash-lite",
                contents=[prompt],
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=50
                )
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = None
            
            if hasattr(response, 'candidates') and response.candidates and len(response.candidates) > 0:
                candidate = response.candidates[0]
                
                if hasattr(candidate, 'content') and candidate.content:
                    content = candidate.content
                    
                    if hasattr(content, 'parts') and content.parts and len(content.parts) > 0:
                        for part in content.parts:
                            if hasattr(part, 'text') and part.text:
                                response_text = part.text
                                break
            
            elif hasattr(response, 'text') and response.text:
                response_text = response.text
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ JSON íŒŒì‹± ì‹œë„
            if response_text:
                try:
                    import json
                    import re
                    
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                    text = response_text.strip()
                    if text.startswith('```'):
                        json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', text, re.DOTALL)
                        if json_match:
                            text = json_match.group(1).strip()
                    
                    # JSON íŒŒì‹±
                    result = json.loads(text)
                    if 'title' in result and result['title']:
                        ai_title = result['title'].strip()
                        if len(ai_title) <= 25:
                            logger.info(f"Successfully generated project chat AI title: '{ai_title}'")
                            return ai_title
                        else:
                            logger.info(f"Project chat AI title too long: '{ai_title}'")
                except json.JSONDecodeError as e:
                    logger.debug(f"Project chat JSON decode error: {e}")
                except Exception as e:
                    logger.debug(f"Project chat JSON parsing error: {e}")
            
        except Exception as api_error:
            logger.info(f"Project chat Gemini API error: {api_error}")
        
        # AI ìƒì„± ì‹¤íŒ¨ ì‹œ fallback ì‚¬ìš©
        logger.info(f"Using fallback title for project chat: '{fallback_title}'")
        return fallback_title
        
    except Exception as e:
        logger.error(f"Project chat room name generation error: {e}", exc_info=True)
        return "í”„ë¡œì íŠ¸ ì±„íŒ…"

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ì´ë¦„ ìƒì„±
@router.post("/{project_id}/chats/{chat_id}/generate-name")
async def generate_project_chat_name(
    project_id: str,
    chat_id: str,
    message_content: str = Form(...),
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ì´ë¦„ì„ ìƒì„±í•˜ê³  ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    # í”„ë¡œì íŠ¸ ë° ì±„íŒ…ë°© ì†Œìœ ê¶Œ í™•ì¸
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    
    try:
        logger.info(f"generate_project_chat_name called - project_id: {project_id}, chat_id: {chat_id}, message: {message_content[:50]}...")
        
        # í”„ë¡œì íŠ¸ ì±„íŒ…ë°© í™•ì¸
        project_chat = crud_project.get_chat(db=db, project_id=project_id, chat_id=chat_id)
        if project_chat:
            current_name = getattr(project_chat, 'name', None)
            current_name_str = str(current_name) if current_name is not None else ""
            logger.info(f"Found project chat: {chat_id}, current name: '{current_name_str}'")
            
            # "ìƒˆ ì±„íŒ…"ì¸ ê²½ìš°ì—ë§Œ ì œëª© ìƒì„±
            if current_name_str and current_name_str.strip() != "" and current_name_str != "ìƒˆ ì±„íŒ…":
                logger.info(f"Project chat already has a name: '{current_name_str}'")
                return {
                    "project_id": project_id,
                    "chat_id": chat_id,
                    "generated_name": current_name_str,
                    "message": "Chat already has a name"
                }
        else:
            logger.info(f"Project chat not found: {chat_id} - may not be created yet")
        
        # í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ì´ë¦„ ìƒì„±
        generated_name = await generate_project_chat_room_name(message_content)
        logger.info(f"AI generated name for project chat: '{generated_name}'")
        
        # ì±„íŒ…ë°© ì´ë¦„ ì—…ë°ì´íŠ¸
        from app.schemas.project import ProjectChatCreate
        chat_update = ProjectChatCreate(name=generated_name)
        
        try:
            logger.info(f"Attempting to update project chat - project_id: {project_id}, chat_id: {chat_id}, update: {chat_update}")
            updated_chat = crud_project.update_chat(
                db=db, 
                project_id=project_id, 
                chat_id=chat_id, 
                obj_in=chat_update, 
                owner_id=current_user.id
            )
            logger.info(f"Project chat name updated successfully: '{generated_name}'")
            
            return {
                "project_id": project_id,
                "chat_id": chat_id,
                "generated_name": generated_name,
                "updated_chat": updated_chat
            }
        except Exception as update_error:
            logger.info(f"Project chat update failed: {update_error} - chat_id: {chat_id}, error type: {type(update_error)}")
            # ì±„íŒ…ë°© ì—…ë°ì´íŠ¸ì— ì‹¤íŒ¨í•´ë„ ìƒì„±ëœ ì´ë¦„ì€ ë°˜í™˜
            return {
                "project_id": project_id,
                "chat_id": chat_id,
                "generated_name": generated_name,
                "message": "Chat name generated but room not found - may not be created yet"
            }
        
    except Exception as e:
        logger.error(f"Project chat name generation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to generate chat name")

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ìˆ˜ì •
@router.patch("/{project_id}/chats/{chat_id}")
def update_project_chat(
    project_id: str,
    chat_id: str,
    chat: dict,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ ì±„íŒ…ë°©ì„ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    
    from app.schemas.chat import ChatUpdate
    chat_data = ChatUpdate(**chat)
    return crud_project.update_chat_by_id(db=db, project_id=project_id, chat_id=chat_id, obj_in=chat_data)

# í”„ë¡œì íŠ¸ ì±„íŒ…ë°© ë©”ì‹œì§€ ì¡°íšŒ
@router.get("/{project_id}/chats/{chat_id}/messages")
def get_project_chat_messages(
    project_id: str,
    chat_id: str,
    db: Session = Depends(deps.get_db),
    current_user: User = Depends(deps.get_current_user)
):
    """í”„ë¡œì íŠ¸ ì±„íŒ…ë°©ì˜ ë©”ì‹œì§€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    project = crud_project.get(db=db, id=project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    if project.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not enough permissions")
    messages = crud_project.get_chat_messages(db=db, project_id=project_id, chat_id=chat_id)
    return {"messages": messages}

# í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹± ê¸°ëŠ¥
@router.post("/{project_id}/cache")
async def create_project_context_cache(
    project_id: str,
    content: str = Form(...),
    model: str = Form(...),
    ttl: int = Form(3600),  # ê¸°ë³¸ 1ì‹œê°„
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìƒì„±"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
        system_instruction = BRIEF_SYSTEM_PROMPT
        if project.type == "assignment":
            system_instruction += "\n\n" + ASSIGNMENT_PROMPT
        elif project.type == "record":
            system_instruction += "\n\n" + RECORD_PROMPT
        
        # ìºì‹œ ìƒì„±
        cache = client.caches.create(
            model=model,
            config=types.CreateCachedContentConfig(
                display_name=f"project_{project_id}_cache_{int(time.time())}",
                system_instruction=system_instruction,
                contents=[content],
                ttl=f"{ttl}s"
            )
        )
        
        return {
            "cache_name": cache.name,
            "project_id": project_id,
            "ttl": ttl,
            "message": "Project cache created successfully"
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create project cache: {str(e)}")

@router.get("/{project_id}/cache")
async def list_project_context_caches(
    project_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ëª©ë¡ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        caches = []
        for cache in client.caches.list():
            if cache.display_name and cache.display_name.startswith(f"project_{project_id}_cache_"):
                caches.append({
                    "name": cache.name,
                    "display_name": cache.display_name,
                    "model": cache.model,
                    "create_time": cache.create_time.isoformat() if hasattr(cache, 'create_time') and cache.create_time else None,
                    "expire_time": cache.expire_time.isoformat() if hasattr(cache, 'expire_time') and cache.expire_time else None
                })
        
        return {"caches": caches, "project_id": project_id}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list project caches: {str(e)}")

@router.delete("/{project_id}/cache/{cache_name}")
async def delete_project_context_cache(
    project_id: str,
    cache_name: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        client.caches.delete(cache_name)
        
        return {"message": "Project cache deleted successfully", "project_id": project_id}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete project cache: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì„ë² ë”© ê¸°ëŠ¥
@router.post("/{project_id}/embeddings")
async def create_project_embeddings(
    project_id: str,
    texts: List[str] = Form(...),
    model: str = Form("text-embedding-004"),
    task_type: str = Form("SEMANTIC_SIMILARITY"),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        embeddings = []
        for text in texts:
            result = client.models.embed_content(
                model=model,
                contents=text,
                config=types.EmbedContentConfig(task_type=task_type)
            )
            embeddings.append({
                "text": text,
                "embedding": result.embeddings[0] if result.embeddings else []
            })
        
        return {
            "embeddings": embeddings,
            "project_id": project_id,
            "project_type": project.type
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create project embeddings: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± ê¸°ëŠ¥
@router.post("/{project_id}/generate-prompt")
async def generate_project_prompt(
    project_id: str,
    category: str = Form(...),
    task_description: str = Form(...),
    style: str = Form("ì¹œê·¼í•œ"),
    complexity: str = Form("ì¤‘ê°„"),
    output_format: str = Form("ììœ í˜•ì‹"),
    include_examples: bool = Form(True),
    include_constraints: bool = Form(False),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ë§ì¶¤í˜• AI í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # í”„ë¡œì íŠ¸ íƒ€ì…ì— ë”°ë¥¸ íŠ¹í™”ëœ ì¹´í…Œê³ ë¦¬ ì§€ì‹œ
        project_specific_instructions = {
            "assignment": "ìˆ˜í–‰í‰ê°€ì™€ ê³¼ì œ í•´ê²°ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í‰ê°€ ê¸°ì¤€ ì¶©ì¡±ê³¼ ì°½ì˜ì  ì ‘ê·¼ì„ ëª¨ë‘ ê³ ë ¤í•©ë‹ˆë‹¤.",
            "record": "ìƒê¸°ë¶€ ì‘ì„±ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. êµìœ¡ë¶€ ê¸°ì¬ìš”ë ¹ ì¤€ìˆ˜ì™€ ì°¨ë³„í™” í¬ì¸íŠ¸ë¥¼ ë™ì‹œì— ê³ ë ¤í•©ë‹ˆë‹¤.",
            "general": "ì¼ë°˜ì ì¸ í•™ìŠµ ëª©ì ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ ì‹œìŠ¤í…œ ì§€ì‹œ ìƒì„±
        category_instructions = {
            "í•™ìŠµ": "êµìœ¡ ë° í•™ìŠµ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì „ë¬¸ê°€ë¡œì„œ, í•™ìŠµìì˜ ì´í•´ë„ë¥¼ ë†’ì´ê³  ë‹¨ê³„ì  í•™ìŠµì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì°½ì‘": "ì°½ì˜ì  ì½˜í…ì¸  ìƒì„± ì „ë¬¸ê°€ë¡œì„œ, ìƒìƒë ¥ì„ ìê·¹í•˜ê³  ë…ì°½ì ì¸ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¶„ì„": "ë°ì´í„° ë¶„ì„ ë° ë…¼ë¦¬ì  ì‚¬ê³  ì „ë¬¸ê°€ë¡œì„œ, ì²´ê³„ì ì´ê³  ê°ê´€ì ì¸ ë¶„ì„ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë²ˆì—­": "ë‹¤êµ­ì–´ ë²ˆì—­ ì „ë¬¸ê°€ë¡œì„œ, ë¬¸ë§¥ê³¼ ë‰˜ì•™ìŠ¤ë¥¼ ì •í™•íˆ ì „ë‹¬í•˜ëŠ” ë²ˆì—­ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì½”ë”©": "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì „ë¬¸ê°€ë¡œì„œ, íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì½”ë“œ ì‘ì„±ì„ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ë° ì˜ì‚¬ê²°ì • ì „ë¬¸ê°€ë¡œì„œ, ì‹¤ìš©ì ì´ê³  ê²°ê³¼ ì§€í–¥ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
            "ì¼ë°˜": "ë²”ìš© í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ë¡œì„œ, ë‹¤ì–‘í•œ ìƒí™©ì— ì ìš© ê°€ëŠ¥í•œ íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        }
        
        system_instruction = f"""
        ë‹¹ì‹ ì€ {category_instructions.get(category, category_instructions["ì¼ë°˜"])}
        
        í”„ë¡œì íŠ¸ íŠ¹í™” ìš”êµ¬ì‚¬í•­: {project_specific_instructions.get(project.type, project_specific_instructions["general"])}
        
        í”„ë¡¬í”„íŠ¸ ìƒì„± ì›ì¹™:
        1. ëª…í™•í•œ ì§€ì‹œì‚¬í•­ê³¼ êµ¬ì²´ì ì¸ ê¸°ëŒ€ ê²°ê³¼ í¬í•¨
        2. í”„ë¡œì íŠ¸ íƒ€ì…({project.type})ì— ìµœì í™”ëœ ì ‘ê·¼ ë°©ì‹
        3. ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ìµœì ì˜ ê²°ê³¼ ë„ì¶œ
        4. êµìœ¡ì  ê°€ì¹˜ì™€ ì‹¤ìš©ì„±ì˜ ê· í˜•
        
        ì‘ë‹µ í˜•ì‹:
        - í”„ë¡¬í”„íŠ¸ ì œëª© (í”„ë¡œì íŠ¸ íƒ€ì… ë°˜ì˜)
        - ë©”ì¸ í”„ë¡¬í”„íŠ¸ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì™„ì„± í˜•íƒœ)
        - í”„ë¡œì íŠ¸ë³„ íŠ¹í™” íŒ
        - ì‘ìš© ë° ë³€í˜• ì œì•ˆ
        """
        
        user_request = f"""
        ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” {project.type} í”„ë¡œì íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        ğŸ“‹ **í”„ë¡œì íŠ¸ ì •ë³´**
        - í”„ë¡œì íŠ¸ëª…: {project.name}
        - í”„ë¡œì íŠ¸ íƒ€ì…: {project.type}
        - ì¹´í…Œê³ ë¦¬: {category}
        - ì‘ì—… ì„¤ëª…: {task_description}
        
        ğŸ“Œ **ìŠ¤íƒ€ì¼ ì„¤ì •**
        - ìŠ¤íƒ€ì¼: {style}
        - ë³µì¡ë„: {complexity}
        - ì¶œë ¥ í˜•ì‹: {output_format}
        - ì˜ˆì‹œ í¬í•¨: {'ì˜ˆ' if include_examples else 'ì•„ë‹ˆì˜¤'}
        - ì œì•½ì‚¬í•­ í¬í•¨: {'ì˜ˆ' if include_constraints else 'ì•„ë‹ˆì˜¤'}
        
        ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ëŠ” {project.type} í”„ë¡œì íŠ¸ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì—¬ ìµœì í™”í•´ì£¼ì„¸ìš”.
        """
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[user_request],
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.8,
                max_output_tokens=4000,
                tools=[
                    types.Tool(google_search=types.GoogleSearch()),
                    types.Tool(code_execution=types.ToolCodeExecution())
                ]
            )
        )
        
        return {
            "generated_prompt": response.text,
            "project_id": project_id,
            "project_name": project.name,
            "project_type": project.type,
            "category": category,
            "task_description": task_description,
            "settings": {
                "style": style,
                "complexity": complexity,
                "output_format": output_format,
                "include_examples": include_examples,
                "include_constraints": include_constraints
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate project prompt: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥
@router.post("/{project_id}/chats/{chat_id}/search")
async def search_project_web(
    project_id: str,
    chat_id: str,
    request: Request,
    query: str = Form(...),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì±„íŒ…ì—ì„œ Google ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ (ìŠ¤íŠ¸ë¦¬ë°)"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ì‚¬ìš©ì ê²€ìƒ‰ ì§ˆë¬¸ì„ DBì— ì €ì¥
        user_message = ChatMessageCreate(
            content=f"ğŸ” ê²€ìƒ‰: {query}",
            role="user",
            room_id=chat_id
        )
        crud_project.create_chat_message(db, project_id=project_id, chat_id=chat_id, obj_in=user_message)
        
        async def generate_project_search_stream():
            try:
                client = get_gemini_client()
                if not client:
                    yield f"data: {json.dumps({'error': 'Gemini client not available'})}\n\n"
                    return
                
                # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
                system_instruction = BRIEF_SYSTEM_PROMPT
                if project.type == "assignment":
                    system_instruction += "\n\n" + ASSIGNMENT_PROMPT
                elif project.type == "record":
                    system_instruction += "\n\n" + RECORD_PROMPT
                
                system_instruction += f"""
                
                í˜„ì¬ í”„ë¡œì íŠ¸ ì •ë³´:
                - í”„ë¡œì íŠ¸ëª…: {project.name}
                - í”„ë¡œì íŠ¸ íƒ€ì…: {project.type}
                - ì„¤ëª…: {project.description or 'ì—†ìŒ'}
                
                ê²€ìƒ‰ ì‹œ í”„ë¡œì íŠ¸ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.
                """
                
                # í”„ë¡œì íŠ¸ ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
                if project.system_instruction and project.system_instruction.strip():
                    system_instruction += "\n\n## ì¶”ê°€ ì§€ì‹œì‚¬í•­\n" + project.system_instruction.strip()
                
                # ê²€ìƒ‰ ë„êµ¬ ì„¤ì •
                tools = [
                    types.Tool(google_search=types.GoogleSearch()),
                    types.Tool(code_execution=types.ToolCodeExecution())
                ]
                
                # ìƒì„± ì„¤ì •
                generation_config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.7,
                    max_output_tokens=3000,
                    tools=tools,
                    thinking_config=types.ThinkingConfig(
                        thinking_budget=8192,
                        include_thoughts=True
                    )
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì‹¤í–‰
                response = client.models.generate_content_stream(
                    model="gemini-2.5-flash",
                    contents=[f"í”„ë¡œì íŠ¸ '{project.name}' ê´€ë ¨í•˜ì—¬ ë‹¤ìŒì— ëŒ€í•´ ê²€ìƒ‰í•´ì£¼ì„¸ìš”: {query}"],
                    config=generation_config
                )
                
                accumulated_content = ""
                accumulated_reasoning = ""
                citations = []
                web_search_queries = []
                citations_sent = set()
                search_completed = False
                is_disconnected = False
                
                for chunk in response:
                    if await request.is_disconnected():
                        is_disconnected = True
                        break
                        
                    if chunk.candidates and len(chunk.candidates) > 0:
                        candidate = chunk.candidates[0]
                        
                        # ì½˜í…ì¸  ì²˜ë¦¬
                        if candidate.content and candidate.content.parts:
                            for part in candidate.content.parts:
                                if hasattr(part, 'thought') and part.thought:
                                    accumulated_reasoning += part.text
                                    try:
                                        yield f"data: {json.dumps({'reasoning_content': part.text})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        return
                                elif part.text:
                                    accumulated_content += part.text
                                    try:
                                        yield f"data: {json.dumps({'content': part.text})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        return
                        
                        # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
                        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                            grounding = candidate.grounding_metadata
                            
                            if hasattr(grounding, 'web_search_queries') and grounding.web_search_queries:
                                web_search_queries.extend(grounding.web_search_queries)
                            
                            if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                                new_citations = []
                                for chunk_info in grounding.grounding_chunks:
                                    if hasattr(chunk_info, 'web') and chunk_info.web:
                                        citation_url = chunk_info.web.uri
                                        
                                        # Mixed Content ë¬¸ì œ ë°©ì§€: HTTP URLì„ HTTPSë¡œ ë³€í™˜
                                        if citation_url.startswith('http://'):
                                            citation_url = citation_url.replace('http://', 'https://', 1)
                                        
                                        if citation_url not in citations_sent:
                                            citation = {
                                                "url": citation_url,
                                                "title": chunk_info.web.title if hasattr(chunk_info.web, 'title') else ""
                                            }
                                            citations.append(citation)
                                            new_citations.append(citation)
                                            citations_sent.add(citation_url)
                                
                                if new_citations:
                                    try:
                                        yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                                        return
                
                if is_disconnected:
                    return
                
                search_completed = True
                
                if web_search_queries:
                    try:
                        yield f"data: {json.dumps({'search_queries': web_search_queries})}\n\n"
                    except (ConnectionError, BrokenPipeError, GeneratorExit):
                        return
                
            except Exception as e:
                search_completed = False
                try:
                    yield f"data: {json.dumps({'error': f'Project search failed: {str(e)}'})}\n\n"
                except (ConnectionError, BrokenPipeError, GeneratorExit):
                    return
            
            # ê²€ìƒ‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ DBì— ì €ì¥
            if search_completed and accumulated_content:
                ai_message = ChatMessageCreate(
                    content=accumulated_content,
                    role="assistant",
                    room_id=chat_id,
                    reasoning_content=accumulated_reasoning if accumulated_reasoning else None,
                    citations=citations if citations else None
                )
                crud_project.create_chat_message(db, project_id=project_id, chat_id=chat_id, obj_in=ai_message)
        
        return StreamingResponse(
            generate_project_search_stream(),
            media_type="text/plain"
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Project search failed: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ í†µê³„ ì¡°íšŒ ê¸°ëŠ¥
@router.get("/{project_id}/stats/token-usage")
async def get_project_token_usage(
    project_id: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
        start_dt = None
        end_dt = None
        
        if start_date:
            # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
            start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
        if end_date:
            # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
            end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
        
        # ê° ì±„íŒ…ë°©ë³„ í† í° ì‚¬ìš©ëŸ‰ í•©ê³„
        total_usage = crud_stats.get_token_usage(
            db=db,
            start_date=start_dt,
            end_date=end_dt,
            user_id=current_user.id
        )
        
        return {
            "project_id": project_id,
            "project_name": project.name,
            "project_type": project.type,
            "token_usage": total_usage,
            "period": {
                "start_date": start_date,
                "end_date": end_date
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get project token usage: {str(e)}")

@router.get("/{project_id}/stats/chat-usage")
async def get_project_chat_usage(
    project_id: str,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ë³„ ì±„íŒ… ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # KST ì‹œê°„ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
        start_dt = None
        end_dt = None
        
        if start_date:
            # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
            start_dt = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S')
        if end_date:
            # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì˜ KST ì‹œê°„ì„ íŒŒì‹±
            end_dt = datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S')
        
        # í”„ë¡œì íŠ¸ë³„ ì±„íŒ… í†µê³„ ì¡°íšŒ
        chat_stats = crud_stats.get_chat_statistics(
            db=db,
            start_date=start_dt,
            end_date=end_dt,
            user_id=current_user.id
        )
        
        return {
            "project_id": project_id,
            "project_name": project.name,
            "project_type": project.type,
            "chat_statistics": chat_stats,
            "period": {
                "start_date": start_date,
                "end_date": end_date
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get project chat usage: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
async def get_or_create_project_context_cache(
    client,
    project: Project,
    model: str,
    cache_name: str,
    ttl: int = 7200
) -> Optional[str]:
    """í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ìºì‹œë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ê¸°ì¡´ ìºì‹œ í™•ì¸
        for cache in client.caches.list():
            if cache.display_name == cache_name and cache.model == model:
                # ìºì‹œê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
                if hasattr(cache, 'expire_time') and cache.expire_time and cache.expire_time > datetime.now(timezone.utc):
                    logger.info(f"Using existing project cache: {cache_name}")
                    return cache.name
        
        # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_instruction = BRIEF_SYSTEM_PROMPT
        if project.type == "assignment":
            system_instruction += "\n\n" + ASSIGNMENT_PROMPT
        elif project.type == "record":
            system_instruction += "\n\n" + RECORD_PROMPT
        
        # ìƒˆ ìºì‹œ ìƒì„± (ë¹ˆ contents ë¬¸ì œ í•´ê²°)
        logger.info(f"Creating new project cache: {cache_name}")
        cache = client.caches.create(
            model=model,
            config=types.CreateCachedContentConfig(
                display_name=cache_name,
                system_instruction=system_instruction,
                contents=["í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ"],  # ë¹ˆ ë°°ì—´ ëŒ€ì‹  ìµœì†Œ ì½˜í…ì¸  ì œê³µ
                ttl=f"{ttl}s"
            )
        )
        return cache.name
    except Exception as e:
        logger.error(f"Project cache creation error: {e}", exc_info=True)
        return None 

# í”„ë¡œì íŠ¸ë³„ íŒŒì¼ ì—…ë¡œë“œ ë° ê´€ë¦¬ API
@router.post("/{project_id}/files/upload")
async def upload_project_file(
    project_id: str,
    files: List[UploadFile] = File(...),
    description: Optional[str] = Form(None),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ì— íŒŒì¼ ì—…ë¡œë“œ ë° ì„ë² ë”© ìƒì„±"""
    # íƒ€ì„ì•„ì›ƒ ì„¤ì • (120ì´ˆ)
    UPLOAD_TIMEOUT = 120
    
    try:
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ì „ì²´ ì—…ë¡œë“œ í”„ë¡œì„¸ìŠ¤ ì œí•œ
        async with asyncio.timeout(UPLOAD_TIMEOUT):
            # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
            project = crud_project.get(db=db, id=project_id)
            if not project or project.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not enough permissions")
            
            client = get_gemini_client()
            if not client:
                raise HTTPException(status_code=500, detail="Gemini client not available")
            
            uploaded_files = []
            
            for file in files:
                # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
                if not await validate_file(file):
                    raise HTTPException(
                        status_code=400,
                        detail=f"Invalid file: {file.filename}"
                    )
                
                # Gemini File APIë¡œ ì—…ë¡œë“œ
                try:
                    # íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
                    file_content = await file.read()
                    
                    # íŒŒì¼ í¬ê¸° ì¬ê²€ì¦
                    if len(file_content) > MAX_FILE_SIZE:
                        raise HTTPException(
                            status_code=400,
                            detail=f"File {file.filename} is too large. Maximum size: {MAX_FILE_SIZE // (1024*1024)}MB"
                        )
                    
                    # Gemini File API ì œí•œ í™•ì¸ ë° ì²˜ë¦¬
                    if len(file_content) > GEMINI_INLINE_DATA_LIMIT:
                        logger.warning(f"Warning: File {file.filename} ({len(file_content)} bytes) exceeds Gemini inline data limit. Using File API instead.")
                        # í° íŒŒì¼ì€ File APIë¥¼ í†µí•´ ì²˜ë¦¬ (ì´ë¯¸ í˜„ì¬ êµ¬í˜„)
                    
                    # File APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ë¡œë“œ
                    uploaded_file = client.files.upload(
                        file=io.BytesIO(file_content),
                        config=dict(
                            mime_type=file.content_type,
                            display_name=f"project_{project_id}_{file.filename}"
                        )
                    )
                    
                    # íŒŒì¼ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆë¡œ ì¦ê°€)
                    max_wait_time = 60
                    wait_time = 0
                    while uploaded_file.state.name == 'PROCESSING' and wait_time < max_wait_time:
                        await asyncio.sleep(2)
                        wait_time += 2
                        try:
                            uploaded_file = client.files.get(name=uploaded_file.name)
                        except Exception as e:
                            logger.error(f"Error checking file status: {e}", exc_info=True)
                            break
                    
                    # ì²˜ë¦¬ ìƒíƒœ í™•ì¸
                    if uploaded_file.state.name != 'ACTIVE':
                        logger.warning(f"Warning: File {file.filename} is in state {uploaded_file.state.name}")
                    
                    # file.nameì—ì„œ 'files/' ì œê±° (clean_file_id ì •ì˜)
                    clean_file_id = uploaded_file.name.replace("files/", "") if uploaded_file.name.startswith("files/") else uploaded_file.name
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„ë² ë”© ìƒì„± (ê°œì„ ëœ ë°©ì‹)
                    extracted_text = ""
                    embeddings = []
                    embedding_data_list = []  # í•­ìƒ ì´ˆê¸°í™”
                
                try:
                    # íŒŒì¼ì´ í™œì„± ìƒíƒœì¼ ë•Œë§Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if uploaded_file.state.name == 'ACTIVE':
                        # PDF íŒŒì¼ ì²˜ë¦¬ (ê°•í™”ëœ ë°©ì‹)
                        if file.content_type == "application/pdf":
                            # ë‹¤ì¤‘ ì‹œë„ ë°©ì‹ìœ¼ë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            extract_attempts = [
                                # 1ì°¨ ì‹œë„: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                {
                                    "prompt": "ì´ PDF ë¬¸ì„œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í‘œ, ê·¸ë˜í”„, ìˆ˜ì‹, ë„í‘œì˜ ë‚´ìš©ë„ í¬í•¨í•´ì„œ ìµœëŒ€í•œ ìì„¸íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš° 'í…ìŠ¤íŠ¸ ì—†ìŒ'ì´ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 8000
                                },
                                # 2ì°¨ ì‹œë„: ì´ë¯¸ì§€ ê¸°ë°˜ OCR (ìŠ¤ìº” ë¬¸ì„œ ëŒ€ì‘)
                                {
                                    "prompt": "ì´ ë¬¸ì„œë¥¼ ìŠ¤ìº”ëœ ì´ë¯¸ì§€ë¡œ ì¸ì‹í•˜ì—¬ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ OCRë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ìˆ˜ì‹, í‘œ, ê·¸ë˜í”„ì˜ ë‚´ìš©ë„ í¬í•¨í•´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì •í™•íˆ ì¸ì‹í•´ì£¼ì„¸ìš”. ì½ì„ ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì€ [ì½ì„ ìˆ˜ ì—†ìŒ]ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.",
                                    "max_tokens": 8000
                                },
                                # 3ì°¨ ì‹œë„: êµ¬ì¡°í™”ëœ ì¶”ì¶œ
                                {
                                    "prompt": "ì´ ë¬¸ì„œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì¡°í™”í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:\n\n[ì œëª©]\n[ë³¸ë¬¸ ë‚´ìš©]\n[í‘œ/ê·¸ë˜í”„ ë‚´ìš©]\n[ìˆ˜ì‹]\n[ê¸°íƒ€ ì •ë³´]\n\nê° ì„¹ì…˜ë³„ë¡œ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ë‚´ìš©ì´ ì—†ëŠ” ì„¹ì…˜ì€ 'ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.",
                                    "max_tokens": 8000
                                },
                                # 4ì°¨ ì‹œë„: ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œ
                                {
                                    "prompt": "ì´ ë¬¸ì„œë¥¼ ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë„í˜•, í‘œë¥¼ ëª¨ë‘ ì„¤ëª…í•˜ê³  í¬í•¨ëœ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì‹œê°ì  ìš”ì†Œë„ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 8000
                                }
                            ]
                            
                            for attempt_idx, attempt in enumerate(extract_attempts):
                                try:
                                    logger.info(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ {attempt_idx + 1}/{len(extract_attempts)}: {file.filename}")
                                    
                                    extract_response = client.models.generate_content(
                                        model="gemini-2.5-flash",
                                        contents=[
                                            uploaded_file,
                                            attempt["prompt"]
                                        ],
                                        config=types.GenerateContentConfig(
                                            temperature=0,
                                            max_output_tokens=attempt["max_tokens"]
                                        )
                                    )
                                    
                                    if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                        extracted_text = extract_response.text[:12000]  # ìµœëŒ€ 12000ìë¡œ ì¦ê°€
                                        if len(extracted_text.strip()) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                                            logger.info(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì‹œë„ {attempt_idx + 1}): {len(extracted_text)}ì")
                                            break
                                    else:
                                        logger.warning(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt_idx + 1}): ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                                        
                                except Exception as e:
                                    logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ {attempt_idx + 1} ì‹¤íŒ¨: {e}", exc_info=True)
                                    continue
                            
                            # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°±
                            if not extracted_text or len(extracted_text.strip()) < 50:
                                # íŒŒì¼ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì •ë³´ ìƒì„±
                                extracted_text = f"""
[íŒŒì¼ ì •ë³´]
íŒŒì¼ëª…: {file.filename}
íŒŒì¼ íƒ€ì…: PDF ë¬¸ì„œ
ìƒíƒœ: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨
ì²˜ë¦¬ ì‹œê°„: {datetime.now().isoformat()}

[ì•Œë¦¼]
ì´ PDF ë¬¸ì„œëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ì´ìœ ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë µìŠµë‹ˆë‹¤:
1. ìŠ¤ìº”ëœ ì´ë¯¸ì§€ í˜•íƒœì˜ PDF
2. ë³µì¡í•œ ë ˆì´ì•„ì›ƒ êµ¬ì¡°
3. ì•”í˜¸í™”ëœ PDF
4. ì†ê¸€ì”¨ ë˜ëŠ” íŠ¹ìˆ˜ í°íŠ¸ ì‚¬ìš©
5. ê·¸ë˜í”½ ìœ„ì£¼ì˜ ë¬¸ì„œ

[ëŒ€ì•ˆ]
- ë‹¤ë¥¸ PDF ë·°ì–´ì—ì„œ í…ìŠ¤íŠ¸ ë³µì‚¬ í›„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì—…ë¡œë“œ
- ì´ë¯¸ì§€ë¡œ ìŠ¤í¬ë¦°ìƒ· í›„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì—…ë¡œë“œ
- íŒŒì¼ì„ ë‹¤ì‹œ PDFë¡œ ë‚´ë³´ë‚´ê¸° ì‹œë„
                                """.strip()
                                logger.warning(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ì „ ì‹¤íŒ¨ - ê¸°ë³¸ ì •ë³´ ìƒì„±: {file.filename}")
                                
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
                        elif file.content_type in ["text/plain"] or file.content_type.startswith("text/"):
                            try:
                                extract_response = client.models.generate_content(
                                    model="gemini-2.5-flash",
                                    contents=[
                                        uploaded_file,
                                        "ì´ í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ë‚´ìš©ì„ ì™„ì „íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
                                    ],
                                    config=types.GenerateContentConfig(
                                        temperature=0,
                                        max_output_tokens=8000
                                    )
                                )
                                
                                if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                    extracted_text = extract_response.text[:10000]
                                else:
                                    extracted_text = "í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                                    
                            except Exception as e:
                                extracted_text = f"í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                                logger.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: {file.filename} - {e}", exc_info=True)
                                
                        # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ (ê°•í™”ëœ OCR)
                        elif file.content_type.startswith("image/"):
                            # ë‹¤ì¤‘ ì‹œë„ ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ OCR
                            ocr_attempts = [
                                # 1ì°¨ ì‹œë„: ê¸°ë³¸ OCR
                                {
                                    "prompt": "ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì¸ì‹í•˜ê³ , í‘œ, ê·¸ë˜í”„, ë„í‘œì˜ ë‚´ìš©ë„ í¬í•¨í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 4000
                                },
                                # 2ì°¨ ì‹œë„: êµ¬ì¡°í™”ëœ OCR
                                {
                                    "prompt": "ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•´ì„œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì œëª©, ë³¸ë¬¸, í‘œ, ê·¸ë˜í”„ ë“±ì„ êµ¬ë¶„í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 4000
                                },
                                # 3ì°¨ ì‹œë„: ìˆ˜ì‹ ë° ê¸°í˜¸ í¬í•¨ OCR
                                {
                                    "prompt": "ì´ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸, ìˆ˜ì‹, ê¸°í˜¸, í‘œë¥¼ ëª¨ë‘ ì¶”ì¶œí•´ì£¼ì„¸ìš”. íŠ¹íˆ ìˆ˜í•™ ê¸°í˜¸ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë„ ì •í™•íˆ ì¸ì‹í•´ì£¼ì„¸ìš”.",
                                    "max_tokens": 4000
                                }
                            ]
                            
                            for attempt_idx, attempt in enumerate(ocr_attempts):
                                try:
                                    logger.info(f"ì´ë¯¸ì§€ OCR ì‹œë„ {attempt_idx + 1}/{len(ocr_attempts)}: {file.filename}")
                                    
                                    extract_response = client.models.generate_content(
                                        model="gemini-2.5-flash",
                                        contents=[
                                            uploaded_file,
                                            attempt["prompt"]
                                        ],
                                        config=types.GenerateContentConfig(
                                            temperature=0,
                                            max_output_tokens=attempt["max_tokens"]
                                        )
                                    )
                                    
                                    if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                        extracted_text = extract_response.text[:10000]
                                        if len(extracted_text.strip()) > 50:  # ìµœì†Œ 50ì ì´ìƒ
                                            logger.info(f"ì´ë¯¸ì§€ OCR ì„±ê³µ (ì‹œë„ {attempt_idx + 1}): {len(extracted_text)}ì")
                                            break
                                    else:
                                        logger.warning(f"ì´ë¯¸ì§€ OCR ì‹¤íŒ¨ (ì‹œë„ {attempt_idx + 1}): ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                                        
                                except Exception as e:
                                    logger.error(f"ì´ë¯¸ì§€ OCR ì‹œë„ {attempt_idx + 1} ì‹¤íŒ¨: {e}", exc_info=True)
                                    continue
                            
                            # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°±
                            if not extracted_text or len(extracted_text.strip()) < 20:
                                # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê¸°ë³¸ ì •ë³´ ìƒì„±
                                extracted_text = f"""
[ì´ë¯¸ì§€ ì •ë³´]
íŒŒì¼ëª…: {file.filename}
íŒŒì¼ íƒ€ì…: {file.content_type}
ìƒíƒœ: OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨
ì²˜ë¦¬ ì‹œê°„: {datetime.now().isoformat()}

[ì•Œë¦¼]
ì´ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë ¤ìš´ ì´ìœ :
1. í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ìˆœìˆ˜ ì´ë¯¸ì§€
2. ì†ê¸€ì”¨ë‚˜ íŠ¹ìˆ˜ í°íŠ¸ ì‚¬ìš©
3. ì´ë¯¸ì§€ í’ˆì§ˆì´ ë‚®ìŒ (í•´ìƒë„, íë¦¼)
4. ë³µì¡í•œ ë°°ê²½ì´ë‚˜ ë…¸ì´ì¦ˆ
5. ê¸°ìš¸ì–´ì§€ê±°ë‚˜ ì™œê³¡ëœ í…ìŠ¤íŠ¸

[ëŒ€ì•ˆ]
- ì´ë¯¸ì§€ í’ˆì§ˆì„ ë†’ì—¬ì„œ ë‹¤ì‹œ ì—…ë¡œë“œ
- í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ í¬ë¡­í•´ì„œ ì—…ë¡œë“œ
- í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ íƒ€ì´í•‘í•´ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì—…ë¡œë“œ
                                """.strip()
                                logger.warning(f"ì´ë¯¸ì§€ OCR ì™„ì „ ì‹¤íŒ¨ - ê¸°ë³¸ ì •ë³´ ìƒì„±: {file.filename}")
                        
                        # ì›Œë“œ ë¬¸ì„œ ì²˜ë¦¬
                        elif file.content_type in [
                            'application/msword',
                            'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
                        ]:
                            try:
                                extract_response = client.models.generate_content(
                                    model="gemini-2.5-flash",
                                    contents=[
                                        uploaded_file,
                                        "ì´ ì›Œë“œ ë¬¸ì„œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í‘œ, ê·¸ë˜í”„, ì´ë¯¸ì§€ ì„¤ëª…ë„ í¬í•¨í•´ì„œ ì™„ì „íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”."
                                    ],
                                    config=types.GenerateContentConfig(
                                        temperature=0,
                                        max_output_tokens=8000
                                    )
                                )
                                
                                if extract_response and hasattr(extract_response, 'text') and extract_response.text:
                                    extracted_text = extract_response.text[:10000]
                                else:
                                    extracted_text = "ì›Œë“œ ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨: ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                                    
                            except Exception as e:
                                extracted_text = f"ì›Œë“œ ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
                                logger.error(f"ì›Œë“œ ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {file.filename} - {e}", exc_info=True)
                        
                        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ê²½ìš° ì„ë² ë”© ìƒì„±
                        if (extracted_text and len(extracted_text.strip()) > 30 and 
                            not any(fail_text in extracted_text for fail_text in [
                                "ì¶”ì¶œ ì‹¤íŒ¨", "OCR ì‹¤íŒ¨", "ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
                            ])):
                            
                            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ê°œì„ ëœ ë°©ì‹)
                            chunk_size = 1000  # ì²­í¬ í¬ê¸° ì¡°ì •
                            overlap = 100      # ì¤‘ë³µ ë²”ìœ„ ì¡°ì •
                            text_chunks = []
                            
                            # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
                            paragraphs = extracted_text.split('\n\n')
                            current_chunk = ""
                            
                            for paragraph in paragraphs:
                                if len(current_chunk) + len(paragraph) <= chunk_size:
                                    current_chunk += paragraph + "\n\n"
                                else:
                                    if current_chunk.strip():
                                        text_chunks.append(current_chunk.strip())
                                    current_chunk = paragraph + "\n\n"
                            
                            if current_chunk.strip():
                                text_chunks.append(current_chunk.strip())
                            
                            # ì²­í¬ê°€ ë„ˆë¬´ í° ê²½ìš° ê°•ì œ ë¶„í• 
                            final_chunks = []
                            for chunk in text_chunks:
                                if len(chunk) > chunk_size:
                                    for i in range(0, len(chunk), chunk_size - overlap):
                                        sub_chunk = chunk[i:i + chunk_size]
                                        if sub_chunk.strip():
                                            final_chunks.append(sub_chunk.strip())
                                else:
                                    final_chunks.append(chunk)
                            
                            logger.info(f"í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì™„ë£Œ: {len(final_chunks)}ê°œ ì²­í¬")
                            
                            # ì„ë² ë”© ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ ê³ ë ¤)
                            for i, chunk in enumerate(final_chunks):
                                if len(chunk.strip()) < 20:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ì œì™¸
                                    continue
                                    
                                try:
                                    embed_result = client.models.embed_content(
                                        model="text-embedding-004",
                                        contents=chunk,
                                        config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT")
                                    )
                                    
                                    if embed_result.embeddings:
                                        # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
                                        embedding_vector = (
                                            embed_result.embeddings[0].values 
                                            if hasattr(embed_result.embeddings[0], 'values') 
                                            else list(embed_result.embeddings[0])
                                        )
                                        
                                        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„
                                        embedding_data_list.append({
                                            "project_id": project_id,
                                            "file_id": clean_file_id,
                                            "file_name": file.filename,
                                            "chunk_index": i,
                                            "chunk_text": chunk,
                                            "embedding_vector": embedding_vector,
                                            "embedding_model": "text-embedding-004",
                                            "task_type": "RETRIEVAL_DOCUMENT",
                                            "chunk_size": len(chunk),
                                            "similarity_threshold": 0.75
                                        })
                                        
                                        # ê¸°ì¡´ í˜•ì‹ë„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
                                        embeddings.append({
                                            "chunk_index": i,
                                            "text": chunk,
                                            "embedding": embed_result.embeddings[0],
                                            "size": len(chunk)
                                        })
                                        
                                except Exception as e:
                                    logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ì²­í¬ {i}): {e}", exc_info=True)
                                    continue
                            
                            logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding_data_list)}ê°œ ì„ë² ë”©")
                        
                        else:
                            logger.warning(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ê°€ ì„ë² ë”© ìƒì„±ì— ë¶€ì í•©: {file.filename}")
                    
                    else:
                        logger.warning(f"íŒŒì¼ì´ ACTIVE ìƒíƒœê°€ ì•„ë‹˜: {file.filename} (ìƒíƒœ: {uploaded_file.state.name})")
                        extracted_text = f"íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘: {uploaded_file.state.name}"
                        
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file.filename} - {e}", exc_info=True)
                    extracted_text = f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì„ë² ë”© ì €ì¥
                if embedding_data_list:
                    try:
                        embedding_creates = [ProjectEmbeddingCreate(**data) for data in embedding_data_list]
                        saved_embeddings = crud_embedding.batch_create_embeddings(db, embedding_creates)
                        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ì„ë² ë”©: {len(saved_embeddings)}ê°œ (íŒŒì¼: {file.filename})")
                    except Exception as e:
                        logger.error(f"ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
                else:
                    logger.warning(f"ì €ì¥í•  ì„ë² ë”©ì´ ì—†ìŒ: {file.filename}")
                
                # íŒŒì¼ ì •ë³´ ì €ì¥ (ì„ë² ë”© ì •ë³´ í¬í•¨)
                file_info = {
                    "file_id": clean_file_id,
                    "original_name": file.filename,
                    "mime_type": file.content_type,
                    "size": len(file_content),
                    "uri": uploaded_file.uri,
                    "state": uploaded_file.state.name,
                    "upload_time": datetime.now().isoformat(),
                    "description": description,
                    "extracted_text": extracted_text,
                    "processing_status": "completed" if uploaded_file.state.name == 'ACTIVE' else "processing",
                    "embeddings": embeddings if embeddings else [],
                    "embedding_stats": {
                        "chunks_count": len(embeddings),
                        "total_chars": sum(chunk["size"] for chunk in embeddings) if embeddings else 0,
                        "embedding_model": "text-embedding-004",
                        "has_embeddings": len(embeddings) > 0
                    }
                }
                
                uploaded_files.append(file_info)
                
            except HTTPException:
                raise
            except Exception as e:
                logger.error(f"Error uploading file {file.filename}: {e}", exc_info=True)
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to upload file {file.filename}: {str(e)}"
                )
        
        return {
            "project_id": project_id,
            "uploaded_files": uploaded_files,
            "total_files": len(uploaded_files),
            "message": f"Successfully uploaded {len(uploaded_files)} files"
        }
    
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=408, 
            detail="íŒŒì¼ ì—…ë¡œë“œ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to upload files: {str(e)}")

@router.get("/{project_id}/files")
async def list_project_files(
    project_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ì— ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # í”„ë¡œì íŠ¸ ê´€ë ¨ íŒŒì¼ë“¤ ì¡°íšŒ
        project_files = []
        for file in client.files.list():
            if file.display_name and file.display_name.startswith(f"project_{project_id}_"):
                # file.nameì—ì„œ 'files/' ì œê±° (Gemini APIì—ì„œ files/file_id í˜•íƒœë¡œ ë°˜í™˜ë¨)
                clean_file_id = file.name.replace("files/", "") if file.name.startswith("files/") else file.name
                
                file_info = {
                    "file_id": clean_file_id,
                    "display_name": file.display_name,
                    "original_name": file.display_name.replace(f"project_{project_id}_", ""),
                    "uri": file.uri,
                    "state": file.state.name,
                    "create_time": file.create_time.isoformat() if hasattr(file, 'create_time') and file.create_time else None,
                    "expire_time": file.expire_time.isoformat() if hasattr(file, 'expire_time') and file.expire_time else None
                }
                project_files.append(file_info)
        
        return {
            "project_id": project_id,
            "files": project_files,
            "total_count": len(project_files)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list project files: {str(e)}")

@router.delete("/{project_id}/files/{file_id}")
async def delete_project_file(
    project_id: str,
    file_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ íŒŒì¼ ì‚­ì œ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # Gemini APIëŠ” files/file_id í˜•íƒœë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ files/ ì ‘ë‘ì‚¬ ì¶”ê°€
        full_file_id = f"files/{file_id}" if not file_id.startswith("files/") else file_id
        
        # ê´€ë ¨ ì„ë² ë”© ë¨¼ì € ì‚­ì œ
        try:
            deleted_embeddings = crud_embedding.delete_by_file(db, project_id, file_id)
            logger.info(f"Deleted {deleted_embeddings} embeddings for file {file_id}")
        except Exception as e:
            logger.error(f"Failed to delete embeddings for file {file_id}: {e}", exc_info=True)
        
        # íŒŒì¼ ì‚­ì œ
        try:
            client.files.delete(name=full_file_id)
        except Exception as e:
            logger.error(f"Failed to delete file {full_file_id}: {e}", exc_info=True)
            # íŒŒì¼ì´ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            pass
        
        return {
            "message": "File deleted successfully",
            "project_id": project_id,
            "file_id": file_id
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete file: {str(e)}")

# í”„ë¡œì íŠ¸ë³„ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ API
@router.post("/{project_id}/knowledge/search")
async def search_project_knowledge(
    project_id: str,
    query: str = Form(...),
    top_k: int = Form(5),
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì—…ë¡œë“œ íŒŒì¼ë“¤ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        # geminiapiupdate ì°¸ê³ : ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹œ RETRIEVAL_QUERY íƒœìŠ¤í¬ íƒ€ì… ì‚¬ìš©
        query_embed_result = client.models.embed_content(
            model="text-embedding-004",
            contents=query,
            config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY")
        )
        
        if not query_embed_result.embeddings:
            raise HTTPException(status_code=500, detail="Failed to generate query embedding")
        
        # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
        query_embedding = query_embed_result.embeddings[0].values if hasattr(query_embed_result.embeddings[0], 'values') else list(query_embed_result.embeddings[0])
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰ (ì„ê³„ê°’ ë‚®ì¶¤)
        try:
            logger.info(f"ğŸ” ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            logger.info(f"   í”„ë¡œì íŠ¸ ID: {project_id}")
            logger.info(f"   ìš”ì²­ ê²°ê³¼ ìˆ˜: {top_k}")
            
            similar_embeddings = crud_embedding.search_similar(
                db=db,
                project_id=project_id,
                query_embedding=query_embedding,
                top_k=top_k,
                threshold=0.4  # ì„ê³„ê°’ì„ 0.75ì—ì„œ 0.4ë¡œ ë‚®ì¶¤
            )
            
            logger.info(f"   ê²€ìƒ‰ëœ ì„ë² ë”© ìˆ˜: {len(similar_embeddings)}")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜
            top_chunks = []
            for i, result in enumerate(similar_embeddings):
                top_chunks.append({
                    "text": result["content"],
                    "similarity": result["similarity"],
                    "source_file": result["file_name"],
                    "chunk_index": result["chunk_index"]
                })
                logger.info(f"   [{i+1}] ìœ ì‚¬ë„: {result['similarity']:.3f}, íŒŒì¼: {result['file_name']}")
                
        except Exception as e:
            logger.error(f"âŒ ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
            # ë””ë²„ê¹…ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´
            all_embeddings = crud_embedding.get_by_project(db, project_id)
            logger.debug(f"   ì „ì²´ ì„ë² ë”© ê°œìˆ˜: {len(all_embeddings)}")
            if all_embeddings:
                logger.debug(f"   íŒŒì¼ ëª©ë¡: {list(set(e.file_name for e in all_embeddings))}")
            # í´ë°±: ë¹ˆ ê²°ê³¼ ë°˜í™˜
            top_chunks = []
        
        # ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
        search_results = []
        if top_chunks:
            # ê´€ë ¨ ì²­í¬ë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            combined_context = "\n\n".join([
                f"[{chunk['source_file']}] {chunk['text']}"
                for chunk in top_chunks
            ])
            
            # AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
            search_response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=[
                    f"""
                    ë‹¤ìŒì€ ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì—ì„œ ì¶”ì¶œí•œ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:
                    
                    {combined_context}
                    
                    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
                    ì§ˆë¬¸: {query}
                    
                    ë‹µë³€ í˜•ì‹:
                    1. í•µì‹¬ ë‚´ìš© ìš”ì•½
                    2. êµ¬ì²´ì ì¸ ë‹µë³€
                    3. ì¶œì²˜ ë° ì°¸ê³ ì‚¬í•­
                    
                    ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš° "ì œê³µëœ ìë£Œì—ì„œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
                    """
                ],
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=2000
                )
            )
            
            search_results.append({
                "content": search_response.text,
                "relevance_score": max(chunk["similarity"] for chunk in top_chunks),
                "source_chunks": len(top_chunks),
                "source_files": list(set(chunk["source_file"] for chunk in top_chunks))
            })
        
        return {
            "project_id": project_id,
            "query": query,
            "results": search_results,
            "total_results": len(search_results),
            "embedding_stats": crud_embedding.get_embedding_stats(db, project_id)  # ì„ë² ë”© í†µê³„ ì¶”ê°€
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to search knowledge: {str(e)}")

# ì„ë² ë”© í†µê³„ ì¡°íšŒ API ì¶”ê°€
@router.get("/{project_id}/embeddings/stats")
async def get_project_embedding_stats(
    project_id: str,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì„ë² ë”© í†µê³„ ì¡°íšŒ"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        # ì„ë² ë”© í†µê³„ ì¡°íšŒ
        stats = crud_embedding.get_embedding_stats(db, project_id)
        
        # ìµœê·¼ ì„ë² ë”© ì •ë³´ ì¡°íšŒ
        recent_embeddings = crud_embedding.get_by_project(db, project_id)
        
        # íŒŒì¼ë³„ í†µê³„
        file_stats = {}
        for embedding in recent_embeddings:
            file_name = embedding.file_name
            if file_name not in file_stats:
                file_stats[file_name] = {
                    "chunks": 0,
                    "total_chars": 0,
                    "embedding_model": embedding.embedding_model,
                    "task_type": embedding.task_type
                }
            file_stats[file_name]["chunks"] += 1
            file_stats[file_name]["total_chars"] += embedding.chunk_size
        
        return {
            "project_id": project_id,
            "project_name": project.name,
            "embedding_stats": stats,
            "file_stats": file_stats,
            "embedding_model": "text-embedding-004",
            "supported_task_types": ["RETRIEVAL_DOCUMENT", "RETRIEVAL_QUERY"],
            "threshold": 0.75
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get embedding stats: {str(e)}")

# ì„ë² ë”© ì¬ìƒì„± API ì¶”ê°€
@router.post("/{project_id}/embeddings/regenerate")
async def regenerate_project_embeddings(
    project_id: str,
    file_id: Optional[str] = None,
    current_user: User = Depends(deps.get_current_user),
    db: Session = Depends(deps.get_db)
):
    """í”„ë¡œì íŠ¸ ì„ë² ë”© ì¬ìƒì„± (íŠ¹ì • íŒŒì¼ ë˜ëŠ” ì „ì²´)"""
    try:
        # í”„ë¡œì íŠ¸ ì†Œìœ ê¶Œ í™•ì¸
        project = crud_project.get(db=db, id=project_id)
        if not project or project.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Not enough permissions")
        
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")
        
        if file_id:
            # íŠ¹ì • íŒŒì¼ì˜ ì„ë² ë”©ë§Œ ì¬ìƒì„±
            deleted_count = crud_embedding.delete_by_file(db, project_id, file_id)
            message = f"Regenerated embeddings for file {file_id} (deleted {deleted_count} old embeddings)"
        else:
            # ì „ì²´ í”„ë¡œì íŠ¸ ì„ë² ë”© ì¬ìƒì„±
            deleted_count = crud_embedding.delete_by_project(db, project_id)
            message = f"Regenerated all project embeddings (deleted {deleted_count} old embeddings)"
        
        return {
            "project_id": project_id,
            "message": message,
            "deleted_embeddings": deleted_count,
            "note": "íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œìš´ ì„ë² ë”©ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to regenerate embeddings: {str(e)}")

# í–¥ìƒëœ í”„ë¡œì íŠ¸ ì±„íŒ…ì—ì„œ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ í™œìš©
async def generate_gemini_stream_response_with_files(
    messages: list,
    model: str,
    room_id: str,
    db: Session,
    user_id: str,
    project_id: str,
    project_type: Optional[str] = None,
    file_data_list: Optional[List[str]] = None,
    file_types: Optional[List[str]] = None,
    file_names: Optional[List[str]] = None
) -> AsyncGenerator[str, None]:
    """íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ í”„ë¡œì íŠ¸ ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    try:
        client = get_gemini_client()
        if not client:
            raise HTTPException(status_code=500, detail="Gemini client not available")

        # í”„ë¡œì íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        project = crud_project.get(db=db, id=project_id)
        
        # í”„ë¡œì íŠ¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = BRIEF_SYSTEM_PROMPT
        if project_type == "assignment":
            system_prompt += "\n\n" + ASSIGNMENT_PROMPT
        elif project_type == "record":
            system_prompt += "\n\n" + RECORD_PROMPT
            
        # í”„ë¡œì íŠ¸ ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        if project and project.system_instruction and project.system_instruction.strip():
            system_prompt += "\n\n## ì¶”ê°€ ì§€ì‹œì‚¬í•­\n" + project.system_instruction.strip()

        # í”„ë¡œì íŠ¸ ì—…ë¡œë“œ íŒŒì¼ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        project_files = []
        try:
            for file in client.files.list():
                if file.display_name and file.display_name.startswith(f"project_{project_id}_"):
                    if file.state.name == "ACTIVE":
                        project_files.append(file)
            
            if project_files:
                system_prompt += f"""
                
                ## ğŸ“ í”„ë¡œì íŠ¸ ì°¸ê³  ìë£Œ
                ì´ í”„ë¡œì íŠ¸ì—ëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ì´ ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤:
                {', '.join([f.display_name.replace(f'project_{project_id}_', '') for f in project_files])}
                
                ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´ ì´ íŒŒì¼ë“¤ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
                """
        except Exception as e:
            logger.error(f"Failed to load project files: {e}", exc_info=True)

        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬ ë° ì²˜ë¦¬
        valid_messages = []
        for msg in messages[-15:]:  # ìµœê·¼ 15ê°œë§Œ 
            if msg.get("content") and msg["content"].strip():
                valid_messages.append(msg)

        if len(valid_messages) == 0:
            raise HTTPException(status_code=400, detail="No valid message content found")

        # ì»¨í…ì¸  êµ¬ì„±
        contents = []
        
        # í”„ë¡œì íŠ¸ íŒŒì¼ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
        for file in project_files[:3]:
            contents.append(file)
        
        # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬
        if file_data_list and file_types and file_names:
            for file_data, file_type, file_name in zip(file_data_list, file_types, file_names):
                if file_type.startswith("image/"):
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )
                elif file_type == "application/pdf":
                    contents.append(
                        types.Part.from_bytes(
                            data=base64.b64decode(file_data),
                            mime_type=file_type
                        )
                    )

        # ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation_text = ""
        for message in valid_messages:
            role_text = "Human" if message["role"] == "user" else "Assistant"
            conversation_text += f"{role_text}: {message['content']}\n"

        contents.append(conversation_text)

        # ë„êµ¬ ì„¤ì •
        tools = [
            types.Tool(google_search=types.GoogleSearch()),
            types.Tool(code_execution=types.ToolCodeExecution())
        ]

        # ìƒì„± ì„¤ì •
        generation_config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            temperature=0.7,
            top_p=0.95,
            max_output_tokens=8192,
            tools=tools,
            thinking_config=types.ThinkingConfig(
                thinking_budget=16384,
                include_thoughts=True
            )
        )

        # í† í° ê³„ì‚°
        input_token_count = count_tokens_with_tiktoken(conversation_text, model)
        input_tokens = input_token_count.get("input_tokens", 0)

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        accumulated_content = ""
        accumulated_reasoning = ""
        thought_time = 0.0
        citations = []
        citations_sent = set()
        new_citations = []

        try:
            response = client.models.generate_content_stream(
                model=model,
                contents=contents,
                config=generation_config
            )

            start_time = time.time()
            
            for chunk in response:
                if chunk.candidates and len(chunk.candidates) > 0:
                    candidate = chunk.candidates[0]
                    
                    # ì½˜í…ì¸  íŒŒíŠ¸ ì²˜ë¦¬
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if hasattr(part, 'thought') and part.thought:
                                accumulated_reasoning += part.text
                                thought_time = time.time() - start_time
                                yield f"data: {json.dumps({'reasoning_content': part.text, 'thought_time': thought_time})}\n\n"
                            elif part.text:
                                accumulated_content += part.text
                                yield f"data: {json.dumps({'content': part.text})}\n\n"

                    # ê·¸ë¼ìš´ë”© ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding = candidate.grounding_metadata
                        
                        if hasattr(grounding, 'grounding_chunks') and grounding.grounding_chunks:
                            for chunk_info in grounding.grounding_chunks:
                                if hasattr(chunk_info, 'web') and chunk_info.web:
                                    citation_url = chunk_info.web.uri
                                    if citation_url not in citations_sent:
                                        citation = {
                                            "url": citation_url,
                                            "title": chunk_info.web.title if hasattr(chunk_info.web, 'title') else ""
                                        }
                                        citations.append(citation)
                                        new_citations.append(citation)
                                        citations_sent.add(citation_url)
                            
                            if new_citations:
                                try:
                                    yield f"data: {json.dumps({'citations': new_citations})}\n\n"
                                    new_citations = []  # ì „ì†¡ í›„ ì´ˆê¸°í™”
                                except (ConnectionError, BrokenPipeError, GeneratorExit):
                                    return

            # ì¶œë ¥ í† í° ê³„ì‚°
            output_token_count = count_tokens_with_tiktoken(accumulated_content, model)
            output_tokens = output_token_count.get("input_tokens", 0)
            
            # ì‚¬ê³  í† í° ê³„ì‚°
            thinking_tokens = 0
            if accumulated_reasoning:
                thinking_token_count = count_tokens_with_tiktoken(accumulated_reasoning, model)
                thinking_tokens = thinking_token_count.get("input_tokens", 0)

            # í† í° ì‚¬ìš©ëŸ‰ ì €ì¥ (KST ì‹œê°„ìœ¼ë¡œ ì €ì¥)
            from pytz import timezone
            kst = timezone('Asia/Seoul')
            crud_stats.create_token_usage(
                db=db,
                user_id=user_id,
                room_id=room_id,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens + thinking_tokens,
                timestamp=datetime.now(kst),
                chat_type=f"project_{project_type}" if project_type else None
            )

            # AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
            if accumulated_content:
                message_create = ChatMessageCreate(
                    content=accumulated_content,
                    role="assistant",
                    room_id=room_id,
                    reasoning_content=accumulated_reasoning if accumulated_reasoning else None,
                    thought_time=thought_time if thought_time > 0 else None,
                    citations=citations if citations else None
                )
                crud_project.create_chat_message(db, project_id=project_id, chat_id=room_id, obj_in=message_create)

        except Exception as api_error:
            error_message = f"Gemini API Error: {str(api_error)}"
            yield f"data: {json.dumps({'error': error_message})}\n\n"

    except Exception as e:
        error_message = f"Enhanced Stream Generation Error: {str(e)}"
        yield f"data: {json.dumps({'error': error_message})}\n\n" 